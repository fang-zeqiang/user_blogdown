<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

        <title>循环神经网络 (Recurrent Neural Network, RNN) - Zeqiang Fang | 方泽强</title>

    <meta name="referrer" content="no-referrer">
    
    <meta property="og:title" content="循环神经网络 (Recurrent Neural Network, RNN) - Zeqiang Fang | 方泽强">
    <meta name="description" property="og:description" content="文章部分内容参考了 Christopher 的博客 Understanding LSTM Networks，内容翻译和图片重绘已得到原作者同意，重绘后的图片源文件请参见 这里。 发展史 循环神经网络 (Recurrent Neural Network, RNN)">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Zeqiang Fang | 方泽强">
    <meta property="og:url" content="https://leovan.me/cn/2018/09/rnn/">

    
    
    
    
    <meta name="author" property="article:author" content="范叶亮">
    
    
    
    <meta name="date" property="article:published_time" content="2018-09-21" scheme="YYYY-MM-DD">
    
    
    <meta name="date" property="article:modified_time" content="2018-09-21" scheme="YYYY-MM-DD">
    

    
    <meta name="keywords" property="article:tag" content ="循环神经网络,Recurrent Neural Network,RNN,长短时记忆网络,Long Short Term Memory,LSTM,Gated Recurrent Unit,GRU">
    
    
    <meta name="theme-color" content="#0d0d0d">
    
    <link rel="icon" type="image/png" sizes="16x16" href="/images/web/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/web/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/images/web/favicon-48x48.png">
    <link rel="icon" type="image/png" sizes="62x62" href="/images/web/favicon-62x62.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/images/web/favicon-192x192.png">
    <link rel="apple-touch-icon" size="192x192" href="/images/web/icon-192x192.png">
    <link rel="manifest" href="/manifest.json">
        
    

    

    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://leovan.me/cn"
        },
        "name": "循环神经网络 (Recurrent Neural Network, RNN)",
        "headline": "循环神经网络 (Recurrent Neural Network, RNN)",
        "description" : "文章部分内容参考了 Christopher 的博客 Understanding LSTM Networks，内容翻译和图片重绘已得到原作者同意，重绘后的图片源文件请参见 这里。 发展史 循环神经网络 (Recurrent Neural Network, RNN)",
        "genre": [
            "深度学习"
        ],
        "datePublished": "2018-09-21",
        "dateModified": "2018-09-21",
        "wordCount": "4528",
        "keywords": [
            "循环神经网络", "Recurrent Neural Network", "RNN", "长短时记忆网络", "Long Short Term Memory", "LSTM", "Gated Recurrent Unit", "GRU"
        ],
        "image": [
            "https://leovan.me/images/cn/2018-09-21-rnn/rnn-loops.png", "https://leovan.me/images/cn/2018-09-21-rnn/rnn-loops-unrolled.png", "https://leovan.me/images/cn/2018-09-21-rnn/tanh-function.png", "https://leovan.me/images/cn/2018-09-21-rnn/rnn-long-term-dependencies-short.png", "https://leovan.me/images/cn/2018-09-21-rnn/rnn-long-term-dependencies-long.png", "https://leovan.me/images/cn/2018-09-21-rnn/rnn.png", "https://leovan.me/images/cn/2018-09-21-rnn/lstm.png", "https://leovan.me/images/cn/2018-09-21-rnn/lstm-operations-symbols.png", "https://leovan.me/images/cn/2018-09-21-rnn/lstm-cell-state.png", "https://leovan.me/images/cn/2018-09-21-rnn/lstm-pointwise-operation.png", "https://leovan.me/images/cn/2018-09-21-rnn/lstm-cell-forget-gate.png", "https://leovan.me/images/cn/2018-09-21-rnn/lstm-cell-input-gate.png", "https://leovan.me/images/cn/2018-09-21-rnn/lstm-cell-state-update.png", "https://leovan.me/images/cn/2018-09-21-rnn/lstm-cell-output-gate.png", "https://leovan.me/images/cn/2018-09-21-rnn/peephole-cell.png", "https://leovan.me/images/cn/2018-09-21-rnn/cfig-cell.png", "https://leovan.me/images/cn/2018-09-21-rnn/gru-cell.png"
        ],
        "author": {
            "@type": "Person",
            "name": "范叶亮"
        },
        "publisher": {
            "@type": "Organization",
            "name": "范叶亮",
            "logo": {
                "@type": "ImageObject",
                "url": "https://leovan.me/images/web/publisher-logo.png"
            }
        },
        "url": "https://leovan.me/cn/2018/09/rnn/"
    }
    </script>
    

    <script src='//cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js'></script>
<script src='//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js'></script>

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/6.5.95/css/materialdesignicons.min.css">





<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<link rel="stylesheet" type="text/css" href="/css/fonts.css" />
<link rel="stylesheet" type="text/css" href="/css/style.css">
<link rel="stylesheet" type="text/css" href="/css/light.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" id="dark-mode-style" disabled="disabled">
<link rel="stylesheet" type="text/css" href="/css/icons.css">
<link rel="stylesheet" type="text/css" href="/css/print.css">

  </head>

  
  <body class="cn">
    <header class="masthead">
      

<div class="logo"></div>
<p class="slogan">优雅永不过时</p>
      <nav class="menu">
  <ul>
  
  
  
    
  
  
    <li><a href="/">首页</a></li>
  
    <li><a href="/cn/">博客</a></li>
  
    <li><a href="/categories/">分类</a></li>
  
    <li class="menu-separator"><span>&nbsp;</span></li>
  
    <li><a href="/cn/about/">关于</a></li>
  
    <li><a href="/cn/resume/">简历</a></li>
  
  


<li class="menu-separator"><span>&nbsp;</span></li>

<li><a href="/cn/index.xml" target="_blank" type="application/rss+xml" title="RSS feed">订阅</a></li>

<li><a href="https://github.com/fang-zeqiang/fang-zeqiang.github.io/blob/master/LICENSE" target="_blank" title="Attribution-NonCommercial-ShareAlike 4.0 International">版权</a></li>


  <li class="light-dark-mode no-border-bottom"><a id="light-dark-mode-action"><span id="light-dark-mode-icon" class="mdi mdi-weather-night"></span></a></li>
  </ul>
</nav>

      <script src="/js/toggle-theme.js"></script>
    </header>

    <article class="main">
      <header class="title">
      
<h1>循环神经网络 (Recurrent Neural Network, RNN)</h1>







<h3>范叶亮 / 
2018-09-21</h3>



<h3 class="post-meta">


<strong>分类: </strong>
<a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>




/




<strong>标签: </strong>
<span>循环神经网络</span>, <span>Recurrent Neural Network</span>, <span>RNN</span>, <span>长短时记忆网络</span>, <span>Long Short Term Memory</span>, <span>LSTM</span>, <span>Gated Recurrent Unit</span>, <span>GRU</span>




/


<strong>字数: </strong>
4528
</h3>



<hr>


      </header>






<blockquote>
<p>文章部分内容参考了 Christopher 的博客 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noreferrer" target="_blank">Understanding LSTM Networks</a>，内容翻译和图片重绘已得到原作者同意，重绘后的图片源文件请参见 <a href="https://github.com/leovan/cdn.leovan.me/tree/master/images/blog/cn/2018-09-21-rnn/" rel="noreferrer" target="_blank">这里</a>。</p>
</blockquote>

<h2 id="发展史">发展史</h2>

<p>循环神经网络 (Recurrent Neural Network, RNN) 一般是指时间递归神经网络而非结构递归神经网络 (Recursive Neural Network)，其主要用于对序列数据进行建模。Salehinejad 等人 <sup class="footnote-ref" id="fnref:salehinejad2017recent"><a href="#fn:salehinejad2017recent">1</a></sup> 的一篇综述文章列举了 RNN 发展过程中的一些重大改进，如下表所示：</p>

<table>
<thead>
<tr>
<th>Year</th>
<th>1st Author</th>
<th>Contribution</th>
</tr>
</thead>

<tbody>
<tr>
<td>1990</td>
<td>Elman</td>
<td>Popularized simple RNNs (Elman network)</td>
</tr>

<tr>
<td>1993</td>
<td>Doya</td>
<td>Teacher forcing for gradient descent (GD)</td>
</tr>

<tr>
<td>1994</td>
<td>Bengio</td>
<td>Difficulty in learning long term dependencies with gradient descend</td>
</tr>

<tr>
<td>1997</td>
<td>Hochreiter</td>
<td>LSTM: long-short term memory for vanishing gradients problem</td>
</tr>

<tr>
<td>1997</td>
<td>Schuster</td>
<td>BRNN: Bidirectional recurrent neural networks</td>
</tr>

<tr>
<td>1998</td>
<td>LeCun</td>
<td>Hessian matrix approach for vanishing gradients problem</td>
</tr>

<tr>
<td>2000</td>
<td>Gers</td>
<td>Extended LSTM with forget gates</td>
</tr>

<tr>
<td>2001</td>
<td>Goodman</td>
<td>Classes for fast Maximum entropy training</td>
</tr>

<tr>
<td>2005</td>
<td>Morin</td>
<td>A hierarchical softmax function for language modeling using RNNs</td>
</tr>

<tr>
<td>2005</td>
<td>Graves</td>
<td>BLSTM: Bidirectional LSTM</td>
</tr>

<tr>
<td>2007</td>
<td>Jaeger</td>
<td>Leaky integration neurons</td>
</tr>

<tr>
<td>2007</td>
<td>Graves</td>
<td>MDRNN: Multi-dimensional RNNs</td>
</tr>

<tr>
<td>2009</td>
<td>Graves</td>
<td>LSTM for hand-writing recognition</td>
</tr>

<tr>
<td>2010</td>
<td>Mikolov</td>
<td>RNN based language model</td>
</tr>

<tr>
<td>2010</td>
<td>Neir</td>
<td>Rectified linear unit (ReLU) for vanishing gradient problem</td>
</tr>

<tr>
<td>2011</td>
<td>Martens</td>
<td>Learning RNN with Hessian-free optimization</td>
</tr>

<tr>
<td>2011</td>
<td>Mikolov</td>
<td>RNN by back-propagation through time (BPTT) for statistical language modeling</td>
</tr>

<tr>
<td>2011</td>
<td>Sutskever</td>
<td>Hessian-free optimization with structural damping</td>
</tr>

<tr>
<td>2011</td>
<td>Duchi</td>
<td>Adaptive learning rates for each weight</td>
</tr>

<tr>
<td>2012</td>
<td>Gutmann</td>
<td>Noise-contrastive estimation (NCE)</td>
</tr>

<tr>
<td>2012</td>
<td>Mnih</td>
<td>NCE for training neural probabilistic language models (NPLMs)</td>
</tr>

<tr>
<td>2012</td>
<td>Pascanu</td>
<td>Avoiding exploding gradient problem by gradient clipping</td>
</tr>

<tr>
<td>2013</td>
<td>Mikolov</td>
<td>Negative sampling instead of hierarchical softmax</td>
</tr>

<tr>
<td>2013</td>
<td>Sutskever</td>
<td>Stochastic gradient descent (SGD) with momentum</td>
</tr>

<tr>
<td>2013</td>
<td>Graves</td>
<td>Deep LSTM RNNs (Stacked LSTM)</td>
</tr>

<tr>
<td>2014</td>
<td>Cho</td>
<td>Gated recurrent units</td>
</tr>

<tr>
<td>2015</td>
<td>Zaremba</td>
<td>Dropout for reducing Overfitting</td>
</tr>

<tr>
<td>2015</td>
<td>Mikolov</td>
<td>Structurally constrained recurrent network (SCRN) to enhance learning longer memory for vanishing gradient problem</td>
</tr>

<tr>
<td>2015</td>
<td>Visin</td>
<td>ReNet: A RNN-based alternative to convolutional neural networks</td>
</tr>

<tr>
<td>2015</td>
<td>Gregor</td>
<td>DRAW: Deep recurrent attentive writer</td>
</tr>

<tr>
<td>2015</td>
<td>Kalchbrenner</td>
<td>Grid long-short term memory</td>
</tr>

<tr>
<td>2015</td>
<td>Srivastava</td>
<td>Highway network</td>
</tr>

<tr>
<td>2017</td>
<td>Jing</td>
<td>Gated orthogonal recurrent units</td>
</tr>
</tbody>
</table>

<h2 id="rnn">RNN</h2>

<h3 id="网络结构">网络结构</h3>

<p>不同于传统的前馈神经网络接受特定的输入得到输出，RNN 由人工神经元和一个或多个反馈循环构成，如下图所示：</p>

<p><img src="/images/cn/2018-09-21-rnn/rnn-loops.png" alt="RNN-Loops" /></p>

<p>其中，<code>$\boldsymbol{x}_t$</code> 为输入层，<code>$\boldsymbol{h}_t$</code> 为带有循环的隐含层，<code>$\boldsymbol{y}_t$</code> 为输出层。其中隐含层包含一个循环，为了便于理解我们将循环进行展开，展开后的网络结构如下图所示：</p>

<p><img src="/images/cn/2018-09-21-rnn/rnn-loops-unrolled.png" alt="RNN-Loops-Unrolled" /></p>

<p>对于展开后的网络结构，其输入为一个时间序列 <code>$\left\{\dotsc, \boldsymbol{x}_{t-1}, \boldsymbol{x}_t, \boldsymbol{x}_{t+1}, \dotsc\right\}$</code>，其中 <code>$\boldsymbol{x}_t \in \mathbb{R}^n$</code>，<code>$n$</code> 为输入层神经元个数。相应的隐含层为 <code>$\left\{\dotsc, \boldsymbol{h}_{t-1}, \boldsymbol{h}_t, \boldsymbol{h}_{t+1}, \dotsc\right\}$</code>，其中 <code>$\boldsymbol{h}_t \in \mathbb{R}^m$</code>，<code>$m$</code> 为隐含层神经元个数。隐含层节点使用较小的非零数据进行初始化可以提升整体的性能和网络的稳定性 <sup class="footnote-ref" id="fnref:sutskever2013on"><a href="#fn:sutskever2013on">2</a></sup>。隐含层定义了整个系统的状态空间 (state space)，或称之为 memory <sup class="footnote-ref" id="fnref:salehinejad2017recent"><a href="#fn:salehinejad2017recent">1</a></sup>：</p>

<p><code>$$
\boldsymbol{h}_t = f_H \left(\boldsymbol{o}_t\right)
$$</code></p>

<p>其中</p>

<p><code>$$
\boldsymbol{o}_t = \boldsymbol{W}_{IH} \boldsymbol{x}_t + \boldsymbol{W}_{HH} \boldsymbol{h}_{t-1} + \boldsymbol{b}_h
$$</code></p>

<p><code>$f_H \left(\cdot\right)$</code> 为隐含层的激活函数，<code>$\boldsymbol{b}_h$</code> 为隐含层的偏置向量。对应的输出层为 <code>$\left\{\dotsc, \boldsymbol{y}_{t-1}, \boldsymbol{y}_t, \boldsymbol{y}_{t+1}, \dotsc\right\}$</code>，其中 <code>$\boldsymbol{y}_t \in \mathbb{R}^p$</code>，<code>$p$</code> 为输出层神经元个数。则：</p>

<p><code>$$
\boldsymbol{y}_t = f_O \left(\boldsymbol{W}_{HO} \boldsymbol{h}_t + \boldsymbol{b}_o\right)
$$</code></p>

<p>其中 <code>$f_O \left(\cdot\right)$</code> 为隐含层的激活函数，<code>$\boldsymbol{b}_o$</code> 为隐含层的偏置向量。</p>

<p>在 RNN 中常用的激活函数为双曲正切函数：</p>

<p><code>$$
\tanh \left(x\right) = \dfrac{e^{2x} - 1}{e^{2x} + 1}
$$</code></p>

<p>Tanh 函数实际上是 Sigmoid 函数的缩放：</p>

<p><code>$$
\sigma \left(x\right) = \dfrac{1}{1 + e^{-x}} = \dfrac{\tanh \left(x / 2\right) + 1}{2}
$$</code></p>

<h3 id="梯度弥散和梯度爆炸">梯度弥散和梯度爆炸</h3>

<p>原始 RNN 存在的严重的问题就是<strong>梯度弥散 (Vanishing Gradients)</strong> 和<strong>梯度爆炸 (Exploding Gradients)</strong>。我们以时间序列中的 3 个时间点 <code>$t = 1, 2, 3$</code> 为例进行说明，首先假设神经元在前向传导过程中没有激活函数，则有：</p>

<p><code>$$
\begin{equation}
\begin{split}
&amp;\boldsymbol{h}_1 = \boldsymbol{W}_{IH} \boldsymbol{x}_1 + \boldsymbol{W}_{HH} \boldsymbol{h}_0 + \boldsymbol{b}_h, &amp;\boldsymbol{y}_1 = \boldsymbol{W}_{HO} \boldsymbol{h}_1 + \boldsymbol{b}_o \\
&amp;\boldsymbol{h}_2 = \boldsymbol{W}_{IH} \boldsymbol{x}_2 + \boldsymbol{W}_{HH} \boldsymbol{h}_1 + \boldsymbol{b}_h, &amp;\boldsymbol{y}_2 = \boldsymbol{W}_{HO} \boldsymbol{h}_2 + \boldsymbol{b}_o \\
&amp;\boldsymbol{h}_3 = \boldsymbol{W}_{IH} \boldsymbol{x}_3 + \boldsymbol{W}_{HH} \boldsymbol{h}_2 + \boldsymbol{b}_h, &amp;\boldsymbol{y}_3 = \boldsymbol{W}_{HO} \boldsymbol{h}_3 + \boldsymbol{b}_o
\end{split}
\end{equation}
$$</code></p>

<p>在对于一个序列训练的损失函数为：</p>

<p><code>$$
\mathcal{L} \left(\boldsymbol{y}, \boldsymbol{\hat{y}}\right) = \sum_{t=0}^{T}{\mathcal{L}_t \left(\boldsymbol{y_t}, \boldsymbol{\hat{y}_t}\right)}
$$</code></p>

<p>其中 <code>$\mathcal{L}_t \left(\boldsymbol{y_t}, \boldsymbol{\hat{y}_t}\right)$</code> 为 <code>$t$</code> 时刻的损失。我们利用 <code>$t = 3$</code> 时刻的损失对 <code>$\boldsymbol{W}_{IH}, \boldsymbol{W}_{HH}, \boldsymbol{W}_{HO}$</code> 求偏导，有：</p>

<p><code>$$
\begin{equation}
\begin{split}
\dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{W}_{HO}} &amp;= \dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{y}_3} \dfrac{\partial \boldsymbol{y}_3}{\partial \boldsymbol{W}_{HO}} \\
\dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{W}_{IH}} &amp;= \dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{y}_3} \dfrac{\partial \boldsymbol{y}_3}{\partial \boldsymbol{h}_3} \dfrac{\partial \boldsymbol{h}_3}{\partial \boldsymbol{W}_{IH}} + \dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{y}_3} \dfrac{\partial \boldsymbol{y}_3}{\partial \boldsymbol{h}_3} \dfrac{\partial \boldsymbol{h}_3}{\partial \boldsymbol{h}_2} \dfrac{\partial \boldsymbol{h}_2}{\partial \boldsymbol{W}_{IH}} + \dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{y}_3} \dfrac{\partial \boldsymbol{y}_3}{\partial \boldsymbol{h}_3} \dfrac{\partial \boldsymbol{h}_3}{\partial \boldsymbol{h}_2} \dfrac{\partial \boldsymbol{h}_2}{\partial \boldsymbol{h}_1} \dfrac{\partial \boldsymbol{h}_1}{\partial \boldsymbol{W}_{IH}} \\
\dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{W}_{HH}} &amp;= \dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{y}_3} \dfrac{\partial \boldsymbol{y}_3}{\partial \boldsymbol{h}_3} \dfrac{\partial \boldsymbol{h}_3}{\partial \boldsymbol{W}_{HH}} + \dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{y}_3} \dfrac{\partial \boldsymbol{y}_3}{\partial \boldsymbol{h}_3} \dfrac{\partial \boldsymbol{h}_3}{\partial \boldsymbol{h}_2} \dfrac{\partial \boldsymbol{h}_2}{\partial \boldsymbol{W}_{HH}} + \dfrac{\partial \mathcal{L}_3}{\partial \boldsymbol{y}_3} \dfrac{\partial \boldsymbol{y}_3}{\partial \boldsymbol{h}_3} \dfrac{\partial \boldsymbol{h}_3}{\partial \boldsymbol{h}_2} \dfrac{\partial \boldsymbol{h}_2}{\partial \boldsymbol{h}_1} \dfrac{\partial \boldsymbol{h}_1}{\partial \boldsymbol{W}_{HH}}
\end{split}
\end{equation}
$$</code></p>

<p>因此，不难得出对于任意时刻 <code>$t$</code>，<code>$\boldsymbol{W}_{IH}, \boldsymbol{W}_{HH}$</code> 的偏导为：</p>

<p><code>$$
\dfrac{\partial \mathcal{L}_t}{\partial \boldsymbol{W}_{IH}} = \sum_{k=0}^{t}{\dfrac{\partial \mathcal{L}_t}{\partial \boldsymbol{y}_t} \dfrac{\partial \boldsymbol{y}_t}{\partial \boldsymbol{h}_t} \left(\prod_{j=k+1}^{t}{\dfrac{\partial \boldsymbol{h}_j}{\partial \boldsymbol{h}_{j-1}}}\right) \dfrac{\partial \boldsymbol{h}_k}{\partial \boldsymbol{W}_{IH}}}
$$</code></p>

<p><code>$\dfrac{\partial \mathcal{L}_t}{\partial \boldsymbol{W}_{HH}}$</code> 同理可得。对于 <code>$\dfrac{\partial \mathcal{L}_t}{\partial \boldsymbol{W}_{HH}}$</code>，在存在激活函数的情况下，有：</p>

<p><code>$$
\prod_{j=k+1}^{t}{\dfrac{\partial \boldsymbol{h}_j}{\partial \boldsymbol{h}_{j-1}}} = \prod_{j=k+1}^{t}{f'_H \left(h_{j-1}\right) \boldsymbol{W}_{HH}}
$$</code></p>

<p>假设激活函数为 <code>$\tanh$</code>，下图刻画了 <code>$\tanh$</code> 函数及其导数的函数取值范围：</p>

<p><img src="/images/cn/2018-09-21-rnn/tanh-function.png" alt="Tanh-Function" /></p>

<p>可得，<code>$0 \leq \tanh' \leq 1$</code>，同时当且仅当 <code>$x = 0$</code> 时，<code>$\tanh' \left(x\right) = 1$</code>。因此：</p>

<ol>
<li>当 <code>$t$</code> 较大时，<code>$\prod_{j=k+1}^{t}{f'_H \left(h_{j-1}\right) \boldsymbol{W}_{HH}}$</code> 趋近于 0，则会产生<strong>梯度弥散</strong>问题。</li>
<li>当 <code>$\boldsymbol{W}_{HH}$</code> 较大时，<code>$\prod_{j=k+1}^{t}{f'_H \left(h_{j-1}\right) \boldsymbol{W}_{HH}}$</code> 趋近于无穷，则会产生<strong>梯度爆炸</strong>问题。</li>
</ol>

<h3 id="长期依赖问题">长期依赖问题</h3>

<p>RNN 隐藏节点以循环结构形成记忆，每一时刻的隐藏层的状态取决于它的过去状态，这种结构使得 RNN 可以保存、记住和处理长时期的过去复杂信号。但有的时候，我们仅需利用最近的信息来处理当前的任务。例如：考虑一个用于利用之前的文字预测后续文字的语言模型，如果我们想预测 “the clouds are in the <strong>sky</strong>” 中的最后一个词，我们不需要太远的上下信息，很显然这个词就应该是 <strong>sky</strong>。在这个情况下，待预测位置与相关的信息之间的间隔较小，RNN 可以有效的利用过去的信息。</p>

<p><img src="/images/cn/2018-09-21-rnn/rnn-long-term-dependencies-short.png" alt="RNN-Long-Term-Dependencies-Short" /></p>

<p>但也有很多的情况需要更多的上下文信息，考虑需要预测的文本为 “I grew up in France &hellip; I speak fluent <strong>French</strong>”。较近的信息表明待预测的位置应该是一种语言，但想确定具体是哪种语言需要更远位置的“在法国长大”的背景信息。理论上 RNN 有能力处理这种<strong>长期依赖</strong>，但在实践中 RNN 却很难解决这个问题 <sup class="footnote-ref" id="fnref:bengio1994learning"><a href="#fn:bengio1994learning">3</a></sup>。</p>

<p><img src="/images/cn/2018-09-21-rnn/rnn-long-term-dependencies-long.png" alt="RNN-Long-Term-Dependencies-Long" /></p>

<h2 id="lstm">LSTM</h2>

<h3 id="lstm-网络结构">LSTM 网络结构</h3>

<p>长短时记忆网络 (Long Short Term Memroy, LSTM) 是由 Hochreiter 和 Schmidhuber <sup class="footnote-ref" id="fnref:hochreiter1997long"><a href="#fn:hochreiter1997long">4</a></sup> 提出一种特殊的 RNN。LSTM 的目的就是为了解决长期依赖问题，记住长时间的信息是 LSTM 的基本功能。</p>

<p>所有的循环神经网络都是由重复的模块构成的一个链条。在标准的 RNN 中，这个重复的模块的结构比较简单，仅包含一个激活函数为 <code>$\tanh$</code> 的隐含层，如下图所示：</p>

<p><img src="/images/cn/2018-09-21-rnn/rnn.png" alt="RNN" /></p>

<p>LSTM 也是类似的链条状结构，但其重复的模块的内部结构不同。模块内部并不是一个隐含层，而是四个，并且以一种特殊的方式进行交互，如下图所示：</p>

<p><img src="/images/cn/2018-09-21-rnn/lstm.png" alt="LSTM" /></p>

<p>下面我们将一步一步的介绍 LSTM 单元 (cell) 的具体工作原理，在之前我们先对使用到的符号进行简单的说明，如下图所示：</p>

<p><img src="/images/cn/2018-09-21-rnn/lstm-operations-symbols.png" alt="LSTM-Operations-Symbols" /></p>

<p>其中，每条线都包含一个从输出节点到其他节点的整个向量，粉红色的圆圈表示逐元素的操作，黄色的矩形为学习到的神经网络层，线条的合并表示连接，线条的分叉表示内容的复制并转移到不同位置。</p>

<h3 id="lstm-单元状态和门控机制">LSTM 单元状态和门控机制</h3>

<p>LSTM 的关键为单元的状态 (cell state)，即下图中顶部水平穿过单元的直线。单元的状态像是一条传送带，其直接运行在整个链条上，同时仅包含少量的线性操作。因此，信息可以很容易得传递下去并保持不变。</p>

<p><img src="/images/cn/2018-09-21-rnn/lstm-cell-state.png" alt="LSTM-Cell-State" /></p>

<p>LSTM 具有向单元状态添加或删除信息的能力，这种能力被由一种称之为“门” (gates) 的结构所控制。门是一种可选择性的让信息通过的组件，其由一层以 Sigmoid 为激活函数的网络层和一个逐元素相乘操作构成的，如下图所示：</p>

<p><img src="/images/cn/2018-09-21-rnn/lstm-pointwise-operation.png" alt="LSTM-Pointwise-Operation" /></p>

<p>Sigmoid 层的输出值介于 0 和 1 之间，代表了所允许通过的数据量。0 表示不允许任何数据通过，1 表示允许所有数据通过。一个 LSTM 单元包含 3 个门用于控制单元的状态。</p>

<h3 id="lstm-工作步骤">LSTM 工作步骤</h3>

<p>LSTM 的第一步是要决定从单元状态中所<strong>忘记</strong>的信息，这一步是通过一个称之为“<strong>遗忘门 (forget gate)</strong>”的 Sigmoid 网络层控制。该层以上一时刻隐含层的输出 <code>$h_{t-1}$</code> 和当前这个时刻的输入 <code>$x_t$</code> 作为输入，输出为一个介于 0 和 1 之间的值，1 代表全部保留，0 代表全部丢弃。回到之前的语言模型，单元状态需要包含主语的性别信息以便选择正确的代词。但当遇见一个新的主语后，则需要忘记之前主语的性别信息。</p>

<p><img src="/images/cn/2018-09-21-rnn/lstm-cell-forget-gate.png" alt="LSTM-Cell-Forget-Gate" /></p>

<p><code>$$
f_t = \sigma \left(W_f \cdot \left[h_{t-1}, x_t\right] + b_f\right)
$$</code></p>

<p>第二步我们需要决定要在单元状态中存储什么样的新信息，这包含两个部分。第一部分为一个称之为“<strong>输入门 (input gate)</strong>” 的 Sigmoid 网络层，其决定更新那些数据。第二部分为一个 Tanh 网络层，其将产生一个新的候选值向量 <code>$\tilde{C}_t$</code> 并用于添加到单元状态中。之后会将两者进行整合，并对单元状态进行更新。在我们的语言模型中，我们希望将新主语的性别信息添加到单元状态中并替代需要忘记的旧主语的性别信息。</p>

<p><img src="/images/cn/2018-09-21-rnn/lstm-cell-input-gate.png" alt="LSTM-Cell-Input-Gate" /></p>

<p><code>$$
\begin{equation}
\begin{split}
i_t &amp;= \sigma \left(W_i \cdot \left[h_{t-1}, x_t\right] + b_i\right) \\
\tilde{C}_t &amp;= \tanh \left(W_C \cdot \left[h_{t-1}, x_t\right] + b_C\right)
\end{split}
\end{equation}
$$</code></p>

<p>接下来需要将旧的单元状态 <code>$C_{t-1}$</code> 更新为 <code>$C_t$</code>。我们将旧的单元状态乘以 <code>$f_t$</code> 以控制需要忘记多少之前旧的信息，再加上 <code>$i_t \odot \tilde{C}_t$</code> 用于控制单元状态的更新。在我们的语言模型中，该操作真正实现了我们对与之前主语性别信息的遗忘和对新信息的增加。</p>

<p><img src="/images/cn/2018-09-21-rnn/lstm-cell-state-update.png" alt="LSTM-Cell-State-Update" /></p>

<p><code>$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
$$</code></p>

<p>最后我们需要确定单元的输出，该输出将基于单元的状态，但为一个过滤版本。首先我们利用一个 Sigmoid 网络层来确定单元状态的输出，其次我们对单元状态进行 <code>$\tanh$</code> 操作 (将其值缩放到 -1 和 1 之间) 并与之前 Sigmoid 层的输出相乘，最终得到需要输出的信息。</p>

<p><img src="/images/cn/2018-09-21-rnn/lstm-cell-output-gate.png" alt="LSTM-Cell-Output-Gate" /></p>

<p><code>$$
\begin{equation}
\begin{split}
o_t &amp;= \sigma \left(W_o \cdot \left[h_{t-1}, x_t\right] + b_o\right) \\
h_t &amp;= o_t \odot \tanh \left(C_t\right)
\end{split}
\end{equation}
$$</code></p>

<h3 id="lstm-变种">LSTM 变种</h3>

<p>上文中介绍的基础的 LSTM 模型，事实上不同学者对 LSTM 的结构进行了或多或少的改变，其中一个比较有名的变种是由 Gers 和 Schmidhuber 提出的 <sup class="footnote-ref" id="fnref:gers2000recurrent"><a href="#fn:gers2000recurrent">5</a></sup>。其添加了一种“窥视孔连接 (peephole connections)”，这使得每一个门结构都能够窥视到单元的状态。</p>

<p><img src="/images/cn/2018-09-21-rnn/peephole-cell.png" alt="Peephole-Cell" /></p>

<p><code>$$
\begin{equation}
\begin{split}
f_t &amp;= \sigma \left(W_f \cdot \left[\boldsymbol{C_{t-1}}, h_{t-1}, x_t\right] + b_f\right) \\
i_t &amp;= \sigma \left(W_i \cdot \left[\boldsymbol{C_{t-1}}, h_{t-1}, x_t\right] + b_i\right) \\
o_t &amp;= \sigma \left(W_o \cdot \left[\boldsymbol{C_t}, h_{t-1}, x_t\right] + b_o\right)
\end{split}
\end{equation}
$$</code></p>

<p>另一个变种是使用了成对的遗忘门和输入门。不同于一般的 LSTM 中分别确定需要遗忘和新添加的信息，成对的遗忘门和输入门仅在需要添加新输入是才会忘记部分信息，同理仅在需要忘记信息时才会添加新的输入。</p>

<p><img src="/images/cn/2018-09-21-rnn/cfig-cell.png" alt="CFIG-Cell" /></p>

<p><code>$$
C_t = f_t \odot C_{t-1} + \boldsymbol{\left(1 - f_t\right)} \odot \tilde{C}_t
$$</code></p>

<p>另外一个比较有名的变种为 Cho 等人提出的 Gated Recurrent Unit (GRU) <sup class="footnote-ref" id="fnref:cho2014learning"><a href="#fn:cho2014learning">6</a></sup>，单元结构如下：</p>

<p><img src="/images/cn/2018-09-21-rnn/gru-cell.png" alt="GRU-Cell" /></p>

<p>GRU 将遗忘门和输入门整个成一层，称之为“<strong>更新门 (update gate)</strong>”，同时配以一个“<strong>重置门 (reset gate)</strong>”。具体的计算过程如下：</p>

<p>首先计算更新门 <code>$z_t$</code> 和重置门 <code>$r_t$</code>：</p>

<p><code>$$
\begin{equation}
\begin{split}
z_t &amp;= \sigma \left(W_z \cdot \left[h_{t-1}, x_t\right]\right) \\
r_t &amp;= \sigma \left(W_r \cdot \left[h_{t-1}, x_t\right]\right)
\end{split}
\end{equation}
$$</code></p>

<p>其次计算候选隐含层 (candidate hidden layer) <code>$\tilde{h}_t$</code>，与 LSTM 中计算 <code>$\tilde{C}_t$</code> 类似，其中 <code>$r_t$</code> 用于控制保留多少之前的信息：</p>

<p><code>$$
\tilde{h}_t = \tanh \left(W \cdot \left[r_t \odot h_{t-1}, x_t\right]\right)
$$</code></p>

<p>最后计算需要从之前的隐含层 <code>$h_{t-1}$</code> 遗忘多少信息，同时加入多少新的信息 <code>$\tilde{h}_t$</code>，<code>$z_t$</code> 用于控制这个比例：</p>

<p><code>$$
h_t = \left(1 - z_t\right) \odot h_{t-1} + z_t \odot \tilde{h}_t
$$</code></p>

<p>因此，对于短距离依赖的单元重置门的值较大，对于长距离依赖的单元更新门的值较大。如果 <code>$r_t = 1$</code> 并且 <code>$z_t = 0$</code>，则 GRU 退化为一个标准的 RNN。</p>

<p>除此之外还有大量的 LSTM 变种，Greff 等人 <sup class="footnote-ref" id="fnref:greff2017lstm"><a href="#fn:greff2017lstm">7</a></sup> 对一些常见的变种进行了比较，Jozefowicz 等人 <sup class="footnote-ref" id="fnref:jozefowicz2015empirical"><a href="#fn:jozefowicz2015empirical">8</a></sup> 测试了大量的 RNN 结构在不同任务上的表现。</p>

<h2 id="扩展与应用">扩展与应用</h2>

<p>循环神经网络在序列建模上有着天然的优势，其在自然语言处理，包括：语言建模，语音识别，机器翻译，对话与QA，文本生成等；计算视觉，包括：目标识别，视觉追踪，图像生成等；以及一些综合场景，包括：图像标题生成，视频字幕生成等，多个领域均有不错的表现，有代表性的论文请参见 <a href="https://github.com/kjw0612/awesome-rnn" rel="noreferrer" target="_blank">awesome-rnn</a>。</p>

<p>Google 的 <a href="https://magenta.tensorflow.org/" rel="noreferrer" target="_blank">Magenta</a> 是一项利用机器学习创作艺术和音乐的研究，其中也包含了大量利用 RNN 相关模型构建的有趣项目。<a href="https://magenta.tensorflow.org/sketch-rnn-demo" rel="noreferrer" target="_blank">SketchRNN</a> 是由 Ha 等人 <sup class="footnote-ref" id="fnref:ha2017neural"><a href="#fn:ha2017neural">9</a></sup> 提出了一种能够根据用户描绘的一些简单图形自动完成后续绘画的 RNN 网络。</p>

<p><img src="/images/cn/2018-09-21-rnn/sketch-rnn-demo.gif" alt="SketchRNN-Demo" /></p>

<p><a href="https://magenta.tensorflow.org/performance-rnn-browser" rel="noreferrer" target="_blank">Performance RNN</a> 是由 Ian
等人 <sup class="footnote-ref" id="fnref:ian2017performance"><a href="#fn:ian2017performance">10</a></sup> 提出了一种基于时间和动态因素生成复合音乐的 LSTM 网络。</p>

<p><img src="/images/cn/2018-09-21-rnn/performance-rnn-demo.gif" alt="Performance-RNN-Demo" /></p>

<p>更多有趣的作品请参见 Megenta 的 <a href="https://magenta.tensorflow.org/demos" rel="noreferrer" target="_blank">Demos</a> 页面。</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:salehinejad2017recent">Salehinejad, H., Sankar, S., Barfett, J., Colak, E., &amp; Valaee, S. (2017). Recent Advances in Recurrent Neural Networks. <em>arXiv preprint arXiv:1801.01078.</em>
 <a class="footnote-return" href="#fnref:salehinejad2017recent">↩</a></li>
<li id="fn:sutskever2013on">Sutskever, I., Martens, J., Dahl, G., &amp; Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In <em>International Conference on Machine Learning</em> (pp. 1139–1147).
 <a class="footnote-return" href="#fnref:sutskever2013on">↩</a></li>
<li id="fn:bengio1994learning">Bengio, Y., Simard, P., &amp; Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. <em>IEEE Transactions on Neural Networks, 5(2)</em>, 157–166.
 <a class="footnote-return" href="#fnref:bengio1994learning">↩</a></li>
<li id="fn:hochreiter1997long">Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780.
 <a class="footnote-return" href="#fnref:hochreiter1997long">↩</a></li>
<li id="fn:gers2000recurrent">Gers, F. A., &amp; Schmidhuber, J. (2000). Recurrent nets that time and count. In <em>Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</em> (Vol. 3, pp. 189–194 vol.3).
 <a class="footnote-return" href="#fnref:gers2000recurrent">↩</a></li>
<li id="fn:cho2014learning">Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> (pp. 1724–1734).
 <a class="footnote-return" href="#fnref:cho2014learning">↩</a></li>
<li id="fn:greff2017lstm">Greff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., &amp; Schmidhuber, J. (2017). LSTM: A Search Space Odyssey. <em>IEEE Transactions on Neural Networks and Learning Systems, 28(10)</em>, 2222–2232.
 <a class="footnote-return" href="#fnref:greff2017lstm">↩</a></li>
<li id="fn:jozefowicz2015empirical">Jozefowicz, R., Zaremba, W., &amp; Sutskever, I. (2015). An Empirical Exploration of Recurrent Network Architectures. In <em>Proceedings of the 32Nd International Conference on International Conference on Machine Learning</em> - Volume 37 (pp. 2342–2350).
 <a class="footnote-return" href="#fnref:jozefowicz2015empirical">↩</a></li>
<li id="fn:ha2017neural">Ha, D., &amp; Eck, D. (2017). A Neural Representation of Sketch Drawings. <em>arXiv preprint arXiv:1704.03477</em>
 <a class="footnote-return" href="#fnref:ha2017neural">↩</a></li>
<li id="fn:ian2017performance">Ian S., &amp; Sageev O. Performance RNN: Generating Music with Expressive Timing and Dynamics. Magenta Blog, 2017. <a href="https://magenta.tensorflow.org/performance-rnn" rel="noreferrer" target="_blank">https://magenta.tensorflow.org/performance-rnn</a>
 <a class="footnote-return" href="#fnref:ian2017performance">↩</a></li>
</ol>
</div>



<link rel="stylesheet" href="/css/donate.css" />


<div class="donate">
  <div class="donate-header"></div>
  <div class="donate-slug" id="donate-slug">rnn</div>
  <button class="donate-button">赞 赏</button>
  <div class="donate-footer">「真诚赞赏，手留余香」</div>
</div>
<div class="donate-modal-wrapper">
  <div class="donate-modal">
    <div class="donate-box">
      <div class="donate-box-content">
        <div class="donate-box-content-inner">
          <div class="donate-box-header">「真诚赞赏，手留余香」</div>
          <div class="donate-box-body">
            <div class="donate-box-money">
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-2" data-v="2" data-unchecked="￥ 2" data-checked="2 元">￥ 2</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-5" data-v="5" data-unchecked="￥ 5" data-checked="5 元">￥ 5</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-10" data-v="10" data-unchecked="￥ 10" data-checked="10 元">￥ 10</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-50" data-v="50" data-unchecked="￥ 50" data-checked="50 元">￥ 50</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-100" data-v="100" data-unchecked="￥ 100" data-checked="100 元">￥ 100</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-custom" data-v="custom" data-unchecked="任意金额" data-checked="任意金额">任意金额</button>
            </div>
            <div class="donate-box-pay">
              <img class="donate-box-pay-qrcode" id="donate-box-pay-qrcode" src=""/>
            </div>
          </div>
          <div class="donate-box-footer">
            <div class="donate-box-pay-method donate-box-pay-method-checked" data-v="wechat-pay">
              <img class="donate-box-pay-method-image" id="donate-box-pay-method-image-wechat-pay" src=""/>
            </div>
            <div class="donate-box-pay-method" data-v="alipay">
              <img class="donate-box-pay-method-image"  id="donate-box-pay-method-image-alipay" src=""/>
            </div>
          </div>
        </div>
      </div>
    </div>
    <button type="button" class="donate-box-close-button">
      <svg class="donate-box-close-button-icon" fill="#fff" viewBox="0 0 24 24" width="24" height="24"><path d="M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z" fill-rule="evenodd"></path></svg>
    </button>
  </div>
</div>

<script type="text/javascript" src="/js/donate.js"></script>
</script>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/cn/2018/09/tour-of-thailand/">泰国之行 (Tour of Thailand)</a></span>
  <span class="nav-next"><a href="/cn/2018/10/word-embeddings/">词向量 (Word Embeddings)</a> &rarr;</span>
</nav>
<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/cn\/2018\/09\/tour-of-thailand\/';
    
  } else if (e.which == 39) {  
    
    url = '\/cn\/2018\/10\/word-embeddings\/';
    
  }
  if (url) window.location = url;
});
</script>




<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-2608165017777396"
     data-ad-slot="1261604535"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>





<section class="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
    this.page.url = "https:\/\/zeqiang.fun" + location.pathname;
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var disqus_js = '//Zeqiang.disqus.com/embed.js';
    var d = document, s = d.createElement('script');
    s.src = disqus_js; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    var t = d.getElementById('disqus_thread');
    var b = false, l = function(scroll) {
      if (b) return;
      (d.head || d.body).appendChild(s); b = true;
      if (scroll) t.scrollIntoView();
    }
    s.onerror = function(e) {
      if (sessionStorage.getItem('failure-note')) return;
      t.innerText = 'Sorry, but you cannot make comments because Disqus failed to load for some reason. It is known to be blocked in China. If you are sure it is not blocked in your region, please refresh the page. 中国大陆地区读者需要翻墙才能发表评论。';
      sessionStorage.setItem('failure-note', true);
    };
    
    if (location.hash.match(/^#comment-[0-9]+$/)) return l(true);
    var c = function() {
      if (b) return;
      var rect = t.getBoundingClientRect();
      if (rect.top < window.innerHeight && rect.bottom >= 0) l();
    };
    window.addEventListener('load', c);
    d.addEventListener('scroll', c);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>




<script async src="/js/fix-toc.js"></script>
<script async src="/js/center-img.js"></script>
<script async src="/js/right-quote.js"></script>
<script async src="/js/fix-footnote.js"></script>
<script async src="/js/external-link.js"></script>
<script async src="/js/alt-title.js"></script>
<script src="/js/no-highlight.js"></script>
<script src="/js/math-code.js"></script>


<script>
window.MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js" crossorigin></script>







<script async src="/js/load-typekit.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lazysizes/5.3.2/lazysizes.min.js"></script>

<script src="//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js"></script>
<script>
addBackToTop({
  diameter: 48
})
</script>



  <hr>
  <div class="copyright no-border-bottom">
    <div class="copyright-author-year">
      <span>&copy; 2017-2021 Leo Van</span>
    </div>
    <div class="copyright-links">
      <a href="https://github.com/leovan" rel="noreferrer" target="_blank">Github</a>
      <span> · </span>
      <a href="https://orcid.org/0000-0002-9556-7821" rel="noreferrer" target="_blank">ORCID</a>
      <span> · </span>
      <span>I am Mr. Black.</span>
    </div>
  </div>
  </footer>
  </article>
  </body>
</html>


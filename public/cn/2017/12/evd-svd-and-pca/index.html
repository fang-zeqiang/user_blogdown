<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

        <title>特征值分解，奇异值分解和主成份分析 (EVD, SVD and PCA) - Zeqiang Fang | 方泽强</title>

    <meta name="referrer" content="no-referrer">
    
    <meta property="og:title" content="特征值分解，奇异值分解和主成份分析 (EVD, SVD and PCA) - Zeqiang Fang | 方泽强">
    <meta name="description" property="og:description" content="$\renewcommand{\diag}{\operatorname{diag}}\renewcommand{\cov}{\operatorname{cov}}$ 准备知识 向量与基 首先，定义 $\boldsymbol{\alpha}$ 为列向量，则维度相同的两个向量 $\boldsymbol{\alpha}, \boldsymbol{\beta}$ 的内积可以表示为： $$\boldsymbol{\alpha} \cdot \boldsymbol{\beta} = \boldsymbol{\alpha}^T \boldsymbol{\beta} = \sum_{i=1}^{n}{\alpha_i b_i}$$ 后续为了便于理解，我们以二维向量为例，则">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Zeqiang Fang | 方泽强">
    <meta property="og:url" content="https://leovan.me/cn/2017/12/evd-svd-and-pca/">

    
    
    
    
    <meta name="author" property="article:author" content="范叶亮">
    
    
    
    <meta name="date" property="article:published_time" content="2017-12-11" scheme="YYYY-MM-DD">
    
    
    <meta name="date" property="article:modified_time" content="2017-12-11" scheme="YYYY-MM-DD">
    

    
    <meta name="keywords" property="article:tag" content ="EVD,SVD,PCA,降维,Dimensionality Reduction">
    
    
    <meta name="theme-color" content="#0d0d0d">
    
    <link rel="icon" type="image/png" sizes="16x16" href="/images/web/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/web/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/images/web/favicon-48x48.png">
    <link rel="icon" type="image/png" sizes="62x62" href="/images/web/favicon-62x62.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/images/web/favicon-192x192.png">
    <link rel="apple-touch-icon" size="192x192" href="/images/web/icon-192x192.png">
    <link rel="manifest" href="/manifest.json">
        
    

    

    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://leovan.me/cn"
        },
        "name": "特征值分解，奇异值分解和主成份分析 (EVD, SVD and PCA)",
        "headline": "特征值分解，奇异值分解和主成份分析 (EVD, SVD and PCA)",
        "description" : "$\\renewcommand{\\diag}{\\operatorname{diag}}\\renewcommand{\\cov}{\\operatorname{cov}}$ 准备知识 向量与基 首先，定义 $\\boldsymbol{\\alpha}$ 为列向量，则维度相同的两个向量 $\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}$ 的内积可以表示为： $$\\boldsymbol{\\alpha} \\cdot \\boldsymbol{\\beta} = \\boldsymbol{\\alpha}^T \\boldsymbol{\\beta} = \\sum_{i=1}^{n}{\\alpha_i b_i}$$ 后续为了便于理解，我们以二维向量为例，则",
        "genre": [
            "机器学习"
        ],
        "datePublished": "2017-12-11",
        "dateModified": "2017-12-11",
        "wordCount": "2473",
        "keywords": [
            "EVD", "SVD", "PCA", "降维", "Dimensionality Reduction"
        ],
        "image": [
            "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/pca-projection.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/vector-inner-product-and-projection.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/vector-bases.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/vector-change-of-bases.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-1.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-2.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-3.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-4.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-5.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/lena-std.png", "https://leovan.me/images/cn/2017-12-11-evd-svd-and-pca/lena-reconstruction.png"
        ],
        "author": {
            "@type": "Person",
            "name": "范叶亮"
        },
        "publisher": {
            "@type": "Organization",
            "name": "范叶亮",
            "logo": {
                "@type": "ImageObject",
                "url": "https://leovan.me/images/web/publisher-logo.png"
            }
        },
        "url": "https://leovan.me/cn/2017/12/evd-svd-and-pca/"
    }
    </script>
    

    <script src='//cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js'></script>
<script src='//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js'></script>

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/6.5.95/css/materialdesignicons.min.css">





<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css">

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism-.min.css">





<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<link rel="stylesheet" type="text/css" href="/css/fonts.css" />
<link rel="stylesheet" type="text/css" href="/css/style.css">
<link rel="stylesheet" type="text/css" href="/css/light.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" id="dark-mode-style" disabled="disabled">
<link rel="stylesheet" type="text/css" href="/css/icons.css">
<link rel="stylesheet" type="text/css" href="/css/print.css">

  </head>

  
  <body class="cn">
    <header class="masthead">
      

<div class="logo"></div>
<p class="slogan">优雅永不过时</p>
      <nav class="menu">
  <ul>
  
  
  
    
  
  
    <li><a href="/">首页</a></li>
  
    <li><a href="/cn/">博客</a></li>
  
    <li><a href="/categories/">分类</a></li>
  
    <li class="menu-separator"><span>&nbsp;</span></li>
  
    <li><a href="/cn/about/">关于</a></li>
  
    <li><a href="/cn/resume/">简历</a></li>
  
  


<li class="menu-separator"><span>&nbsp;</span></li>

<li><a href="/cn/index.xml" target="_blank" type="application/rss+xml" title="RSS feed">订阅</a></li>

<li><a href="https://github.com/fang-zeqiang/fang-zeqiang.github.io/blob/master/LICENSE" target="_blank" title="Attribution-NonCommercial-ShareAlike 4.0 International">版权</a></li>


  <li class="light-dark-mode no-border-bottom"><a id="light-dark-mode-action"><span id="light-dark-mode-icon" class="mdi mdi-weather-night"></span></a></li>
  </ul>
</nav>

      <script src="/js/toggle-theme.js"></script>
    </header>

    <article class="main">
      <header class="title">
      
<h1>特征值分解，奇异值分解和主成份分析 (EVD, SVD and PCA)</h1>







<h3>范叶亮 / 
2017-12-11</h3>



<h3 class="post-meta">


<strong>分类: </strong>
<a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>




/




<strong>标签: </strong>
<span>EVD</span>, <span>SVD</span>, <span>PCA</span>, <span>降维</span>, <span>Dimensionality Reduction</span>




/


<strong>字数: </strong>
2473
</h3>



<hr>


      </header>






<p><code>$\renewcommand{\diag}{\operatorname{diag}}\renewcommand{\cov}{\operatorname{cov}}$</code></p>

<h2 id="准备知识">准备知识</h2>

<h3 id="向量与基">向量与基</h3>

<p>首先，定义 <code>$\boldsymbol{\alpha}$</code> 为列向量，则维度相同的两个向量 <code>$\boldsymbol{\alpha}, \boldsymbol{\beta}$</code> 的内积可以表示为：</p>

<p><code>$$\boldsymbol{\alpha} \cdot \boldsymbol{\beta} = \boldsymbol{\alpha}^T \boldsymbol{\beta} = \sum_{i=1}^{n}{\alpha_i b_i}$$</code></p>

<p>后续为了便于理解，我们以二维向量为例，则 <code>$\boldsymbol{\alpha} = \left(x_1, y_1\right)^T, \boldsymbol{\beta} = \left(x_2, y_2\right)^T$</code>，在直角座标系中可以两个向量表示如下：</p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/vector-inner-product-and-projection.png" alt="" /></p>

<p>我们从 <code>$A$</code> 点向向量 <code>$\boldsymbol{\beta}$</code> 的方向做一条垂线，交于点 <code>$C$</code>，则称 <code>$OC$</code> 为 <code>$OA$</code> 在 <code>$OB$</code> 方向上的投影。设向量 <code>$\boldsymbol{\alpha}$</code> 和向量 <code>$\boldsymbol{\beta}$</code> 的夹角为 <code>$\theta$</code>，则：</p>

<p><code>$$\cos \left(\theta\right) = \dfrac{\boldsymbol{\alpha} \cdot \boldsymbol{\beta}}{\lvert\boldsymbol{\alpha}\rvert \lvert\boldsymbol{\beta}\rvert}$$</code></p>

<p>其中，<code>$\lvert\boldsymbol{\alpha}\rvert = \sqrt{x_1^2 + y_1^2}$</code>，则 <code>$OC$</code> 的长度为 <code>$\lvert\boldsymbol{\alpha}\rvert \cos\left(\theta\right)$</code>。</p>

<p>在 <code>$n$</code> 维的线性空间 <code>$V$</code> 中，<code>$n$</code> 个线性无关的向量 <code>$\boldsymbol{\epsilon_1, \epsilon_2, ..., \epsilon_n}$</code> 称为 <code>$V$</code> 的一组<strong>基</strong>。则对于 <code>$V$</code> 中的任一向量 <code>$\boldsymbol{\alpha}$</code> 可以由这组基线性表示出来：</p>

<p><code>$$\boldsymbol{\alpha} = x_1 \boldsymbol{\epsilon_1} + x_2 \boldsymbol{\epsilon_2} + ... + x_n \boldsymbol{\epsilon_n}$$</code></p>

<p>则对于向量 <code>$\boldsymbol{\alpha} = \left(3, 2\right)^T$</code>，可以表示为：</p>

<p><code>$$\boldsymbol{\alpha} = 2 \left(1, 0\right)^T + 3 \left(0, 1\right)^T$$</code></p>

<p>其中 <code>$\left(1, 0\right)^T$</code> 和 <code>$\left(0, 1\right)^T$</code> 为二维空间中的一组基。</p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/vector-bases.png" alt="" /></p>

<p>因此，当我们确定好一组基之后，我们仅需利用向量在基上的投影值即可表示对应的向量。一般情况下，我们会选择由坐标轴方向上的单位向量构成的基作为默认的基来表示向量，但我们仍可选择其他的基。例如，我们选择 <code>$\left(-\dfrac{1}{\sqrt{2}}, \dfrac{1}{\sqrt{2}}\right)$</code> 和 <code>$\left(\dfrac{1}{\sqrt{2}}, \dfrac{1}{\sqrt{2}}\right)$</code> 作为一组基，则向量在这组基上的坐标为 <code>$\left(-\dfrac{1}{\sqrt{2}}, \dfrac{5}{\sqrt{2}}\right)$</code>，示例如下：</p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/vector-change-of-bases.png" alt="" /></p>

<h3 id="线性变换">线性变换</h3>

<p>以二维空间为例，定义一个如下矩阵</p>

<p><code>$$
A = \left\lgroup
    \begin{array}{cc}
        a_{11} &amp; a_{12} \\
        a_{21} &amp; a_{22}
    \end{array}
\right\rgroup
$$</code></p>

<p>则对于二维空间中一个向量 <code>$\boldsymbol{\alpha} = \left(x, y\right)^T$</code> ，通过同上述矩阵进行乘法运算，可得</p>

<p><code>$$
\boldsymbol{\alpha'} = A \boldsymbol{\alpha} =
\left\lgroup
    \begin{array}{cc}
        a_{11} &amp; a_{12} \\
        a_{21} &amp; a_{22}
    \end{array}
\right\rgroup
\left\lgroup
    \begin{array}{c}
        x \\
        y
    \end{array}
\right\rgroup = 
\left\lgroup
    \begin{array}{c}
        x' \\
        y'
    \end{array}
\right\rgroup
$$</code></p>

<p>(1) 通过变换将任意一个点 <code>$x$</code> 变成它关于 <code>$x$</code> 轴对称的点 <code>$x'$</code></p>

<p><code>$$
x' =
\left\lgroup
    \begin{array}{cc}
        1 &amp; 0 \\
        0 &amp; -1
    \end{array}
\right\rgroup
\left\lgroup
    \begin{array}{c}
        x \\
        y
    \end{array}
\right\rgroup = 
\left\lgroup
    \begin{array}{c}
        x \\
        -y
    \end{array}
\right\rgroup
$$</code></p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-1.png" alt="" /></p>

<p>(2) 通过变换将任意一个点 <code>$x$</code> 变成它关于 <code>$y = x$</code> 对称的点 <code>$x'$</code></p>

<p><code>$$
x' =
\left\lgroup
    \begin{array}{cc}
        0 &amp; 1 \\
        1 &amp; 0
    \end{array}
\right\rgroup
\left\lgroup
    \begin{array}{c}
        x \\
        y
    \end{array}
\right\rgroup = 
\left\lgroup
    \begin{array}{c}
        y \\
        x
    \end{array}
\right\rgroup
$$</code></p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-2.png" alt="" /></p>

<p>(3) 变换将任意一个点 <code>$x$</code> 变成在它与原点连线上，与原点距离伸缩为 <code>$|\lambda|$</code> 倍的点 <code>$x'$</code></p>

<p><code>$$
x' =
\left\lgroup
    \begin{array}{cc}
        \lambda &amp; 0 \\
        0 &amp; \lambda
    \end{array}
\right\rgroup
\left\lgroup
    \begin{array}{c}
        x \\
        y
    \end{array}
\right\rgroup = 
\left\lgroup
    \begin{array}{c}
        \lambda x \\
        \lambda y
    \end{array}
\right\rgroup
$$</code></p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-3.png" alt="" /></p>

<p>(4) 通过变换将任意一个点 <code>$x$</code> 绕原点旋转了角度 <code>$\theta$</code> 的点 <code>$x'$</code></p>

<p><code>$$
\begin{equation}
\begin{split}
x'&amp; =
\left\lgroup
    \begin{array}{cc}
        \cos \theta &amp; -\sin \theta \\
        \sin \theta &amp; \cos \theta
    \end{array}
\right\rgroup
\left\lgroup
    \begin{array}{c}
        x \\
        y
    \end{array}
\right\rgroup \\
&amp; = 
\left\lgroup
    \begin{array}{cc}
        \cos \theta &amp; -\sin \theta \\
        \sin \theta &amp; \cos \theta
    \end{array}
\right\rgroup
\left\lgroup
    \begin{array}{c}
        r \cos \phi \\
        r \sin \phi
    \end{array}
\right\rgroup \\
&amp; = 
\left\lgroup
    \begin{array}{c}
        r \cos \left(\phi + \theta\right) \\
        r \sin \left(\phi + \theta\right)
    \end{array}
\right\rgroup
\end{split}
\end{equation}
$$</code></p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-4.png" alt="" /></p>

<p>(5) 变换将任意一个点 <code>$x$</code> 变成它在 <code>$x$</code> 轴上 的投影点 <code>$x'$</code></p>

<p><code>$$
x' =
\left\lgroup
    \begin{array}{cc}
        1 &amp; 0 \\
        0 &amp; 0
    \end{array}
\right\rgroup
\left\lgroup
    \begin{array}{c}
        x \\
        y
    \end{array}
\right\rgroup = 
\left\lgroup
    \begin{array}{c}
        x \\
        0
    \end{array}
\right\rgroup
$$</code></p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/vector-linear-transformation-5.png" alt="" /></p>

<h2 id="特征值分解">特征值分解</h2>

<p>设 <code>$A$</code> 是线性空间 <code>$V$</code> 上的一个线性变换，对于一个非零向量 <code>$\boldsymbol{\alpha} = \left(x_1, x_2, ..., x_n\right)^T$</code> 使得</p>

<p><code>$$A \boldsymbol{\alpha} = \lambda \boldsymbol{\alpha}$$</code></p>

<p>则 <code>$\lambda$</code> 称为 <code>$A$</code> 的一个<strong>特征值</strong>，<code>$\boldsymbol{\alpha}$</code> 称为 <code>$A$</code> 的一个<strong>特征向量</strong>。通过</p>

<p><code>$$
\begin{equation}
\begin{split}
A \boldsymbol{\alpha} &amp;= \lambda \boldsymbol{\alpha} \\
A \boldsymbol{\alpha} - \lambda \boldsymbol{\alpha} &amp;= 0 \\
\left(A - \lambda E\right) \boldsymbol{\alpha} &amp;= 0 \\
A - \lambda E &amp;= 0
\end{split}
\end{equation}
$$</code></p>

<p>其中 <code>$E = \diag \left(1, 1, ..., 1\right)$</code> 为单位对角阵，即可求解其特征值，进而求解特征向量。若 <code>$A$</code> 是一个可逆矩阵，则上式可以改写为：</p>

<p><code>$$
A = Q \sum Q^{-1}
$$</code></p>

<p>这样，一个方阵 <code>$A$</code> 就被一组特征值和特征向量表示了。例如，对于如下矩阵进行特征值分解</p>

<p><code>$$
A = \left\lgroup
\begin{array}{cccc}
    3 &amp; -2 &amp; -0.9 &amp; 0 \\
    -2 &amp; 4 &amp; 1 &amp; 0 \\
    0 &amp; 0 &amp; -1 &amp; 0 \\
    -0.5 &amp; -0.5 &amp; 0.1 &amp; 1
\end{array}
\right\rgroup
$$</code></p>

<pre><code class="language-r">A &lt;- matrix(c(3, -2, -0.9, 0,
              -2, 4, 1, 0,
              0, 0, -1, 0,
              -0.5, -0.5, 0.1, 1),
            4, 4, byrow = T)
A_eig &lt;- eigen(A)
print(A_eig)

# eigen() decomposition
# $values
# [1]  5.561553  1.438447  1.000000 -1.000000
# 
# $vectors
#             [,1]       [,2] [,3]        [,4]
# [1,] -0.61530186  0.4176225    0  0.15282144
# [2,]  0.78806410  0.3260698    0 -0.13448286
# [3,]  0.00000000  0.0000000    0  0.97805719
# [4,] -0.01893678 -0.8480979    1 -0.04431822
</code></pre>

<p>则利用特征值和特征向量，可以还原原矩阵</p>

<pre><code class="language-r">A_re &lt;- A_eig$vectors %*%
    diag(A_eig$values) %*%
    solve(A_eig$vectors)
print(A_re)

#      [,1] [,2] [,3] [,4]
# [1,]  3.0 -2.0 -0.9    0
# [2,] -2.0  4.0  1.0    0
# [3,]  0.0  0.0 -1.0    0
# [4,] -0.5 -0.5  0.1    1
</code></pre>

<h2 id="奇异值分解">奇异值分解</h2>

<p>特征值分解针对的是方阵，对于一个 <code>$m*n$</code> 的矩阵是无法进行特征值分解的，这时我们就需要使用奇异值分解来解决这个问题。对于 <code>$m*n$</code> 的矩阵 <code>$A$</code>，可得 <code>$A A^T$</code> 是一个 <code>$m*m$</code> 的方阵，则针对 <code>$A A^T$</code>，通过 <code>$\left(A A^T\right) \boldsymbol{\alpha} = \lambda \boldsymbol{\alpha}$</code>，即可求解这个方阵的特征值和特征向量。针对矩阵 <code>$A$</code>，奇异值分解是将原矩阵分解为三个部分</p>

<p><code>$$
A_{m*n} = U_{m*r} \sum\nolimits_{r*r} V_{r*n}^T
$$</code></p>

<p>其中 <code>$U$</code> 称之为左奇异向量，即为 <code>$A A^T$</code> 单位化后的特征向量；<code>$V$</code> 称之为右奇异向量，即为 <code>$A^T A$</code> 单位化后的特征向量；<code>$\sum$</code>矩阵对角线上的值称之为奇异值，即为 <code>$A A^T$</code> 或 <code>$A^T A$</code> 特征值的平方根。</p>

<p>我们利用经典的 lena 图片展示一下 SVD 的作用，lena图片为一张 <code>$512*512$</code> 像素的彩色图片</p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/lena-std.png" alt="" /></p>

<p>我们对原始图片进行灰度处理后，进行特征值分解，下图中从左到右，从上到下分别是原始的灰度图像，利用 20 个左奇异向量和 20 个右奇异向量重构图像，利用 50 个左奇异向量和 100 个右奇异向量重构图像，利用 200 个左奇异向量和 200 个右奇异向量重构图像。</p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/lena-reconstruction.png" alt="" /></p>

<p>从图中可以看出，我们仅用了 200 个左奇异向量和 200 个右奇异向量重构图像与原始灰度图像已经基本看不出任何区别。因此，我们利用 SVD 可以通过仅保留较大的奇异值实现数据的压缩。</p>

<h2 id="主成份分析">主成份分析</h2>

<p>主成份分析<sup class="footnote-ref" id="fnref:wold1987principal"><a href="#fn:wold1987principal">1</a></sup>可以通俗的理解为一种降维方法。其目标可以理解为将一个 <code>$m$</code> 维的数据转换称一个 <code>$k$</code> 维的数据，其中 <code>$k &lt; m$</code>。对于具有 <code>$n$</code> 个样本的数据集，设 <code>$\boldsymbol{x_i}$</code> 表示 <code>$m$</code> 维的列向量，则</p>

<p><code>$$
X_{m*n} = \left(\boldsymbol{x_1}, \boldsymbol{x_2}, ..., \boldsymbol{x_n}\right)
$$</code></p>

<p>对每一个维度进行零均值化，即减去这一维度的均值</p>

<p><code>$$
X'_{m*n} = X - \boldsymbol{u}\boldsymbol{h}
$$</code></p>

<p>其中，<code>$\boldsymbol{u}$</code> 是一个 <code>$m$</code> 维的行向量，<code>$\boldsymbol{u}[m] = \dfrac{1}{n} \sum_{i=1}^{n} X[m, i]$</code>；<code>$h$</code> 是一个值全为 <code>$1$</code> 的 <code>$n$</code> 维行向量。</p>

<p>对于两个随机变量，我们可以利用协方差简单表示这两个变量之间的相关性</p>

<p><code>$$
\cov \left(x, y\right) = E \left(\left(x - \mu_x\right) \left(x - \mu_x\right)\right)
$$</code></p>

<p>对于已经零均值化后的矩阵 <code>$X'$</code>，计算得出如下矩阵</p>

<p><code>$$
C = \dfrac{1}{n} X' X'^T = \left\lgroup
\begin{array}{cccc}
   \dfrac{1}{n} \sum_{i=1}^{n} x_{1i}^2 &amp; \dfrac{1}{n} \sum_{i=1}^{n} x_{1i} x_{2i} &amp; \cdots &amp; \dfrac{1}{n} \sum_{}^{} x_{1i} x_{ni} \\
   \dfrac{1}{n} \sum_{i=1}^{n} x_{2i} x_{1i} &amp; \dfrac{1}{n} \sum_{i=1}^{n} x_{2i}^2 &amp; \cdots &amp; \dfrac{1}{n} \sum_{}^{} x_{2i} x_{ni} \\
   \vdots &amp; \vdots &amp; &amp; \vdots \\
   \dfrac{1}{n} \sum_{i=1}^{n} x_{mi} x_{1i} &amp; \dfrac{1}{n} \sum_{i=1}^{n} x_{mi} x_{2i} &amp; \cdots &amp; \dfrac{1}{n} \sum_{}^{} x_{mi}^2 \\
\end{array}
\right\rgroup
$$</code></p>

<p>因为矩阵 <code>$X'$</code> 已经经过了零均值化处理，因此矩阵 <code>$C$</code> 中对角线上的元素为维度 <code>$m$</code> 的方差，其他元素则为两个维度之间的协方差。</p>

<p>从 PCA 的目标来看，我们则可以通过求解矩阵 <code>$C$</code> 的特征值和特征向量，将其特征值按照从大到小的顺序按行重排其对应的特征向量，则取前 <code>$k$</code> 个，则实现了数据从 <code>$m$</code> 维降至 <code>$k$</code> 维。</p>

<p>例如，我们将二维数据</p>

<p><code>$$
\left\lgroup
\begin{array}
  -1 &amp; -1 &amp; 0 &amp; 0 &amp; 2 \\
  -2 &amp; 0 &amp; 0 &amp; 1 &amp; 1
\end{array}
\right\rgroup
$$</code></p>

<p>降至一维</p>

<pre><code class="language-r">x &lt;- matrix(c(-1, -1, 0, 0, 2,
              -2, 0, 0, 1, 1),
            5, 2, byrow = F)
x_pca &lt;- prcomp(x)

print(pca)
# Standard deviations (1, .., p=2):
# [1] 1.5811388 0.7071068
# 
# Rotation (n x k) = (2 x 2):
#            PC1        PC2
# [1,] 0.7071068  0.7071068
# [2,] 0.7071068 -0.7071068

summary(pca)
# Importance of components:
#                           PC1    PC2
# Standard deviation     1.5811 0.7071
# Proportion of Variance 0.8333 0.1667
# Cumulative Proportion  0.8333 1.0000

x_ &lt;- predict(x_pca, x)
print(x_)
#             PC1        PC2
# [1,] -2.1213203  0.7071068
# [2,] -0.7071068 -0.7071068
# [3,]  0.0000000  0.0000000
# [4,]  0.7071068 -0.7071068
# [5,]  2.1213203  0.7071068
</code></pre>

<p>降维的投影结果如图所示</p>

<p><img src="/images/cn/2017-12-11-evd-svd-and-pca/pca-projection.png" alt="" /></p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:wold1987principal">Wold, Svante, Kim Esbensen, and Paul Geladi. &ldquo;Principal component analysis.&rdquo; <em>Chemometrics and intelligent laboratory systems</em> 2.1-3 (1987): 37-52.
 <a class="footnote-return" href="#fnref:wold1987principal">↩</a></li>
</ol>
</div>



<link rel="stylesheet" href="/css/donate.css" />


<div class="donate">
  <div class="donate-header"></div>
  <div class="donate-slug" id="donate-slug">evd-svd-and-pca</div>
  <button class="donate-button">赞 赏</button>
  <div class="donate-footer">「真诚赞赏，手留余香」</div>
</div>
<div class="donate-modal-wrapper">
  <div class="donate-modal">
    <div class="donate-box">
      <div class="donate-box-content">
        <div class="donate-box-content-inner">
          <div class="donate-box-header">「真诚赞赏，手留余香」</div>
          <div class="donate-box-body">
            <div class="donate-box-money">
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-2" data-v="2" data-unchecked="￥ 2" data-checked="2 元">￥ 2</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-5" data-v="5" data-unchecked="￥ 5" data-checked="5 元">￥ 5</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-10" data-v="10" data-unchecked="￥ 10" data-checked="10 元">￥ 10</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-50" data-v="50" data-unchecked="￥ 50" data-checked="50 元">￥ 50</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-100" data-v="100" data-unchecked="￥ 100" data-checked="100 元">￥ 100</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-custom" data-v="custom" data-unchecked="任意金额" data-checked="任意金额">任意金额</button>
            </div>
            <div class="donate-box-pay">
              <img class="donate-box-pay-qrcode" id="donate-box-pay-qrcode" src=""/>
            </div>
          </div>
          <div class="donate-box-footer">
            <div class="donate-box-pay-method donate-box-pay-method-checked" data-v="wechat-pay">
              <img class="donate-box-pay-method-image" id="donate-box-pay-method-image-wechat-pay" src=""/>
            </div>
            <div class="donate-box-pay-method" data-v="alipay">
              <img class="donate-box-pay-method-image"  id="donate-box-pay-method-image-alipay" src=""/>
            </div>
          </div>
        </div>
      </div>
    </div>
    <button type="button" class="donate-box-close-button">
      <svg class="donate-box-close-button-icon" fill="#fff" viewBox="0 0 24 24" width="24" height="24"><path d="M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z" fill-rule="evenodd"></path></svg>
    </button>
  </div>
</div>

<script type="text/javascript" src="/js/donate.js"></script>
</script>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/cn/2017/08/trip-to-melbourne/">墨尔本之行 (Trip to Melbourne)</a></span>
  <span class="nav-next"><a href="/cn/2017/12/mcmc-and-gibbs-sampling/">马尔科夫链蒙特卡洛方法和吉布斯采样 (MCMC and Gibbs Sampling)</a> &rarr;</span>
</nav>
<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/cn\/2017\/08\/trip-to-melbourne\/';
    
  } else if (e.which == 39) {  
    
    url = '\/cn\/2017\/12\/mcmc-and-gibbs-sampling\/';
    
  }
  if (url) window.location = url;
});
</script>




<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-2608165017777396"
     data-ad-slot="1261604535"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>





<section class="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
    this.page.url = "https:\/\/zeqiang.fun" + location.pathname;
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var disqus_js = '//Zeqiang.disqus.com/embed.js';
    var d = document, s = d.createElement('script');
    s.src = disqus_js; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    var t = d.getElementById('disqus_thread');
    var b = false, l = function(scroll) {
      if (b) return;
      (d.head || d.body).appendChild(s); b = true;
      if (scroll) t.scrollIntoView();
    }
    s.onerror = function(e) {
      if (sessionStorage.getItem('failure-note')) return;
      t.innerText = 'Sorry, but you cannot make comments because Disqus failed to load for some reason. It is known to be blocked in China. If you are sure it is not blocked in your region, please refresh the page. 中国大陆地区读者需要翻墙才能发表评论。';
      sessionStorage.setItem('failure-note', true);
    };
    
    if (location.hash.match(/^#comment-[0-9]+$/)) return l(true);
    var c = function() {
      if (b) return;
      var rect = t.getBoundingClientRect();
      if (rect.top < window.innerHeight && rect.bottom >= 0) l();
    };
    window.addEventListener('load', c);
    d.addEventListener('scroll', c);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>





<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/autoloader/prism-autoloader.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/show-language/prism-show-language.min.js"></script>

<script>
    (function() {
        if (!self.Prism) {
            return;
        }

        Prism.languages.dos = Prism.languages.powershell;
        Prism.languages.gremlin = Prism.languages.groovy;

        var Languages = {
            'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',
            'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',
            'powershell': 'PowerShell', 'javascript': 'JavaScript',
            'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',
            'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',
            'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',
            'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration'
        };

        Prism.hooks.add('before-highlight', function(env) {
        	var language = Languages[env.language] || env.language;
        	env.element.setAttribute('data-language', language);
        });
    })();
</script>




<script async src="/js/fix-toc.js"></script>
<script async src="/js/center-img.js"></script>
<script async src="/js/right-quote.js"></script>
<script async src="/js/fix-footnote.js"></script>
<script async src="/js/external-link.js"></script>
<script async src="/js/alt-title.js"></script>
<script src="/js/no-highlight.js"></script>
<script src="/js/math-code.js"></script>


<script>
window.MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js" crossorigin></script>







<script async src="/js/load-typekit.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lazysizes/5.3.2/lazysizes.min.js"></script>

<script src="//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js"></script>
<script>
addBackToTop({
  diameter: 48
})
</script>



  <hr>
  <div class="copyright no-border-bottom">
    <div class="copyright-author-year">
      <span>&copy; 2017-2021 Leo Van</span>
    </div>
    <div class="copyright-links">
      <a href="https://github.com/leovan" rel="noreferrer" target="_blank">Github</a>
      <span> · </span>
      <a href="https://orcid.org/0000-0002-9556-7821" rel="noreferrer" target="_blank">ORCID</a>
      <span> · </span>
      <span>I am Mr. Black.</span>
    </div>
  </div>
  </footer>
  </article>
  </body>
</html>


<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

        <title>强化学习简介 (Introduction of Reinforcement Learning) - Zeqiang Fang | 方泽强</title>

    <meta name="referrer" content="no-referrer">
    
    <meta property="og:title" content="强化学习简介 (Introduction of Reinforcement Learning) - Zeqiang Fang | 方泽强">
    <meta name="description" property="og:description" content="本文为《强化学习系列》文章 强化学习简介 强化学习（Reinforcement Learning，RL）是机器学习中的一个领域，是学习“做什么（即">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Zeqiang Fang | 方泽强">
    <meta property="og:url" content="http://zeqiang.fun/user_blogdown/cn/2020/05/introduction-of-reinforcement-learning/">

    
    
    
    
    <meta name="author" property="article:author" content="范叶亮">
    
    
    
    <meta name="date" property="article:published_time" content="2020-05-09" scheme="YYYY-MM-DD">
    
    
    <meta name="date" property="article:modified_time" content="2020-05-09" scheme="YYYY-MM-DD">
    

    
    <meta name="keywords" property="article:tag" content ="强化学习,Reinforcement Learning,AlphaGo,AlphaStar,OpenAI Five,Pluribus">
    
    
    <meta name="theme-color" content="#0d0d0d">
    
    <link rel="icon" type="image/png" sizes="16x16" href="/images/web/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/web/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/images/web/favicon-48x48.png">
    <link rel="icon" type="image/png" sizes="62x62" href="/images/web/favicon-62x62.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/images/web/favicon-192x192.png">
    <link rel="apple-touch-icon" size="192x192" href="/images/web/icon-192x192.png">
    <link rel="manifest" href="/manifest.json">
        
    

    

    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http://zeqiang.fun/user_blogdown/cn"
        },
        "name": "强化学习简介 (Introduction of Reinforcement Learning)",
        "headline": "强化学习简介 (Introduction of Reinforcement Learning)",
        "description" : "本文为《强化学习系列》文章 强化学习简介 强化学习（Reinforcement Learning，RL）是机器学习中的一个领域，是学习“做什么（即",
        "genre": [
            "机器学习", "强化学习"
        ],
        "datePublished": "2020-05-09",
        "dateModified": "2020-05-09",
        "wordCount": "2067",
        "keywords": [
            "强化学习", "Reinforcement Learning", "AlphaGo", "AlphaStar", "OpenAI Five", "Pluribus"
        ],
        "image": [
            "http://zeqiang.fun/images/cn/2020-05-09-introduction-of-reinforcement-learning/machine-learning-types.png", "http://zeqiang.fun/images/cn/2020-05-09-introduction-of-reinforcement-learning/reinforcement-learning.png"
        ],
        "author": {
            "@type": "Person",
            "name": "范叶亮"
        },
        "publisher": {
            "@type": "Organization",
            "name": "范叶亮",
            "logo": {
                "@type": "ImageObject",
                "url": "http://zeqiang.fun/images/web/publisher-logo.png"
            }
        },
        "url": "http://zeqiang.fun/user_blogdown/cn/2020/05/introduction-of-reinforcement-learning/"
    }
    </script>
    

    <script src='//cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js'></script>
<script src='//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js'></script>

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/6.5.95/css/materialdesignicons.min.css">





<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<link rel="stylesheet" type="text/css" href="/user_blogdown/css/fonts.css" />
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/style.css">
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/light.css">
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/dark.css" id="dark-mode-style" disabled="disabled">
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/icons.css">
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/print.css">

  </head>

  
  <body class="user_blogdown">
    <header class="masthead">
      

<div class="logo"></div>
<p class="slogan">优雅永不过时</p>
      <nav class="menu">
  <ul>
  
  
  
    
  
  
    <li><a href="/user_blogdown/">首页</a></li>
  
    <li><a href="/user_blogdown/cn/">博客</a></li>
  
    <li><a href="/user_blogdown/cn/about/">关于</a></li>
  
    <li class="menu-separator"><span>&nbsp;</span></li>
  
    <li><a href="/user_blogdown/">Home</a></li>
  
    <li><a href="/user_blogdown/en/">Blog</a></li>
  
    <li><a href="/user_blogdown/en/about/">About</a></li>
  
  


<li class="menu-separator"><span>&nbsp;</span></li>

<li><a href="/cn/index.xml" target="_blank" type="application/rss+xml" title="RSS feed">订阅</a></li>

<li><a href="https://github.com/fang-zeqiang/fang-zeqiang.github.io/blob/master/LICENSE" target="_blank" title="Attribution-NonCommercial-ShareAlike 4.0 International">版权</a></li>


  <li class="light-dark-mode no-border-bottom"><a id="light-dark-mode-action"><span id="light-dark-mode-icon" class="mdi mdi-weather-night"></span></a></li>
  </ul>
</nav>

      <script src="/js/toggle-theme.js"></script>
    </header>

    <article class="main">
      <header class="title">
      
<h1>强化学习简介 (Introduction of Reinforcement Learning)</h1>
<h2><span class="subtitle">强化学习系列</span></h2>






<h3>范叶亮 / 
2020-05-09</h3>



<h3 class="post-meta">


<strong>分类: </strong>
<a href="/user_blogdown/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>, <a href="/user_blogdown/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">强化学习</a>




/




<strong>标签: </strong>
<span>强化学习</span>, <span>Reinforcement Learning</span>, <span>AlphaGo</span>, <span>AlphaStar</span>, <span>OpenAI Five</span>, <span>Pluribus</span>




/


<strong>字数: </strong>
2067
</h3>



<hr>


      </header>






<blockquote>
<p>本文为<a href="/categories/强化学习/">《强化学习系列》</a>文章</p>
</blockquote>

<h2 id="强化学习简介">强化学习简介</h2>

<p><strong>强化学习（Reinforcement Learning，RL）</strong>是机器学习中的一个领域，是学习“做什么（即如何把当前的情景映射成动作）才能使得数值化的收益信号最大化”。学习者不会被告知应该采取什么动作，而是必须自己通过尝试去发现哪些动作会产生最丰厚的收益。</p>

<p>强化学习同机器学习领域中的<strong>有监督学习</strong>和<strong>无监督学习</strong>不同，有监督学习是从外部监督者提供的带标注训练集中进行学习（任务驱动型），无监督学习是一个典型的寻找未标注数据中隐含结构的过程（数据驱动型）。强化学习是与两者并列的第三种机器学习范式，强化学习带来了一个独有的挑战——<strong>“试探”</strong>与<strong>“开发”</strong>之间的折中权衡，智能体必须开发已有的经验来获取收益，同时也要进行试探，使得未来可以获得更好的动作选择空间（即从错误中学习）。</p>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/machine-learning-types.png" class="lazyload"/>
  
</figure>

<p>在强化学习中，有两个可以进行交互的对象：<strong>智能体（Agnet）</strong>和<strong>环境（Environment）</strong>：</p>

<ul>
<li>智能体：可以感知环境的<strong>状态（State）</strong>，并根据反馈的<strong>奖励（Reward）</strong>学习选择一个合适的<strong>动作（Action）</strong>，来最大化长期总收益。</li>
<li>环境：环境会接收智能体执行的一系列动作，对这一系列动作进行评价并转换为一种可量化的信号反馈给智能体。</li>
</ul>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/reinforcement-learning.png" class="lazyload"/>
  <figcaption><p class="figcaption">图片来源：<a href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noreferrer" target="_blank">https://en.wikipedia.org/wiki/Reinforcement_learning</a></p></figcaption>
</figure>

<p>除了智能体和环境之外，强化学习系统有四个核心要素：<strong>策略（Policy）</strong>、<strong>回报函数（收益信号，Reward Function）</strong>、<strong>价值函数（Value Function）</strong>和<strong>环境模型（Environment Model）</strong>，其中环境模型是可选的。</p>

<ul>
<li>策略：定义了智能体在特定时间的行为方式。策略是环境状态到动作的映射。</li>
<li>回报函数：定义了强化学习问题中的目标。在每一步中，环境向智能体发送一个称为收益的标量数值。</li>
<li>价值函数：表示了从长远的角度看什么是好的。一个状态的价值是一个智能体从这个状态开始，对将来累积的总收益的期望。</li>
<li>环境模型：是一种对环境的反应模式的模拟，它允许对外部环境的行为进行推断。</li>
</ul>

<p>强化学习是一种对目标导向的学习与决策问题进行理解和自动化处理的计算方法。它强调智能体通过与环境的直接互动来学习，而不需要可效仿的监督信号或对周围环境的完全建模，因而与其他的计算方法相比具有不同的范式。</p>

<p>强化学习使用马尔可夫决策过程的形式化框架，使用<strong>状态</strong>，<strong>动作</strong>和<strong>收益</strong>定义学习型智能体与环境的互动过程。这个框架力图简单地表示人工智能问题的若干重要特征，这些特征包含了对<strong>因果关系</strong>的认知，对<strong>不确定性</strong>的认知，以及对<strong>显式目标存在性</strong>的认知。</p>

<p>价值与价值函数是强化学习方法的重要特征，价值函数对于策略空间的有效搜索来说十分重要。相比于进化方法以对完整策略的反复评估为引导对策略空间进行直接搜索，使用价值函数是强化学习方法与进化方法的不同之处。</p>

<h2 id="示例和应用">示例和应用</h2>

<p>以经典的 Flappy Bird 游戏为例，智能体就是游戏中我们操作的小鸟，整个游戏中的天空和遮挡管道即为环境，动作为玩家单击屏幕使小鸟飞起的行为，如下图所示：</p>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/flappy-bird-rl.png" class="lazyload"/>
  <figcaption><p class="figcaption">图片来源：<a href="https://easyai.tech/ai-definition/reinforcement-learning" rel="noreferrer" target="_blank">https://easyai.tech/ai-definition/reinforcement-learning</a></p></figcaption>
</figure>

<p>目前，强化学习在包括<strong>游戏</strong>，<strong>广告和推荐</strong>，<strong>对话系统</strong>，<strong>机器人</strong>等多个领域均展开了广泛的应用。</p>

<h3 id="游戏">游戏</h3>

<p><strong>AlphaGo</strong> <sup class="footnote-ref" id="fnref:alphago"><a href="#fn:alphago">1</a></sup> 是于 2014 年开始由英国伦敦 Google DeepMind 开发的人工智能围棋软件。AlphaGo 使用蒙特卡洛树搜索（Monte Carlo tree search），借助估值网络（value network）与走棋网络（policy network）这两种深度神经网络，通过估值网络来评估大量选点，并通过走棋网络选择落点。</p>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/alphago.png" class="lazyload"/>
  
</figure>

<p><strong>AlphaStar</strong> <sup class="footnote-ref" id="fnref:alphastar-1"><a href="#fn:alphastar-1">2</a></sup> <sup class="footnote-ref" id="fnref:alphastar-2"><a href="#fn:alphastar-2">3</a></sup> 是由 DeepMind 开发的玩 <a href="https://zh.wikipedia.org/wiki/%E6%98%9F%E6%B5%B7%E7%88%AD%E9%9C%B8II%EF%BC%9A%E8%87%AA%E7%94%B1%E4%B9%8B%E7%BF%BC" rel="noreferrer" target="_blank">星际争霸 II</a> 游戏的人工智能程序。AlphaStar 是由一个深度神经网路生成的，它接收来自原始游戏界面的输入数据，并输出一系列指令，构成游戏中的一个动作。</p>

<p>更具体地说，神经网路体系结构将 Transformer 框架运用于模型单元（类似于关系深度强化学习），结合一个深度 LSTM 核心、一个带有 pointer network 的自回归策略前端和一个集中的值基线。这种先进的模型将有助于解决机器学习研究中涉及长期序列建模和大输出空间（如翻译、语言建模和视觉表示）的许多其他挑战。</p>

<p>AlphaStar 还使用了一种新的多智能体学习算法。该神经网路最初是通过在 Blizzard 发布的匿名人类游戏中进行监督学习来训练的。这使得 AlphaStar 能够通过模仿学习星际争霸上玩家所使用的基本微观和宏观策略。这个初级智能体在 95% 的游戏中击败了内置的「精英」AI 关卡（相当于人类玩家的黄金级别）。</p>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/alphastar.png" class="lazyload"/>
  
</figure>

<p><strong>OpenAI Five</strong> <sup class="footnote-ref" id="fnref:openai-five"><a href="#fn:openai-five">4</a></sup> 是一个由 OpenAI 开发的用于多人视频游戏 <a href="https://zh.wikipedia.org/zh-hans/Dota_2" rel="noreferrer" target="_blank">Dota 2</a> 的人工智能程序。OpenAI Five 通过与自己进行超过 10,000 年时长的游戏进行优化学习，最终获得了专家级别的表现。</p>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/openai-five.png" class="lazyload"/>
  
</figure>

<p><strong>Pluribus</strong> <sup class="footnote-ref" id="fnref:pluribus"><a href="#fn:pluribus">5</a></sup> 是由 Facebook 开发的第一个在六人无限注德州扑克中击败人类专家的 AI 智能程序，其首次在复杂游戏中击败两个人或两个团队。</p>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/facebook-pluribus.jpg" class="lazyload"/>
  
</figure>

<h3 id="广告和推荐">广告和推荐</h3>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/recommendation.png" class="lazyload"/>
  <figcaption><p class="figcaption">图片来源：A Reinforcement Learning Framework for Explainable Recommendation</p></figcaption>
</figure>

<h3 id="对话系统">对话系统</h3>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/dialogue-system.png" class="lazyload"/>
  <figcaption><p class="figcaption">图片来源：End-to-End Task-Completion Neural Dialogue Systems</p></figcaption>
</figure>

<h3 id="机器人">机器人</h3>

<figure>
  <img data-src="/images/cn/2020-05-09-introduction-of-reinforcement-learning/robot.png" class="lazyload"/>
  <figcaption><p class="figcaption">图片来源：Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning</p></figcaption>
</figure>

<h2 id="开放资源">开放资源</h2>

<h3 id="开源实验平台">开源实验平台</h3>

<ul>
<li><a href="https://github.com/openai/gym" rel="noreferrer" target="_blank">openai/gym</a></li>
<li><a href="http://mujoco.org/" rel="noreferrer" target="_blank">MuJoCo</a></li>
<li><a href="https://github.com/openai/mujoco-py" rel="noreferrer" target="_blank">openai/mujoco-py</a></li>
<li><a href="https://github.com/deepmind/lab" rel="noreferrer" target="_blank">deepmind/lab</a></li>
</ul>

<h3 id="开源框架">开源框架</h3>

<ul>
<li><a href="https://github.com/deepmind/trfl/" rel="noreferrer" target="_blank">deepmind/trfl/</a> <i class="icon icon-tensorflow"></i></li>
<li><a href="https://github.com/deepmind/open_spiel" rel="noreferrer" target="_blank">deepmind/open_spiel</a> <i class="icon icon-tensorflow"></i></li>
<li><a href="https://github.com/google/dopamine" rel="noreferrer" target="_blank">google/dopamine</a> <i class="icon icon-tensorflow"></i></li>
<li><a href="https://github.com/tensorflow/agents" rel="noreferrer" target="_blank">tensorflow/agents</a> <i class="icon icon-tensorflow"></i></li>
<li><a href="https://github.com/keras-rl/keras-rl" rel="noreferrer" target="_blank">keras-rl/keras-rl</a> <i class="icon icon-keras"></i></li>
<li><a href="https://github.com/tensorforce/tensorforce" rel="noreferrer" target="_blank">tensorforce/tensorforce</a> <i class="icon icon-tensorflow"></i></li>
<li><a href="https://github.com/facebookresearch/ReAgent" rel="noreferrer" target="_blank">facebookresearch/ReAgent</a> <i class="icon icon-pytorch"></i></li>
<li><a href="https://github.com/thu-ml/tianshou" rel="noreferrer" target="_blank">thu-ml/tianshou</a> <i class="icon icon-pytorch"></i></li>
<li><a href="https://github.com/astooke/rlpyt" rel="noreferrer" target="_blank">astooke/rlpyt</a> <i class="icon icon-pytorch"></i></li>
<li><a href="https://github.com/NervanaSystems/coach" rel="noreferrer" target="_blank">NervanaSystems/coach</a> <i class="icon icon-tensorflow"></i></li>
<li><a href="https://github.com/PaddlePaddle/PARL" rel="noreferrer" target="_blank">PaddlePaddle/PARL</a> <i class="icon icon-paddlepaddle"></i></li>
</ul>

<h3 id="开源模型">开源模型</h3>

<ul>
<li><a href="https://github.com/dennybritz/reinforcement-learning" rel="noreferrer" target="_blank">dennybritz/reinforcement-learning</a> <i class="icon icon-tensorflow"></i></li>
<li><a href="https://github.com/openai/baselines" rel="noreferrer" target="_blank">openai/baselines</a> <i class="icon icon-tensorflow"></i></li>
</ul>

<h3 id="其他资源">其他资源</h3>

<ul>
<li><a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" rel="noreferrer" target="_blank">ShangtongZhang/reinforcement-learning-an-introduction</a></li>
<li><a href="https://github.com/aikorea/awesome-rl" rel="noreferrer" target="_blank">aikorea/awesome-rl</a></li>
<li><a href="https://github.com/openai/spinningup" rel="noreferrer" target="_blank">openai/spinningup</a></li>
<li><a href="https://github.com/udacity/deep-reinforcement-learning" rel="noreferrer" target="_blank">udacity/deep-reinforcement-learning</a></li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:alphago"><a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noreferrer" target="_blank">https://deepmind.com/research/case-studies/alphago-the-story-so-far</a>
 <a class="footnote-return" href="#fnref:alphago">↩</a></li>
<li id="fn:alphastar-1"><a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii" rel="noreferrer" target="_blank">https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii</a>
 <a class="footnote-return" href="#fnref:alphastar-1">↩</a></li>
<li id="fn:alphastar-2"><a href="https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning" rel="noreferrer" target="_blank">https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning</a>
 <a class="footnote-return" href="#fnref:alphastar-2">↩</a></li>
<li id="fn:openai-five"><a href="https://openai.com/projects/five/" rel="noreferrer" target="_blank">https://openai.com/projects/five/</a>
 <a class="footnote-return" href="#fnref:openai-five">↩</a></li>
<li id="fn:pluribus"><a href="https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/" rel="noreferrer" target="_blank">https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/</a>
 <a class="footnote-return" href="#fnref:pluribus">↩</a></li>
</ol>
</div>



<link rel="stylesheet" href="/css/donate.css" />


<div class="donate">
  <div class="donate-header"></div>
  <div class="donate-slug" id="donate-slug">introduction-of-reinforcement-learning</div>
  <button class="donate-button">赞 赏</button>
  <div class="donate-footer">「真诚赞赏，手留余香」</div>
</div>
<div class="donate-modal-wrapper">
  <div class="donate-modal">
    <div class="donate-box">
      <div class="donate-box-content">
        <div class="donate-box-content-inner">
          <div class="donate-box-header">「真诚赞赏，手留余香」</div>
          <div class="donate-box-body">
            <div class="donate-box-money">
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-2" data-v="2" data-unchecked="￥ 2" data-checked="2 元">￥ 2</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-5" data-v="5" data-unchecked="￥ 5" data-checked="5 元">￥ 5</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-10" data-v="10" data-unchecked="￥ 10" data-checked="10 元">￥ 10</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-50" data-v="50" data-unchecked="￥ 50" data-checked="50 元">￥ 50</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-100" data-v="100" data-unchecked="￥ 100" data-checked="100 元">￥ 100</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-custom" data-v="custom" data-unchecked="任意金额" data-checked="任意金额">任意金额</button>
            </div>
            <div class="donate-box-pay">
              <img class="donate-box-pay-qrcode" id="donate-box-pay-qrcode" src=""/>
            </div>
          </div>
          <div class="donate-box-footer">
            <div class="donate-box-pay-method donate-box-pay-method-checked" data-v="wechat-pay">
              <img class="donate-box-pay-method-image" id="donate-box-pay-method-image-wechat-pay" src=""/>
            </div>
            <div class="donate-box-pay-method" data-v="alipay">
              <img class="donate-box-pay-method-image"  id="donate-box-pay-method-image-alipay" src=""/>
            </div>
          </div>
        </div>
      </div>
    </div>
    <button type="button" class="donate-box-close-button">
      <svg class="donate-box-close-button-icon" fill="#fff" viewBox="0 0 24 24" width="24" height="24"><path d="M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z" fill-rule="evenodd"></path></svg>
    </button>
  </div>
</div>

<script type="text/javascript" src="/js/donate.js"></script>
</script>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/user_blogdown/cn/2020/05/compile-and-install-tmux-on-synology-nas/">在群晖 NAS 上编译安装 tmux</a></span>
  <span class="nav-next"><a href="/user_blogdown/cn/2020/05/multi-armed-bandit/">多臂赌博机 (Multi-armed Bandit)</a> &rarr;</span>
</nav>
<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/user_blogdown\/cn\/2020\/05\/compile-and-install-tmux-on-synology-nas\/';
    
  } else if (e.which == 39) {  
    
    url = '\/user_blogdown\/cn\/2020\/05\/multi-armed-bandit\/';
    
  }
  if (url) window.location = url;
});
</script>




<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-2608165017777396"
     data-ad-slot="1261604535"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>





<section class="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
    this.page.url = "https:\/\/zeqiang.fun" + location.pathname;
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var disqus_js = '//Zeqiang.disqus.com/embed.js';
    var d = document, s = d.createElement('script');
    s.src = disqus_js; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    var t = d.getElementById('disqus_thread');
    var b = false, l = function(scroll) {
      if (b) return;
      (d.head || d.body).appendChild(s); b = true;
      if (scroll) t.scrollIntoView();
    }
    s.onerror = function(e) {
      if (sessionStorage.getItem('failure-note')) return;
      t.innerText = 'Sorry, but you cannot make comments because Disqus failed to load for some reason. It is known to be blocked in China. If you are sure it is not blocked in your region, please refresh the page. 中国大陆地区读者需要翻墙才能发表评论。';
      sessionStorage.setItem('failure-note', true);
    };
    
    if (location.hash.match(/^#comment-[0-9]+$/)) return l(true);
    var c = function() {
      if (b) return;
      var rect = t.getBoundingClientRect();
      if (rect.top < window.innerHeight && rect.bottom >= 0) l();
    };
    window.addEventListener('load', c);
    d.addEventListener('scroll', c);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>




<script async src="/js/fix-toc.js"></script>
<script async src="/js/center-img.js"></script>
<script async src="/js/right-quote.js"></script>
<script async src="/js/fix-footnote.js"></script>
<script async src="/js/external-link.js"></script>
<script async src="/js/alt-title.js"></script>
<script src="/js/no-highlight.js"></script>
<script src="/js/math-code.js"></script>








<script async src="/js/load-typekit.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lazysizes/5.3.2/lazysizes.min.js"></script>

<script src="//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js"></script>
<script>
addBackToTop({
  diameter: 48
})
</script>



  <hr>
  <div class="copyright no-border-bottom">
    <div class="copyright-author-year">
      <span>&copy; 2017-2021 Leo Van</span>
    </div>
    <div class="copyright-links">
      <a href="https://github.com/leovan" rel="noreferrer" target="_blank">Github</a>
      <span> · </span>
      <a href="https://orcid.org/0000-0002-9556-7821" rel="noreferrer" target="_blank">ORCID</a>
      <span> · </span>
      <span>I am Mr. Black.</span>
    </div>
  </div>
  </footer>
  </article>
  </body>
</html>


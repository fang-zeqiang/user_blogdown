<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

        <title>马尔可夫决策过程 (Markov Decision Process) - Zeqiang Fang | 方泽强</title>

    <meta name="referrer" content="no-referrer">
    
    <meta property="og:title" content="马尔可夫决策过程 (Markov Decision Process) - Zeqiang Fang | 方泽强">
    <meta name="description" property="og:description" content="本文为《强化学习系列》文章 本文内容主要参考自： 1.《强化学习》1 2. CS234: Reinforcement Learning 2 3. UCL Course on RL 3 马尔可夫模型 马尔可夫模型是一种用于序列数据建模的随机模">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Zeqiang Fang | 方泽强">
    <meta property="og:url" content="https://leovan.me/cn/2020/05/markov-decision-process/">

    
    
    
    
    <meta name="author" property="article:author" content="范叶亮">
    
    
    
    <meta name="date" property="article:published_time" content="2020-05-23" scheme="YYYY-MM-DD">
    
    
    <meta name="date" property="article:modified_time" content="2020-05-23" scheme="YYYY-MM-DD">
    

    
    <meta name="keywords" property="article:tag" content ="强化学习,Reinforcement Learning,马尔可夫模型,Markov Model,马尔可夫链,Markov Chain,MC,隐马尔可夫模型,Hidden Markov Model,HMM,马尔可夫奖励过程,Markov Reward Process,MRP,分幕,马尔可夫决策过程,Markov Decision Process,MDP,部分可观测马尔可夫决策过程,Partially Observable Markov Decision Process,POMDP">
    
    
    <meta name="theme-color" content="#0d0d0d">
    
    <link rel="icon" type="image/png" sizes="16x16" href="/images/web/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/web/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/images/web/favicon-48x48.png">
    <link rel="icon" type="image/png" sizes="62x62" href="/images/web/favicon-62x62.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/images/web/favicon-192x192.png">
    <link rel="apple-touch-icon" size="192x192" href="/images/web/icon-192x192.png">
    <link rel="manifest" href="/manifest.json">
        
    

    

    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://leovan.me/cn"
        },
        "name": "马尔可夫决策过程 (Markov Decision Process)",
        "headline": "马尔可夫决策过程 (Markov Decision Process)",
        "description" : "本文为《强化学习系列》文章 本文内容主要参考自： 1.《强化学习》1 2. CS234: Reinforcement Learning 2 3. UCL Course on RL 3 马尔可夫模型 马尔可夫模型是一种用于序列数据建模的随机模",
        "genre": [
            "机器学习", "强化学习"
        ],
        "datePublished": "2020-05-23",
        "dateModified": "2020-05-23",
        "wordCount": "2931",
        "keywords": [
            "强化学习", "Reinforcement Learning", "马尔可夫模型", "Markov Model", "马尔可夫链", "Markov Chain", "MC", "隐马尔可夫模型", "Hidden Markov Model", "HMM", "马尔可夫奖励过程", "Markov Reward Process", "MRP", "分幕", "马尔可夫决策过程", "Markov Decision Process", "MDP", "部分可观测马尔可夫决策过程", "Partially Observable Markov Decision Process", "POMDP"
        ],
        "image": [
            "https://leovan.me/images/cn/2020-05-23-markov-decision-process/student-markov-chain.png", "https://leovan.me/images/cn/2020-05-23-markov-decision-process/student-mrp.png", "https://leovan.me/images/cn/2020-05-23-markov-decision-process/student-mdp.png"
        ],
        "author": {
            "@type": "Person",
            "name": "范叶亮"
        },
        "publisher": {
            "@type": "Organization",
            "name": "范叶亮",
            "logo": {
                "@type": "ImageObject",
                "url": "https://leovan.me/images/web/publisher-logo.png"
            }
        },
        "url": "https://leovan.me/cn/2020/05/markov-decision-process/"
    }
    </script>
    

    <script src='//cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js'></script>
<script src='//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js'></script>

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/6.5.95/css/materialdesignicons.min.css">





<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css">

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism-.min.css">





<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<link rel="stylesheet" type="text/css" href="/css/fonts.css" />
<link rel="stylesheet" type="text/css" href="/css/style.css">
<link rel="stylesheet" type="text/css" href="/css/light.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" id="dark-mode-style" disabled="disabled">
<link rel="stylesheet" type="text/css" href="/css/icons.css">
<link rel="stylesheet" type="text/css" href="/css/print.css">

  </head>

  
  <body class="cn">
    <header class="masthead">
      

<div class="logo"></div>
<p class="slogan">优雅永不过时</p>
      <nav class="menu">
  <ul>
  
  
  
    
  
  
    <li><a href="/">首页</a></li>
  
    <li><a href="/cn/">博客</a></li>
  
    <li><a href="/categories/">分类</a></li>
  
    <li class="menu-separator"><span>&nbsp;</span></li>
  
    <li><a href="/cn/about/">关于</a></li>
  
    <li><a href="/cn/resume/">简历</a></li>
  
  


<li class="menu-separator"><span>&nbsp;</span></li>

<li><a href="/cn/index.xml" target="_blank" type="application/rss+xml" title="RSS feed">订阅</a></li>

<li><a href="https://github.com/fang-zeqiang/fang-zeqiang.github.io/blob/master/LICENSE" target="_blank" title="Attribution-NonCommercial-ShareAlike 4.0 International">版权</a></li>


  <li class="light-dark-mode no-border-bottom"><a id="light-dark-mode-action"><span id="light-dark-mode-icon" class="mdi mdi-weather-night"></span></a></li>
  </ul>
</nav>

      <script src="/js/toggle-theme.js"></script>
    </header>

    <article class="main">
      <header class="title">
      
<h1>马尔可夫决策过程 (Markov Decision Process)</h1>
<h2><span class="subtitle">强化学习系列</span></h2>






<h3>范叶亮 / 
2020-05-23</h3>



<h3 class="post-meta">


<strong>分类: </strong>
<a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>, <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">强化学习</a>




/




<strong>标签: </strong>
<span>强化学习</span>, <span>Reinforcement Learning</span>, <span>马尔可夫模型</span>, <span>Markov Model</span>, <span>马尔可夫链</span>, <span>Markov Chain</span>, <span>MC</span>, <span>隐马尔可夫模型</span>, <span>Hidden Markov Model</span>, <span>HMM</span>, <span>马尔可夫奖励过程</span>, <span>Markov Reward Process</span>, <span>MRP</span>, <span>分幕</span>, <span>马尔可夫决策过程</span>, <span>Markov Decision Process</span>, <span>MDP</span>, <span>部分可观测马尔可夫决策过程</span>, <span>Partially Observable Markov Decision Process</span>, <span>POMDP</span>




/


<strong>字数: </strong>
2931
</h3>



<hr>


      </header>






<blockquote>
<p>本文为<a href="/categories/强化学习/">《强化学习系列》</a>文章<br />
本文内容主要参考自：<br />
1.《强化学习》<sup class="footnote-ref" id="fnref:sutton2018reinforcement"><a href="#fn:sutton2018reinforcement">1</a></sup><br />
2. CS234: Reinforcement Learning <sup class="footnote-ref" id="fnref:stanford-cs234"><a href="#fn:stanford-cs234">2</a></sup><br />
3. UCL Course on RL <sup class="footnote-ref" id="fnref:ucl-course-on-rl"><a href="#fn:ucl-course-on-rl">3</a></sup></p>
</blockquote>

<h2 id="马尔可夫模型">马尔可夫模型</h2>

<p>马尔可夫模型是一种用于序列数据建模的随机模型，其假设未来的状态仅取决于当前的状态，即：</p>

<p><code>$$
\mathbb{P} \left[S_{t+1} | S_t\right] = \mathbb{P} \left[S_{t+1} | S_1, \cdots, S_t\right]
$$</code></p>

<p>也就是认为当前状态捕获了历史中所有相关的信息。根据系统状态是否完全可被观测以及系统是自动的还是受控的，可以将马尔可夫模型分为 4 种，如下表所示：</p>

<table>
<thead>
<tr>
<th></th>
<th>状态状态完全可被观测</th>
<th>系统状态不是完全可被观测</th>
</tr>
</thead>

<tbody>
<tr>
<td><strong>状态是自动的</strong></td>
<td>马尔可夫链（MC）</td>
<td>隐马尔可夫模型（HMM）</td>
</tr>

<tr>
<td><strong>系统是受控的</strong></td>
<td>马尔可夫决策过程（MDP）</td>
<td>部分可观测马尔可夫决策过程（POMDP）</td>
</tr>
</tbody>
</table>

<p>马尔可夫链（Markov Chain，MC）为从一个状态到另一个状态转换的随机过程，当马尔可夫链的状态只能部分被观测到时，即为<a href="/cn/2020/05/hmm-crf-and-sequence-labeling/">隐马尔可夫模型（Hidden Markov Model，HMM）</a>，也就是说观测值与系统状态有关，但通常不足以精确地确定状态。马尔可夫决策过程（Markov Decision Process，MDP）也是马尔可夫链，但其状态转移取决于当前状态和采取的动作，通常一个马尔可夫决策过程用于计算依据期望回报最大化某些效用的行动策略。部分可观测马尔可夫决策过程（Partially Observable Markov Decision Process，POMDP）即为系统状态仅部分可见情况下的马尔可夫决策过程。</p>

<h2 id="马尔可夫过程">马尔可夫过程</h2>

<p>对于一个马尔可夫状态 <code>$s$</code> 和一个后继状态 <code>$s'$</code>，状态转移概率定义为：</p>

<p><code>$$
\mathcal{P}_{ss'} = \mathbb{P} \left[S_t = s' | S_{t-1} = s\right]
$$</code></p>

<p><strong>状态概率矩阵</strong> <code>$\mathcal{P}$</code> 定义了从所有状态 <code>$s$</code> 到后继状态 <code>$s'$</code> 的转移概率：</p>

<p><code>$$
\mathcal{P} = \left[\begin{array}{ccc}
\mathcal{P}_{11} &amp; \cdots &amp; \mathcal{P}_{1 n} \\
\vdots &amp; &amp; \\
\mathcal{P}_{n 1} &amp; \cdots &amp; \mathcal{P}_{n n}
\end{array}\right]
$$</code></p>

<p>其中每一行的加和为 1。</p>

<p><strong>马尔可夫过程（马尔可夫链）</strong>是一个无记忆的随机过程，一个马尔可夫过程可以定义为 <code>$\langle \mathcal{S}, \mathcal{P} \rangle$</code>，其中 <code>$\mathcal{S}$</code> 是一个有限状态集合，<code>$\mathcal{P}_{ss'} = \mathbb{P} \left[S_t = s' | S_{t-1} = s\right]$</code>，<code>$\mathcal{P}$</code> 为状态转移概率矩阵。以一个学生的日常生活为例，Class <code>$i$</code> 表示第 <code>$i$</code> 门课程，Facebook 表示在 Facebook 上进行社交，Pub 表示去酒吧，Pass 表示通过考试，Sleep 表示睡觉，这个马尔可夫过程如下图所示：</p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/student-markov-chain.png" class="lazyload"/>
  
</figure>

<p>从而可以产生多种不同的序列，例如：</p>

<pre><code>C1 -&gt; C2 -&gt; C3 -&gt; Pass -&gt; Sleep
C1 -&gt; FB -&gt; FB -&gt; C1 -&gt; C2 -&gt; Sleep
C1 -&gt; C2 -&gt; C3 -&gt; Pub -&gt; C2 -&gt; C3 -&gt; Pass -&gt; Sleep
</code></pre>

<p>状态转移概率矩阵如下所示：</p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/student-markov-chain-transition-matrix.png" class="lazyload"/>
  
</figure>

<p>据此我们可以定义<strong>马尔可夫奖励过程（Markov Reward Process，MRP）</strong>为 <code>$\langle \mathcal{S, P, R}, \gamma \rangle$</code>，其中 <code>$\mathcal{S}$</code> 和 <code>$\mathcal{P}$</code> 同马尔可夫过程定义中的参数相同，<code>$\mathcal{R}$</code> 为收益函数，<code>$\mathcal{R}_s = \mathbb{E} \left[R_t | S_{t-1} = s\right]$</code>，<code>$\gamma \in \left[0, 1\right]$</code> 为<strong>折扣率</strong>。如下图所示：</p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/student-mrp.png" class="lazyload"/>
  
</figure>

<p><strong>期望回报</strong> <code>$G_t$</code> 定义为从时刻 <code>$t$</code> 之后的所有衰减的收益之和，即：</p>

<p><code>$$
G_t = R_{t+1} + \gamma R_{t+2} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$</code></p>

<p>当 <code>$\gamma$</code> 接近 <code>$0$</code> 时，智能体更倾向于近期收益，当 <code>$\gamma$</code> 接近 <code>$1$</code> 时，智能体更侧重考虑长远收益。邻接时刻的收益可以按如下递归方式表示：</p>

<p><code>$$
G_t = R_{t+1} + \gamma G_{t+1}
$$</code></p>

<p>对于存在“最终时刻”的应用中，智能体和环境的交互能被自然地分成一个系列子序列，每个子序列称之为“<strong>幕（episodes）</strong>”，例如一盘游戏、一次走迷宫的过程，每幕都以一种特殊状态结束，称之为<strong>终结状态</strong>。这些幕可以被认为在同样的终结状态下结束，只是对不同的结果有不同的收益，具有这种<strong>分幕</strong>重复特性的任务称之为<strong>分幕式任务</strong>。</p>

<p>MRP 的状态价值函数 <code>$v \left(s\right)$</code> 给出了状态 <code>$s$</code> 的长期价值，定义为：</p>

<p><code>$$
\begin{aligned}
v(s) &amp;=\mathbb{E}\left[G_{t} | S_{t}=s\right] \\
&amp;=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots | S_{t}=s\right] \\
&amp;=\mathbb{E}\left[R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) | S_{t}=s\right] \\
&amp;=\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} | S_{t}=s\right] \\
&amp;=\mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) | S_{t}=s\right]
\end{aligned}
$$</code></p>

<p>价值函数可以分解为两部分：即时收益 <code>$R_{t+1}$</code> 和后继状态的折扣价值 <code>$\gamma v \left(S_{t+1}\right)$</code>。上式我们称之为<strong>贝尔曼方程（Bellman Equation）</strong>，其衡量了状态价值和后继状态价值之间的关系。</p>

<h2 id="马尔可夫决策过程">马尔可夫决策过程</h2>

<p>一个<strong>马尔可夫决策过程（Markov Decision Process，MDP）</strong>定义为包含决策的马尔可夫奖励过程 <code>$\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma\rangle$</code>，在这个环境中所有的状态均具有马尔可夫性。其中，<code>$\mathcal{S}$</code> 为有限的状态集合，<code>$\mathcal{A}$</code> 为有限的动作集合，<code>$\mathcal{P}$</code> 为状态转移概率矩阵，<code>$\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} | S_{t}=s, A_{t}=a\right]$</code>，<code>$\mathcal{R}$</code> 为奖励函数，<code>$\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} | S_{t}=s, A_{t}=a\right]$</code>，<code>$\gamma \in \left[0, 1\right]$</code> 为折扣率。上例中的马尔可夫决策过程如下图所示：</p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/student-mdp.png" class="lazyload"/>
  
</figure>

<p><strong>策略（Policy）</strong>定义为给定状态下动作的概率分布：</p>

<p><code>$$
\pi \left(a | s\right) = \mathbb{P} \left[A_t = a | S_t = s\right]
$$</code></p>

<p>一个策略完全确定了一个智能体的行为，同时 MDP 策略仅依赖于当前状态。给定一个 MDP <code>$\mathcal{M}=\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma\rangle$</code> 和一个策略 <code>$\pi$</code>，状态序列 <code>$S_1, S_2, \cdots$</code> 为一个马尔可夫过程 <code>$\langle \mathcal{S}, \mathcal{P}^{\pi} \rangle$</code>，状态和奖励序列 <code>$S_1, R_2, S_2, \cdots$</code> 为一个马尔可夫奖励过程 <code>$\left\langle\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma\right\rangle$</code>，其中</p>

<p><code>$$
\begin{aligned}
\mathcal{P}_{s s^{\prime}}^{\pi} &amp;=\sum_{a \in \mathcal{A}} \pi(a | s) \mathcal{P}_{s s^{\prime}}^{a} \\
\mathcal{R}_{s}^{\pi} &amp;=\sum_{a \in \mathcal{A}} \pi(a | s) \mathcal{R}_{s}^{a}
\end{aligned}
$$</code></p>

<p>在策略 <code>$\pi$</code> 下，状态 <code>$s$</code> 的价值函数记为 <code>$v_{\pi} \left(s\right)$</code>，即从状态 <code>$s$</code> 开始，智能体按照策略进行决策所获得的回报的概率期望值，对于 MDP 其定义为：</p>

<p><code>$$
\begin{aligned}
v_{\pi} \left(s\right) &amp;= \mathbb{E}_{\pi} \left[G_t | S_t = s\right] \\
&amp;= \mathbb{E}_{\pi} \left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s\right]
\end{aligned}
$$</code></p>

<p>在策略 <code>$\pi$</code> 下，在状态 <code>$s$</code> 时采取动作 <code>$a$</code> 的价值记为 <code>$q_\pi \left(s, a\right)$</code>，即根据策略 <code>$\pi$</code>，从状态 <code>$s$</code> 开始，执行动作 <code>$a$</code> 之后，所有可能的决策序列的期望回报：</p>

<p><code>$$
\begin{aligned}
q_\pi \left(s, a\right) &amp;= \mathbb{E}_{\pi} \left[G_t | S_t = s, A_t = a\right] \\
&amp;= \mathbb{E}_{\pi} \left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a\right]
\end{aligned}
$$</code></p>

<p>状态价值函数 <code>$v_{\pi}$</code> 和动作价值函数 <code>$q_{\pi}$</code> 都能从经验中估计得到，两者都可以分解为当前和后继两个部分：</p>

<p><code>$$
\begin{aligned}
v_{\pi}(s) &amp;= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s\right] \\
q_{\pi}(s, a) &amp;= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) | S_{t}=s, A_{t}=a\right]
\end{aligned}
$$</code></p>

<p>从一个状态 <code>$s$</code> 出发，采取一个行动 <code>$a$</code>，状态价值函数为：</p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/bellman-expection-eq-state-value-1.png" class="lazyload"/>
  
</figure>

<p><code>$$
v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a)
$$</code></p>

<p>从一个动作 <code>$s$</code> 出发，再采取一个行动 <code>$a$</code> 后，动作价值函数为：</p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/bellman-expection-eq-action-value-1.png" class="lazyload"/>
  
</figure>

<p><code>$$
q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)
$$</code></p>

<p>利用后继状态价值函数表示当前状态价值函数为：</p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/bellman-expection-eq-state-value-2.png" class="lazyload"/>
  
</figure>

<p><code>$$
v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a | s)\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right)
$$</code></p>

<p>利用后继动作价值函数表示当前动作价值函数为：</p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/bellman-expection-eq-action-value-2.png" class="lazyload"/>
  
</figure>

<p><code>$$
q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \sum_{a^{\prime} \in \mathcal{A}} \pi\left(a^{\prime} | s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)
$$</code></p>

<p><strong>最优状态价值函数</strong> <code>$v_* \left(s\right)$</code> 定义为所有策略上最大值的状态价值函数：</p>

<p><code>$$
v_* \left(s\right) = \mathop{\max}_{\pi} v_{\pi} \left(s\right)
$$</code></p>

<p><strong>最优动作价值函数</strong> <code>$q_* \left(s, a\right)$</code> 定义为所有策略上最大值的动作价值函数：</p>

<p><code>$$
q_* \left(s, a\right) = \mathop{\max}_{\pi} q_{\pi} \left(s, a\right)
$$</code></p>

<p>定义不同策略之间的大小关系为：</p>

<p><code>$$
\pi \geq \pi^{\prime} \text { if } v_{\pi}(s) \geq v_{\pi^{\prime}}(s), \forall s
$$</code></p>

<p>对于任意一个马尔可夫决策过程有：</p>

<ul>
<li>存在一个比其他策略更优或相等的策略，<code>$\pi_* \geq \pi, \forall \pi$</code></li>
<li>所有的最优策略均能够获得最优的状态价值函数，<code>$v_{\pi_*} \left(s\right) = v_* \left(s\right)$</code></li>
<li>所有的最优策略均能够获得最优的动作价值函数，<code>$q_{\pi_*} \left(s, a\right) = q_* \left(s, a\right)$</code></li>
</ul>

<p>一个最优策略可以通过最大化 <code>$q_* \left(s, a\right)$</code> 获得：</p>

<p><code>$$
\pi_{*}(a | s)=\left\{\begin{array}{ll}
1 &amp; \text { if } a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} q_{*}(s, a) \\
0 &amp; \text { otherwise }
\end{array}\right.
$$</code></p>

<p>对于任意一个 MDP 均会有一个确定的最优策略，如果已知 <code>$q_* \left(s, a\right)$</code> 即可知晓最优策略。</p>

<p>最优状态价值函数循环依赖于贝尔曼最优方程：</p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/bellman-optimality-eq-state-value-1.png" class="lazyload"/>
  
</figure>

<p><code>$$
v_{*}(s)=\max _{a} q_{*}(s, a)
$$</code></p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/bellman-optimality-eq-action-value-1.png" class="lazyload"/>
  
</figure>

<p><code>$$
q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)
$$</code></p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/bellman-optimality-eq-state-value-2.png" class="lazyload"/>
  
</figure>

<p><code>$$
v_{*}(s)=\max _{a} \mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)
$$</code></p>

<figure>
  <img data-src="/images/cn/2020-05-23-markov-decision-process/bellman-optimality-eq-action-value-2.png" class="lazyload"/>
  
</figure>

<p><code>$$
q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)
$$</code></p>

<p>显式求解贝尔曼最优方程给出了找到一个最优策略的方法，但这种解法至少依赖于三条实际情况很难满足的假设：</p>

<ol>
<li>准确地知道环境的动态变化特性</li>
<li>有足够的计算资源来求解</li>
<li>马尔可夫性质</li>
</ol>

<p>尤其是假设 2 很难满足，现实问题中状态的数量一般很大，即使利用最快的计算机也需要花费难以接受的时间才能求解完成。</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:sutton2018reinforcement">Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em>. MIT press.
 <a class="footnote-return" href="#fnref:sutton2018reinforcement">↩</a></li>
<li id="fn:stanford-cs234">CS234: Reinforcement Learning <a href="http://web.stanford.edu/class/cs234/index.html" rel="noreferrer" target="_blank">http://web.stanford.edu/class/cs234/index.html</a>
 <a class="footnote-return" href="#fnref:stanford-cs234">↩</a></li>
<li id="fn:ucl-course-on-rl">UCL Course on RL <a href="https://www.davidsilver.uk/teaching" rel="noreferrer" target="_blank">https://www.davidsilver.uk/teaching</a>
 <a class="footnote-return" href="#fnref:ucl-course-on-rl">↩</a></li>
</ol>
</div>



<link rel="stylesheet" href="/css/donate.css" />


<div class="donate">
  <div class="donate-header"></div>
  <div class="donate-slug" id="donate-slug">markov-decision-process</div>
  <button class="donate-button">赞 赏</button>
  <div class="donate-footer">「真诚赞赏，手留余香」</div>
</div>
<div class="donate-modal-wrapper">
  <div class="donate-modal">
    <div class="donate-box">
      <div class="donate-box-content">
        <div class="donate-box-content-inner">
          <div class="donate-box-header">「真诚赞赏，手留余香」</div>
          <div class="donate-box-body">
            <div class="donate-box-money">
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-2" data-v="2" data-unchecked="￥ 2" data-checked="2 元">￥ 2</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-5" data-v="5" data-unchecked="￥ 5" data-checked="5 元">￥ 5</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-10" data-v="10" data-unchecked="￥ 10" data-checked="10 元">￥ 10</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-50" data-v="50" data-unchecked="￥ 50" data-checked="50 元">￥ 50</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-100" data-v="100" data-unchecked="￥ 100" data-checked="100 元">￥ 100</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-custom" data-v="custom" data-unchecked="任意金额" data-checked="任意金额">任意金额</button>
            </div>
            <div class="donate-box-pay">
              <img class="donate-box-pay-qrcode" id="donate-box-pay-qrcode" src=""/>
            </div>
          </div>
          <div class="donate-box-footer">
            <div class="donate-box-pay-method donate-box-pay-method-checked" data-v="wechat-pay">
              <img class="donate-box-pay-method-image" id="donate-box-pay-method-image-wechat-pay" src=""/>
            </div>
            <div class="donate-box-pay-method" data-v="alipay">
              <img class="donate-box-pay-method-image"  id="donate-box-pay-method-image-alipay" src=""/>
            </div>
          </div>
        </div>
      </div>
    </div>
    <button type="button" class="donate-box-close-button">
      <svg class="donate-box-close-button-icon" fill="#fff" viewBox="0 0 24 24" width="24" height="24"><path d="M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z" fill-rule="evenodd"></path></svg>
    </button>
  </div>
</div>

<script type="text/javascript" src="/js/donate.js"></script>
</script>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/cn/2020/05/multi-armed-bandit/">多臂赌博机 (Multi-armed Bandit)</a></span>
  <span class="nav-next"><a href="/cn/2020/06/bayesian-optimization/">贝叶斯优化 (Bayesian Optimization)</a> &rarr;</span>
</nav>
<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/cn\/2020\/05\/multi-armed-bandit\/';
    
  } else if (e.which == 39) {  
    
    url = '\/cn\/2020\/06\/bayesian-optimization\/';
    
  }
  if (url) window.location = url;
});
</script>




<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-2608165017777396"
     data-ad-slot="1261604535"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>





<section class="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
    this.page.url = "https:\/\/zeqiang.fun" + location.pathname;
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var disqus_js = '//Zeqiang.disqus.com/embed.js';
    var d = document, s = d.createElement('script');
    s.src = disqus_js; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    var t = d.getElementById('disqus_thread');
    var b = false, l = function(scroll) {
      if (b) return;
      (d.head || d.body).appendChild(s); b = true;
      if (scroll) t.scrollIntoView();
    }
    s.onerror = function(e) {
      if (sessionStorage.getItem('failure-note')) return;
      t.innerText = 'Sorry, but you cannot make comments because Disqus failed to load for some reason. It is known to be blocked in China. If you are sure it is not blocked in your region, please refresh the page. 中国大陆地区读者需要翻墙才能发表评论。';
      sessionStorage.setItem('failure-note', true);
    };
    
    if (location.hash.match(/^#comment-[0-9]+$/)) return l(true);
    var c = function() {
      if (b) return;
      var rect = t.getBoundingClientRect();
      if (rect.top < window.innerHeight && rect.bottom >= 0) l();
    };
    window.addEventListener('load', c);
    d.addEventListener('scroll', c);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>





<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/autoloader/prism-autoloader.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/show-language/prism-show-language.min.js"></script>

<script>
    (function() {
        if (!self.Prism) {
            return;
        }

        Prism.languages.dos = Prism.languages.powershell;
        Prism.languages.gremlin = Prism.languages.groovy;

        var Languages = {
            'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',
            'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',
            'powershell': 'PowerShell', 'javascript': 'JavaScript',
            'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',
            'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',
            'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',
            'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration'
        };

        Prism.hooks.add('before-highlight', function(env) {
        	var language = Languages[env.language] || env.language;
        	env.element.setAttribute('data-language', language);
        });
    })();
</script>




<script async src="/js/fix-toc.js"></script>
<script async src="/js/center-img.js"></script>
<script async src="/js/right-quote.js"></script>
<script async src="/js/fix-footnote.js"></script>
<script async src="/js/external-link.js"></script>
<script async src="/js/alt-title.js"></script>
<script src="/js/no-highlight.js"></script>
<script src="/js/math-code.js"></script>


<script>
window.MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js" crossorigin></script>







<script async src="/js/load-typekit.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lazysizes/5.3.2/lazysizes.min.js"></script>

<script src="//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js"></script>
<script>
addBackToTop({
  diameter: 48
})
</script>



  <hr>
  <div class="copyright no-border-bottom">
    <div class="copyright-author-year">
      <span>&copy; 2017-2021 Leo Van</span>
    </div>
    <div class="copyright-links">
      <a href="https://github.com/leovan" rel="noreferrer" target="_blank">Github</a>
      <span> · </span>
      <a href="https://orcid.org/0000-0002-9556-7821" rel="noreferrer" target="_blank">ORCID</a>
      <span> · </span>
      <span>I am Mr. Black.</span>
    </div>
  </div>
  </footer>
  </article>
  </body>
</html>


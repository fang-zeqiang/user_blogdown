<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

        <title>贝叶斯优化 (Bayesian Optimization) - Zeqiang Fang | 方泽强</title>

    <meta name="referrer" content="no-referrer">
    
    <meta property="og:title" content="贝叶斯优化 (Bayesian Optimization) - Zeqiang Fang | 方泽强">
    <meta name="description" property="og:description" content="本文内容主要参考自： 1. 从高斯分布到高斯过程、高斯过程回归、贝叶斯优化 2. A Visual Exploration of Gaussian Processes 3. Gaussian Process Regression 4. Exploring Bayesian Optimization 高斯分布 一元高斯分布 若随机变量 $X$ 服从一个均">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Zeqiang Fang | 方泽强">
    <meta property="og:url" content="http://zeqiang.fun/user_blogdown/cn/2020/06/bayesian-optimization/">

    
    
    
    
    <meta name="author" property="article:author" content="范叶亮">
    
    
    
    <meta name="date" property="article:published_time" content="2020-06-06" scheme="YYYY-MM-DD">
    
    
    <meta name="date" property="article:modified_time" content="2020-06-06" scheme="YYYY-MM-DD">
    

    
    <meta name="keywords" property="article:tag" content ="高斯分布,Gaussian Distribution,正态分布,Normal Distribution,边缘化,Marginalization,条件化,Conditioning,高斯过程,Gaussian Processes,高斯过程回归,Gaussian Processes Regression,主动学习,Active Learning,代理模型,Surrogate Model,贝叶斯优化,Bayesian Optimization,采集函数,Acquisition Functions">
    
    
    <meta name="theme-color" content="#0d0d0d">
    
    <link rel="icon" type="image/png" sizes="16x16" href="/images/web/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/web/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/images/web/favicon-48x48.png">
    <link rel="icon" type="image/png" sizes="62x62" href="/images/web/favicon-62x62.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/images/web/favicon-192x192.png">
    <link rel="apple-touch-icon" size="192x192" href="/images/web/icon-192x192.png">
    <link rel="manifest" href="/manifest.json">
        
    

    

    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http://zeqiang.fun/user_blogdown/cn"
        },
        "name": "贝叶斯优化 (Bayesian Optimization)",
        "headline": "贝叶斯优化 (Bayesian Optimization)",
        "description" : "本文内容主要参考自： 1. 从高斯分布到高斯过程、高斯过程回归、贝叶斯优化 2. A Visual Exploration of Gaussian Processes 3. Gaussian Process Regression 4. Exploring Bayesian Optimization 高斯分布 一元高斯分布 若随机变量 $X$ 服从一个均",
        "genre": [
            "机器学习", "最优化"
        ],
        "datePublished": "2020-06-06",
        "dateModified": "2020-06-06",
        "wordCount": "5043",
        "keywords": [
            "高斯分布", "Gaussian Distribution", "正态分布", "Normal Distribution", "边缘化", "Marginalization", "条件化", "Conditioning", "高斯过程", "Gaussian Processes", "高斯过程回归", "Gaussian Processes Regression", "主动学习", "Active Learning", "代理模型", "Surrogate Model", "贝叶斯优化", "Bayesian Optimization", "采集函数", "Acquisition Functions"
        ],
        "image": [
            "http://zeqiang.fun/images/cn/2020-06-06-bayesian-optimization/marginalization-and-conditioning.png", "http://zeqiang.fun/images/cn/2020-06-06-bayesian-optimization/gp-prior.png", "http://zeqiang.fun/images/cn/2020-06-06-bayesian-optimization/gp-posterior.png", "http://zeqiang.fun/images/cn/2020-06-06-bayesian-optimization/prior-posterior-convariance.png"
        ],
        "author": {
            "@type": "Person",
            "name": "范叶亮"
        },
        "publisher": {
            "@type": "Organization",
            "name": "范叶亮",
            "logo": {
                "@type": "ImageObject",
                "url": "http://zeqiang.fun/images/web/publisher-logo.png"
            }
        },
        "url": "http://zeqiang.fun/user_blogdown/cn/2020/06/bayesian-optimization/"
    }
    </script>
    

    <script src='//cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js'></script>
<script src='//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js'></script>

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/6.5.95/css/materialdesignicons.min.css">





<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css">

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism-.min.css">





<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<link rel="stylesheet" type="text/css" href="/user_blogdown/css/fonts.css" />
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/style.css">
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/light.css">
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/dark.css" id="dark-mode-style" disabled="disabled">
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/icons.css">
<link rel="stylesheet" type="text/css" href="/user_blogdown/css/print.css">

  </head>

  
  <body class="user_blogdown">
    <header class="masthead">
      

<div class="logo"></div>
<p class="slogan">优雅永不过时</p>
      <nav class="menu">
  <ul>
  
  
  
    
  
  
    <li><a href="/user_blogdown/">首页</a></li>
  
    <li><a href="/user_blogdown/cn/">博客</a></li>
  
    <li><a href="/user_blogdown/cn/about/">关于</a></li>
  
    <li class="menu-separator"><span>&nbsp;</span></li>
  
    <li><a href="/user_blogdown/">Home</a></li>
  
    <li><a href="/user_blogdown/en/">Blog</a></li>
  
    <li><a href="/user_blogdown/en/about/">About</a></li>
  
  


<li class="menu-separator"><span>&nbsp;</span></li>

<li><a href="/cn/index.xml" target="_blank" type="application/rss+xml" title="RSS feed">订阅</a></li>

<li><a href="https://github.com/fang-zeqiang/fang-zeqiang.github.io/blob/master/LICENSE" target="_blank" title="Attribution-NonCommercial-ShareAlike 4.0 International">版权</a></li>


  <li class="light-dark-mode no-border-bottom"><a id="light-dark-mode-action"><span id="light-dark-mode-icon" class="mdi mdi-weather-night"></span></a></li>
  </ul>
</nav>

      <script src="/js/toggle-theme.js"></script>
    </header>

    <article class="main">
      <header class="title">
      
<h1>贝叶斯优化 (Bayesian Optimization)</h1>







<h3>范叶亮 / 
2020-06-06</h3>



<h3 class="post-meta">


<strong>分类: </strong>
<a href="/user_blogdown/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>, <a href="/user_blogdown/categories/%E6%9C%80%E4%BC%98%E5%8C%96">最优化</a>




/




<strong>标签: </strong>
<span>高斯分布</span>, <span>Gaussian Distribution</span>, <span>正态分布</span>, <span>Normal Distribution</span>, <span>边缘化</span>, <span>Marginalization</span>, <span>条件化</span>, <span>Conditioning</span>, <span>高斯过程</span>, <span>Gaussian Processes</span>, <span>高斯过程回归</span>, <span>Gaussian Processes Regression</span>, <span>主动学习</span>, <span>Active Learning</span>, <span>代理模型</span>, <span>Surrogate Model</span>, <span>贝叶斯优化</span>, <span>Bayesian Optimization</span>, <span>采集函数</span>, <span>Acquisition Functions</span>




/


<strong>字数: </strong>
5043
</h3>



<hr>


      </header>






<blockquote>
<p>本文内容主要参考自：<br />
1. <a href="https://zhuanlan.zhihu.com/p/139478368" rel="noreferrer" target="_blank">从高斯分布到高斯过程、高斯过程回归、贝叶斯优化</a><br />
2. <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/" rel="noreferrer" target="_blank">A Visual Exploration of Gaussian Processes</a><br />
3. <a href="https://www.aidanscannell.com/post/gaussian-process-regression/" rel="noreferrer" target="_blank">Gaussian Process Regression</a><br />
4. <a href="https://distill.pub/2020/bayesian-optimization/" rel="noreferrer" target="_blank">Exploring Bayesian Optimization</a></p>
</blockquote>

<h2 id="高斯分布">高斯分布</h2>

<h3 id="一元高斯分布">一元高斯分布</h3>

<p>若随机变量 <code>$X$</code> 服从一个均值为 <code>$\mu$</code>，方差为 <code>$\sigma^2$</code> 的高斯分布，则记为：</p>

<p><code>$$
X \sim N \left(\mu, \sigma^2\right)
$$</code></p>

<p>其概率密度函数为：</p>

<p><code>$$
f \left(x\right) = \dfrac{1}{\sigma \sqrt{2 \pi}} e^{- \dfrac{\left(x - \mu\right)^2}{2 \sigma^2}}
$$</code></p>

<figure>
  <img data-src="/images/cn/2020-06-06-bayesian-optimization/univariate-gaussian-distribution.png" class="lazyload"/>
  <figcaption><p class="figcaption">图片来源：<a href="https://zh.wikipedia.org/wiki/正态分布" rel="noreferrer" target="_blank">https://zh.wikipedia.org/wiki/正态分布</a></p></figcaption>
</figure>

<h3 id="二元高斯分布">二元高斯分布</h3>

<p>若随机变量 <code>$X, Y$</code> 服从均值为 <code>$\mu = \left(\mu_X, \mu_Y\right)^{\top}$</code>，方差为 <code>$\mu = \left(\sigma_X, \sigma_Y\right)^{\top}$</code> 的高斯分布，则记为：</p>

<p><code>$$
\left(X, Y\right) \sim \mathcal{N} \left(\mu, \sigma\right)
$$</code></p>

<p>其概率密度函数为：</p>

<p><code>$$
f(x, y)=\frac{1}{2 \pi \sigma_{X} \sigma_{Y} \sqrt{1-\rho^{2}}} e^{-\dfrac{1}{2\left(1-\rho^{2}\right)}\left[\dfrac{\left(x-\mu_{X}\right)^{2}}{\sigma_{X}^{2}}+\dfrac{\left(y-\mu_{Y}\right)^{2}}{\sigma_{Y}^{2}}-\dfrac{2 \rho\left(x-\mu_{X}\right)\left(y-\mu_{X}\right)}{\sigma_{X} \sigma_{Y}}\right]}
$$</code></p>

<p>其中，<code>$\rho$</code> 是 <code>$X$</code> 和 <code>$Y$</code> 之间的相关系数，<code>$\sigma_X &gt; 0$</code> 且 <code>$\sigma_Y &gt; 0$</code>。</p>

<figure>
  <img data-src="/images/cn/2020-06-06-bayesian-optimization/bivariate-gaussian-distribution.png" class="lazyload"/>
  <figcaption><p class="figcaption">图片来源：Bayesian tracking of multiple point targets using expectation maximization</p></figcaption>
</figure>

<h3 id="多元高斯分布">多元高斯分布</h3>

<p>若 <code>$K$</code> 维随机向量 <code>$X = \left[X_1, \cdots, X_K\right]^{\top}$</code> 服从多元高斯分布，则必须满足如下三个等价条件：</p>

<ol>
<li>任何线性组合 <code>$Y = a_1 X_1 + \cdots a_K X_K$</code> 均服从高斯分布。</li>
<li>存在随机向量 <code>$Z = \left[Z_1, \cdots, Z_L\right]^{\top}$</code>（每个元素服从独立标准高斯分布），向量 <code>$\mu = \left[\mu_1, \cdots, \mu_K\right]^{\top}$</code> 以及 <code>$K \times L$</code> 的矩阵 <code>$A$</code>，满足 <code>$X = A Z + \mu$</code>。</li>
<li>存在 <code>$\mu$</code> 和一个对称半正定矩阵 <code>$\Sigma$</code> 满足 <code>$X$</code> 的特征函数 <code>$\phi_X \left(u; \mu, \Sigma\right) = \exp \left(i \mu^{\top} u - \dfrac{1}{2} u^{\top} \Sigma u\right)$</code></li>
</ol>

<p>如果 <code>$\Sigma$</code> 是非奇异的，则概率密度函数为：</p>

<p><code>$$
f \left(x_1, \cdots, x_k\right) = \dfrac{1}{\sqrt{\left(2 \pi\right)^k \lvert\Sigma\rvert}} e^{- \dfrac{1}{2} \left(x - \mu\right)^{\top} \Sigma^{-1} \left(x - \mu\right)}
$$</code></p>

<p>其中 <code>$\lvert\Sigma\rvert$</code> 表示协方差矩阵的行列式。</p>

<h3 id="边缘化和条件化">边缘化和条件化</h3>

<p>高斯分布具有一个优秀的代数性质，即在边缘化和条件化下是闭合的，也就是说从这些操作中获取的结果分布也是高斯的。<strong>边缘化（Marginalization）</strong>和<strong>条件化（Conditioning）</strong>都作用于原始分布的子集上：</p>

<p><code>$$
P_{X, Y}=\left[\begin{array}{l}
X \\
Y
\end{array}\right] \sim \mathcal{N}(\mu, \Sigma)=\mathcal{N}\left(\left[\begin{array}{l}
\mu_{X} \\
\mu_{Y}
\end{array}\right],\left[\begin{array}{l}
\Sigma_{X X} \Sigma_{X Y} \\
\Sigma_{Y X} \Sigma_{Y Y}
\end{array}\right]\right)
$$</code></p>

<p>其中，<code>$X$</code> 和 <code>$Y$</code> 表示原始随机变量的子集。</p>

<p>对于随机向量 <code>$X$</code> 和 <code>$Y$</code> 的高斯概率分布 <code>$P \left(X, Y\right)$</code>，其边缘概率分布为：</p>

<p><code>$$
\begin{array}{l}
X \sim \mathcal{N}\left(\mu_{X}, \Sigma_{X X}\right) \\
Y \sim \mathcal{N}\left(\mu_{Y}, \Sigma_{Y Y}\right)
\end{array}
$$</code></p>

<p><code>$X$</code> 和 <code>$Y$</code> 两个子集各自只依赖于 <code>$\mu$</code> 和 <code>$\Sigma$</code> 中它们对应的值。因此从高斯分布中边缘化一个随机变量仅需从 <code>$\mu$</code> 和 <code>$\Sigma$</code> 中舍弃相应的变量即可：</p>

<p><code>$$
p_{X}(x)=\int_{y} p_{X, Y}(x, y) d y=\int_{y} p_{X | Y}(x | y) p_{Y}(y) d y
$$</code></p>

<p>条件化可以用于得到一个变量在另一个变量条件下的概率分布：</p>

<p><code>$$
\begin{array}{l}
X | Y \sim \mathcal{N}\left(\mu_{X}+\Sigma_{X Y} \Sigma_{Y Y}^{-1}\left(Y-\mu_{Y}\right), \Sigma_{X X}-\Sigma_{X Y} \Sigma_{Y Y}^{-1} \Sigma_{Y X}\right) \\
Y | X \sim \mathcal{N}\left(\mu_{Y}+\Sigma_{Y X} \Sigma_{X X}^{-1}\left(X-\mu_{X}\right), \Sigma_{Y Y}-\Sigma_{Y X} \Sigma_{X X}^{-1} \Sigma_{X Y}\right)
\end{array}
$$</code></p>

<p>需要注意新的均值仅依赖于作为条件的变量，协方差矩阵和这个变量无关。</p>

<p>边缘化可以理解为在高斯分布的一个维度上的累加，条件化可以理解为在多元分布上切一刀从而获得一个维数更少的高斯分布，如下图所示：</p>

<figure>
  <img data-src="/images/cn/2020-06-06-bayesian-optimization/marginalization-and-conditioning.png" class="lazyload"/>
  
</figure>

<h2 id="高斯过程">高斯过程</h2>

<p><strong>高斯过程（Gaussian Process）</strong>是观测值出现在一个连续域（例如时间或空间）的随机过程。在高斯过程中，连续输入空间中每个点都是与一个正态分布的随机变量相关联。此外，这些随机变量的每个有限集合都有一个多元正态分布，换句话说它们的任意有限线性组合是一个正态分布。高斯过程的分布是所有那些（无限多个）随机变量的联合分布，正因如此，它是连续域（例如时间或空间）上函数的分布。</p>

<p>简单而言，高斯过程即为一系列随机变量，这些随机变量的任意有限集合均为一个多元高斯分布。从<strong>一元高斯分布</strong>到<strong>多元高斯分布</strong>相当于增加了空间维度，从<strong>高斯分布</strong>到<strong>高斯过程</strong>相当于引入了时间维度。一个高斯过程可以被均值函数 <code>$m \left(x\right)$</code> 和协方差函数 <code>$K \left(x, x'\right)$</code> 共同唯一确定：</p>

<p><code>$$
\begin{aligned}
m(x) &amp;=\mathbb{E}[f(x)] \\
K\left(x, x'\right) &amp;=\mathbb{E}\left[(f(x)-m(x))\left(f\left(x^{\prime}\right)-m\left(x^{\prime}\right)\right)\right]
\end{aligned}
$$</code></p>

<p>则高斯过程可以表示为：</p>

<p><code>$$
f \left(x\right) \sim \mathcal{GP} \left(m \left(x\right), K \left(x, x'\right)\right)
$$</code></p>

<p>均值函数决定了样本出现的整体位置，如果为零则表示以 <code>$y = 0$</code> 为基准线。协方差函数描述了不同点之间的关系，从而可以利用输入的训练数据预测未知点的值。常用的协方差函数有：</p>

<ul>
<li>常数：<code>$K_c \left(x, x'\right) = C$</code></li>
<li>线性：<code>$K_L \left(x, x'\right) = x^{\top} x'$</code></li>
<li>高斯噪声：<code>$K_{GN} \left(x, x'\right) = \sigma^2 \delta_{x, x'}$</code></li>
<li>指数平方：<code>$K_{\mathrm{SE}}\left(x, x^{\prime}\right)=\exp \left(-\dfrac{|d|^{2}}{2 \ell^{2}}\right)$</code></li>
<li>Ornstein-Uhlenbeck：<code>$K_{\mathrm{OU}}\left(x, x^{\prime}\right)=\exp \left(-\dfrac{|d|}{\ell}\right)$</code></li>
<li>Matérn：<code>$K_{\text {Matern }}\left(x, x^{\prime}\right)=\dfrac{2^{1-\nu}}{\Gamma(\nu)}\left(\dfrac{\sqrt{2 \nu}|d|}{\ell}\right)^{\nu} K_{\nu}\left(\dfrac{\sqrt{2 \nu}|d|}{\ell}\right)$</code></li>
<li>周期：<code>$K_{\mathrm{P}}\left(x, x^{\prime}\right)=\exp \left(-\dfrac{2 \sin ^{2}\left(\dfrac{d}{2}\right)}{\ell^{2}}\right)$</code></li>
<li>有理平方：<code>$K_{\mathrm{RQ}}\left(x, x^{\prime}\right)=\left(1+|d|^{2}\right)^{-\alpha}, \quad \alpha \geq 0$</code></li>
</ul>

<h2 id="高斯过程回归">高斯过程回归</h2>

<p>回归任务的目标是给定一个输入变量 <code>$x \in \mathbb{R}^D$</code> 预测一个或多个连续目标变量 <code>$y$</code> 的值。更确切的说，给定一个包含 <code>$N$</code> 个观测值的训练集 <code>$\mathbf{X} = \left\{x_n\right\}^N_1$</code> 和对应的目标值 <code>$\mathbf{Y} = \left\{y_n\right\}^N_1$</code>，回归的目标是对于一个新的 <code>$x$</code> 预测对应的 <code>$y$</code>。目标值和观测值之间通过一个映射进行关联：</p>

<p><code>$$
f: X \to Y
$$</code></p>

<p>在贝叶斯模型中，我们通过观测数据 <code>$\mathcal{D} = \left\{\left(\mathbf{x}_n, \mathbf{y}_n\right)\right\}^N_{n=1}$</code> 更新先验分布 <code>$P \left(\mathbf{\Theta}\right)$</code>。通过贝叶斯公式我们可以利用先验概率 <code>$P \left(\mathbf{\Theta}\right)$</code> 和似然函数 <code>$P \left(\mathcal{D} | \mathbf{\Theta}\right)$</code> 推导出后验概率：</p>

<p><code>$$
p\left(\mathbf{\Theta} | \mathcal{D}\right)=\frac{p\left(\mathcal{D} | \mathbf{\Theta}\right) p\left(\mathbf{\Theta}\right)}{p\left(\mathcal{D}\right)}
$$</code></p>

<p>其中 <code>$p\left(\mathcal{D}\right)$</code> 为边际似然。在贝叶斯回归中我们不仅希望获得未知输入对应的预测值 <code>$\mathbf{y}_*$</code> ，还希望知道预测的不确定性。因此我们需要利用联合分布和边缘化模型参数 <code>$\mathbf{\Theta}$</code> 来构造预测分布：</p>

<p><code>$$
p\left(\mathbf{y}_{*} | \mathbf{x}_{*}, \mathcal{D}\right)=\int p\left(\mathbf{y}_{*}, \mathbf{\Theta} | \mathbf{x}_{*}, \mathcal{D}\right) \mathrm{d} \Theta=\int p\left(\mathbf{y}_{*} | \mathbf{x}_{*}, \mathbf{\Theta}, \mathcal{D}\right) p(\mathbf{\Theta} | \mathcal{D}) \mathrm{d} \mathbf{\Theta}
$$</code></p>

<p>通常情况下，由于积分形式 <code>$p \left(\Theta | \mathcal{D}\right)$</code> 不具有解析可解性（Analytically Tractable）：</p>

<p><code>$$
p\left(\mathcal{D}\right)=\int p\left(\mathcal{D} | \mathbf{\Theta}\right) p\left(\mathbf{\Theta}\right) d \Theta
$$</code></p>

<p>但在高斯似然和高斯过程先验的前提下，后验采用函数的高斯过程的形式，同时是解析可解的。</p>

<p>对于高斯过程回归，我们构建一个贝叶斯模型，首先定义函数输出的先验为一个高斯过程：</p>

<p><code>$$
p \left(f | \mathbf{X}, \theta\right) = \mathcal{N} \left(\mathbf{0}, K \left(\mathbf{X}, \mathbf{X}\right)\right)
$$</code></p>

<p>其中 <code>$K \left(\cdot, \cdot\right)$</code> 为协方差函数，<code>$\theta$</code> 为过程的超参数。假设数据已经变换为零均值，因此我们不需要在先验中设置均值函数，则令似然形式如下：</p>

<p><code>$$
p \left(\mathbf{Y} | f\right) \sim \mathcal{N} \left(f, \sigma^2_n \mathbf{I}\right)
$$</code></p>

<p>假设观测值为独立同分布的高斯噪音的累加，则整个模型的联合分布为：</p>

<p><code>$$
p \left(\mathbf{Y} , f | \mathbf{X}, \theta\right) = p \left(\mathbf{Y} | f\right) p \left(f | \mathbf{X}, \theta\right)
$$</code></p>

<p>虽然我们并不关心变量 <code>$f$</code>，但由于我们需要对不确定性进行建模，我们仍需考虑 <code>$\mathbf{Y}$</code> 和 <code>$f$</code> 以及 <code>$f$</code> 和 <code>$\mathbf{X}$</code> 之间的关系。高斯过程作为一个非参数模型，其先验分布构建于映射 <code>$f$</code> 之上，<code>$f$</code> 仅依赖于核函数的超参数 <code>$\theta$</code>，且这些超参数可以通过数据进行估计。我们可以将超参数作为先验，即：</p>

<p><code>$$
p \left(\mathbf{Y} , f | \mathbf{X}, \theta\right) = p \left(\mathbf{Y} | f\right) p \left(f | \mathbf{X}, \theta\right) p \left(\theta\right)
$$</code></p>

<p>然后进行贝叶斯推断和模型选择，但是通常情况下这是不可解的。David MacKay 引入了一个利用最优化边际似然来估计贝叶斯平均的框架，即计算如下积分：</p>

<p><code>$$
p \left(\mathbf{Y} | \mathbf{X}, \theta\right) = \int p \left(\mathbf{Y} | f\right) p \left(f | \mathbf{X}, \theta\right) df
$$</code></p>

<p>其中，高斯似然 <code>$p \left(\mathbf{Y} | f\right)$</code> 表示模型拟合数据的程度，<code>$p \left(f | \mathbf{X}, \theta\right)$</code> 为高斯过程先验。经过边缘化后，<code>$\mathbf{Y}$</code> 不在依赖于 <code>$f$</code> 而仅依赖于 <code>$\theta$</code>。</p>

<p>假设采用零均值函数，对于一个高斯过程先验，我们仅需指定一个协方差函数。以指数平方协方差函数为例，选择一系列测试输入点 <code>$X_*$</code>，利用协方差矩阵和测试输入点可以生成一个高斯向量：</p>

<p><code>$$
\mathbf{f}_* \sim \mathcal{N} \left(\mathbf{0}, K \left(X_*, X_*\right)\right)
$$</code></p>

<p>从高斯先验中进行采样，我们首先需要利用标准正态来表示多元正态：</p>

<p><code>$$
\mathbf{f}_* \sim \mu + \mathbf{B} \mathcal{N} \left(0, \mathbf{I}\right)
$$</code></p>

<p>其中，<code>$\mathbf{BB}^{\top} = K \left(X_*, X_*\right)$</code>，<code>$\mathbf{B}$</code> 本质上是协方差矩阵的平方根，可以通过 <a href="https://zh.wikipedia.org/wiki/Cholesky分解" rel="noreferrer" target="_blank">Cholesky 分解</a>获得。</p>

<figure>
  <img data-src="/images/cn/2020-06-06-bayesian-optimization/gp-prior.png" class="lazyload"/>
  
</figure>

<p>上图（左）为从高斯先验中采样的 10 个序列，上图（右）为先验的协方差。如果输入点 <code>$x_n$</code> 和 <code>$x_m$</code> 接近，则对应的 <code>$f \left(x_n\right)$</code> 和 <code>$f \left(x_m\right)$</code> 相比于不接近的点是强相关的。</p>

<p>我们关注的并不是这些随机的函数，而是如何将训练数据中的信息同先验进行合并。假设观测数据为 <code>$\left\{\left(\mathbf{x}_{i}, f_{i}\right) | i=1, \ldots, n\right\}$</code>，则训练目标 <code>$\mathbf{f}$</code> 和测试目标 <code>$\mathbf{f}_*$</code> 之间的联合分布为：</p>

<p><code>$$
\left[\begin{array}{l}
\mathbf{f} \\
\mathbf{f}_{*}
\end{array}\right] \sim \mathcal{N}\left(\mathbf{0},\left[\begin{array}{ll}
K(X, X) &amp; K\left(X, X_{*}\right) \\
K\left(X_{*}, X\right) &amp; K\left(X_{*}, X_{*}\right)
\end{array}\right]\right)
$$</code></p>

<p>根据观测值对联合高斯先验分布进行条件化处理可以得到高斯过程回归的关键预测方程：</p>

<p><code>$$
\mathbf{f}_{*} | X, X_{*}, \mathbf{f} \sim \mathcal{N}\left(\overline{\mathbf{f}}_{*}, \operatorname{cov}\left(\mathbf{f}_{*}\right)\right)
$$</code></p>

<p>其中</p>

<p><code>$$
\begin{aligned}
\overline{\mathbf{f}}_{*} &amp; \triangleq \mathbb{E}\left[\mathbf{f}_{*} | X, X_{*}, \mathbf{f}\right]=K\left(X_{*}, X\right) K(X, X)^{-1} \mathbf{f} \\
\operatorname{cov}\left(\mathbf{f}_{*}\right) &amp;=K\left(X_{*}, X_{*}\right)-K\left(X_{*}, X\right) K(X, X)^{-1} K\left(X, X_{*}\right)
\end{aligned}
$$</code></p>

<p>函数值可以通过对联合后验分布采样获得。</p>

<p>我们以三角函数作为给定的函数，并随机采样一些训练数据 <code>$\left\{\left(\mathbf{x}_{i}, f_{i}\right) | i=1, \ldots, n\right\}$</code>，如下图所示：</p>

<figure>
  <img data-src="/images/cn/2020-06-06-bayesian-optimization/underlying-functions-and-training-points.png" class="lazyload"/>
  
</figure>

<p>我们希望将训练数据和高斯过程先验进行合并得到联合后验分布，我们可以通过在观测值上条件化联合高斯先验分布，预测的均值和协方差为：</p>

<p><code>$$
\begin{aligned}
\overline{\mathbf{f}}_{*} &amp;=K\left(X_{*}, X\right) K(X, X)^{-1} \mathbf{f} \\
\operatorname{cov}\left(\mathbf{f}_{*}\right) &amp;=K\left(X_{*}, X_{*}\right)-K\left(X_{*}, X\right) K(X, X)^{-1} K\left(X, X_{*}\right)
\end{aligned}
$$</code></p>

<p><a href="http://www.gaussianprocess.org/gpml/" rel="noreferrer" target="_blank">Rasmussen 和 Williams</a> 给出了一个实现高斯过程回归的实用方法：</p>



<link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">


<div><pre class="pseudocode">
\begin{algorithm}
\caption{高斯过程回归算法}
\begin{algorithmic}
\REQUIRE \\
    输入 $\mathbf{X}$ \\
    目标 $\mathbf{y}$ \\
    协方差函数 $k$ \\
    噪音水平 $\sigma^2_n$ \\
    测试输入 $\mathbf{x}_*$
\ENSURE \\
    均值 $\bar{f}_*$ \\
    方差 $\mathbb{V}\left[f_{*}\right]$
\FUNCTION{GaussianProcessRegression}{$\mathbf{X}, \mathbf{y}, k, \sigma^2_n, \mathbf{x}_*$}
\STATE $L \gets \text{cholesky} \left(K + \sigma^2_n I\right)$
\STATE $\alpha \gets L^{\top} \setminus \left(L \setminus \mathbf{y}\right)$
\STATE $\bar{f}_* \gets \mathbf{k}^{\top}_* \alpha$
\STATE $\mathbf{v} \gets L \setminus \mathbf{k}_*$
\STATE $\mathbb{V}\left[f_{*}\right] \gets k \left(\mathbf{x}_*, \mathbf{x}_*\right) - \mathbf{v}^{\top} \mathbf{v}$
\RETURN $\bar{f}_*, \mathbb{V}\left[f_{*}\right]$
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}
</pre></div>


<p>高斯过程后验和采样的序列如下图所示：</p>

<figure>
  <img data-src="/images/cn/2020-06-06-bayesian-optimization/gp-posterior.png" class="lazyload"/>
  
</figure>

<p>先验的协方差矩阵和后验的协方差矩阵可视化如下图所示：</p>

<figure>
  <img data-src="/images/cn/2020-06-06-bayesian-optimization/prior-posterior-convariance.png" class="lazyload"/>
  
</figure>

<p>本小结代码请参见<a href="https://github.com/leovan/leovan.me/blob/master/scripts/cn/2020-06-06-bayesian-optimization/gaussian-process-regression.py" rel="noreferrer" target="_blank">这里</a>。</p>

<h2 id="贝叶斯优化">贝叶斯优化</h2>

<h3 id="主动学习">主动学习</h3>

<p>在很多机器学习问题中，数据标注往往需要耗费很大成本。<strong>主动学习（Active Learning）</strong>在最大化模型准确率时最小化标注成本，例如对不确定性最高的数据进行标注。由于我们仅知道少量数据点，因此我们需要一个代理模型（Surrogate Model）来建模真正的模型。高斯过程因其灵活性和具有估计不确定性估计的特性不失为一个常用的代理模型。</p>

<p>在估计 <code>$f \left(x\right)$</code> 的过程中，我们希望最小化评估的次数，因此我们可以通过主动学习来“智能”地选择下一个评估的数据点。通过不断的选择具有最高不确定性的数据点来获得 <code>$f \left(x\right)$</code> 更准确的估计，直至收敛或达到停止条件。下图展示了利用主动学习估计真实数据分布的过程：</p>


  
  <link rel="stylesheet" href="/css/figure-slider.css"/>


<div class="figure-slider">
  <div class="figure-slider-parameters">
    <span class="base-url">/images/cn/2020-06-06-bayesian-optimization/</span>
    <span class="image-filename-prefix">active-gp-</span>
    <span class="image-format">png</span>
    <span class="milliseconds">300</span>
  </div>
  <div class="figure-slider-image-container">
    <img class="figure-slider-image" src='/images/cn/2020-06-06-bayesian-optimization/active-gp-0.png'>
  </div>
  <div class="figure-slider-controls">
    <div class="figure-slider-buttons">
      <button class="figure-slider-button-previous">
        <span class="mdi mdi-skip-previous"></span>
      </button>
    </div>
    <div class="figure-slider-scroll-bar-container">
      <input class="figure-slider-scroll-bar" type="range" min='0' max='9' value='0'>
    </div>
    <div class="figure-slider-buttons">
      <button class="figure-slider-button-next">
        <span class="mdi mdi-skip-next"></span>
      </button>
    </div>
    <div class="figure-slider-buttons">
      <button class="figure-slider-button-play-pause figure-slider-button-play">
        <span class="mdi mdi-play"></span>
      </button>
    </div>
  </div>
</div>


<h3 id="贝叶斯优化问题">贝叶斯优化问题</h3>

<p>贝叶斯优化的核心问题是：基于现有的已知情况，如果选择下一步评估的数据点？在主动学习中我们选择不确定性最大的点，但在贝叶斯优化中我们需要在探索不确定性区域（探索）和关注已知具有较优目标值的区域之间进行权衡（开发）。这种评价的依据称之为<strong>采集函数（Acquisition Functions）</strong>，采集函数通过当前模型启发式的评估是否选择一个数据点。</p>

<p>贝叶斯优化的目标是找到一个函数 <code>$f: \mathbb{R}^d \mapsto \mathbb{R}$</code> 最大值（或最小值）对应的位置 <code>$x \in \mathbb{R}^d$</code>。为了解决这个问题，我们遵循如下算法：</p>

<ol>
<li>选择一个代理模型用于建模真实函数 <code>$f$</code> 和定义其先验。</li>
<li>给定观测集合，利用贝叶斯公式获取后验。</li>
<li>利用采集函数 <code>$\alpha \left(x\right)$</code> 确性下一个采样点 <code>$x_t = \arg\max_x \alpha \left(x\right)$</code>。</li>
<li>将采样的点加入观测集合，重复步骤 2 直至收敛或达到停止条件。</li>
</ol>

<h3 id="采集函数">采集函数</h3>

<ul>
<li>Probability of Improvement (PI)</li>
</ul>

<p>Probability of Improvement (PI) 采集函数会选择具有最大可能性提高当前最大的 <code>$f \left(x^{+}\right)$</code> 值的点作为下一个查询点，即：</p>

<p><code>$$
x_{t+1} = \arg\max \left(\alpha_{PI} \left(x\right)\right) = \arg\max \left(P \left(f \left(x\right)\right) \geq \left(f \left(x^{+}\right) + \epsilon\right)\right)
$$</code></p>

<p>其中，<code>$P \left(\cdot\right)$</code> 表示概率，<code>$\epsilon$</code> 为一个较小的正数，<code>$x^{+} = \arg\max_{x_i \in x_{1:t}} f \left(x_i\right)$</code>，<code>$x_i$</code> 为第 <code>$i$</code> 步查询点的位置。如果采用高斯过程作为代理模型，上式则转变为：</p>

<p><code>$$
x_{t+1} = \arg\max_x \Phi \left(\dfrac{\mu_t \left(x\right) - f \left(x^{+}\right) - \epsilon}{\sigma_t \left(x\right)}\right)
$$</code></p>

<p>其中，<code>$\Phi \left(\cdot\right)$</code> 表示标准正态分布累积分布函数。PI 利用 <code>$\epsilon$</code> 来权衡探索和开发，增加 <code>$\epsilon$</code> 的值会更加倾向进行探索。</p>

<ul>
<li>Expected Improvement (EI)</li>
</ul>

<p>PI 仅关注了有多大的可能性能够提高，而没有关注能够提高多少。Expected Improvement (EI) 则会选择具有最大期望提高的点作为下一个查询点，即：</p>

<p><code>$$
x_{t+1} = \arg\min_x \mathbb{E} \left(\left\|h_{t+1} \left(x\right) - f \left(x^*\right)\right\| | \mathcal{D}_t\right)
$$</code></p>

<p>其中，<code>$f$</code> 为真实函数，<code>$h_{t+1}$</code> 为代理模型在 <code>$t+1$</code> 步的后验均值，<code>$\mathcal{D}_t = \left\{\left(x_i, f\left(x_i\right)\right)\right\}, \forall x \in x_{1:t}$</code> 为训练数据，<code>$x^*$</code> 为 <code>$f$</code> 取得最大值的真实位置。</p>

<p>上式中我们希望选择能够最小化与最大目标值之间距离的点，由于我们并不知道真实函数 <code>$f$</code>，Mockus <sup class="footnote-ref" id="fnref:mockus1991bayesian"><a href="#fn:mockus1991bayesian">1</a></sup> 提出了一种解决办法：</p>

<p><code>$$
x_{t+1} = \arg\max_x \mathbb{E} \left(\max \left\{0, h_{t+1} \left(x\right) - f \left(x^{+}\right)\right\} | \mathcal{D}_t\right)
$$</code></p>

<p>其中，<code>$f \left(x^{+}\right)$</code> 为到目前为止遇见的最大函数值，如果采用高斯过程作为代理模型，上式则转变为：</p>

<p><code>$$
\begin{aligned}
EI(x) &amp;= \left\{\begin{array}{ll}
\left(\mu_{t}(x)-f\left(x^{+}\right)-\epsilon\right) \Phi(Z)+\sigma_{t}(x) \phi(Z), &amp; \text { if } \sigma_{t}(x)&gt;0 \\
0 &amp; \text { if } \sigma_{t}(x)=0
\end{array}\right. \\
Z &amp;= \frac{\mu_{t}(x)-f\left(x^{+}\right)-\epsilon}{\sigma_{t}(x)}
\end{aligned}
$$</code></p>

<p>其中 <code>$\Phi \left(\cdot\right)$</code> 表示标准正态分布累积分布函数，<code>$\phi \left(\cdot\right)$</code> 表示标准正态分布概率密度函数。类似 PI，EI 也可以利用 <code>$\epsilon$</code> 来权衡探索和开发，增加 <code>$\epsilon$</code> 的值会更加倾向进行探索。</p>

<ul>
<li>对比和其他采集函数</li>
</ul>

<figure>
  <img data-src="/images/cn/2020-06-06-bayesian-optimization/pi-vs-ei.svg" class="lazyload"/>
  
</figure>

<p>上图展示了在仅包含一个训练观测数据 <code>$\left(0.5, f \left(0.5\right)\right)$</code> 情况下不同点的采集函数值。可以看出 <code>$\alpha_{EI}$</code> 和 <code>$\alpha_{PI}$</code> 的最大值分别为 0.3 和 0.47。选择一个具有较小的 <code>$\alpha_{PI}$</code> 和一个较大的 <code>$\alpha_{EI}$</code> 的点可以理解为一个高的风险和高的回报。因此，当多个点具有相同的 <code>$\alpha_{EI}$</code> 时，我们应该优先选择具有较小风险（高 <code>$\alpha_{PI}$</code>）的点，类似的，当多个点具有相同的 <code>$\alpha_{PI}$</code> 时，我们应该优先选择具有较大回报（高 <code>$\alpha_{EI}$</code>）的点。</p>

<p>其他采集函数还有 Thompson Sampling <sup class="footnote-ref" id="fnref:thompson1993likelihood"><a href="#fn:thompson1993likelihood">2</a></sup>，Upper Confidence Bound (UCB)，Gaussian Process Upper Confidence Bound (GP-UCB) <sup class="footnote-ref" id="fnref:auer2002using"><a href="#fn:auer2002using">3</a></sup>，Entropy Search <sup class="footnote-ref" id="fnref:hennig2012entropy"><a href="#fn:hennig2012entropy">4</a></sup>，Predictive Entropy Search <sup class="footnote-ref" id="fnref:hern-ndez2014predictive"><a href="#fn:hern-ndez2014predictive">5</a></sup> 等，细节请参见原始论文或 A Tutorial on Bayesian Optimization <sup class="footnote-ref" id="fnref:frazier2018tutorial"><a href="#fn:frazier2018tutorial">6</a></sup>。</p>

<h2 id="开放资源">开放资源</h2>

<ul>
<li><a href="https://github.com/scikit-optimize/scikit-optimize" rel="noreferrer" target="_blank">scikit-optimize/scikit-optimize</a></li>
<li><a href="https://github.com/hyperopt/hyperopt" rel="noreferrer" target="_blank">hyperopt/hyperopt</a></li>
<li><a href="https://github.com/automl/SMAC3" rel="noreferrer" target="_blank">automl/SMAC3</a></li>
<li><a href="https://github.com/fmfn/BayesianOptimization" rel="noreferrer" target="_blank">fmfn/BayesianOptimization</a></li>
<li><a href="https://github.com/pytorch/botorch" rel="noreferrer" target="_blank">pytorch/botorch</a></li>
<li><a href="https://github.com/GPflow/GPflowOpt" rel="noreferrer" target="_blank">GPflow/GPflowOpt</a></li>
<li><a href="https://github.com/keras-team/keras-tuner" rel="noreferrer" target="_blank">keras-team/keras-tuner</a></li>
<li><a href="https://github.com/tobegit3hub/advisor" rel="noreferrer" target="_blank">tobegit3hub/advisor</a></li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:mockus1991bayesian">Mockus, J. B., &amp; Mockus, L. J. (1991). Bayesian approach to global optimization and application to multiobjective and constrained problems. <em>Journal of Optimization Theory and Applications, 70</em>(1), 157-172.
 <a class="footnote-return" href="#fnref:mockus1991bayesian">↩</a></li>
<li id="fn:thompson1993likelihood">Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. <em>Biometrika, 25</em>(<sup>3</sup>&frasl;<sub>4</sub>), 285-294.
 <a class="footnote-return" href="#fnref:thompson1993likelihood">↩</a></li>
<li id="fn:auer2002using">Auer, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. <em>Journal of Machine Learning Research, 3</em>(Nov), 397-422.
 <a class="footnote-return" href="#fnref:auer2002using">↩</a></li>
<li id="fn:hennig2012entropy">Hennig, P., &amp; Schuler, C. J. (2012). Entropy search for information-efficient global optimization. <em>Journal of Machine Learning Research, 13</em>(Jun), 1809-1837.
 <a class="footnote-return" href="#fnref:hennig2012entropy">↩</a></li>
<li id="fn:hern-ndez2014predictive">Hernández-Lobato, J. M., Hoffman, M. W., &amp; Ghahramani, Z. (2014). Predictive entropy search for efficient global optimization of black-box functions. <em>In Advances in neural information processing systems</em> (pp. 918-926).
 <a class="footnote-return" href="#fnref:hern-ndez2014predictive">↩</a></li>
<li id="fn:frazier2018tutorial">Frazier, P. I. (2018). A tutorial on bayesian optimization. <em>arXiv preprint arXiv:1807.02811</em>.
 <a class="footnote-return" href="#fnref:frazier2018tutorial">↩</a></li>
</ol>
</div>



<link rel="stylesheet" href="/css/donate.css" />


<div class="donate">
  <div class="donate-header"></div>
  <div class="donate-slug" id="donate-slug">bayesian-optimization</div>
  <button class="donate-button">赞 赏</button>
  <div class="donate-footer">「真诚赞赏，手留余香」</div>
</div>
<div class="donate-modal-wrapper">
  <div class="donate-modal">
    <div class="donate-box">
      <div class="donate-box-content">
        <div class="donate-box-content-inner">
          <div class="donate-box-header">「真诚赞赏，手留余香」</div>
          <div class="donate-box-body">
            <div class="donate-box-money">
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-2" data-v="2" data-unchecked="￥ 2" data-checked="2 元">￥ 2</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-5" data-v="5" data-unchecked="￥ 5" data-checked="5 元">￥ 5</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-10" data-v="10" data-unchecked="￥ 10" data-checked="10 元">￥ 10</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-50" data-v="50" data-unchecked="￥ 50" data-checked="50 元">￥ 50</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-100" data-v="100" data-unchecked="￥ 100" data-checked="100 元">￥ 100</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-custom" data-v="custom" data-unchecked="任意金额" data-checked="任意金额">任意金额</button>
            </div>
            <div class="donate-box-pay">
              <img class="donate-box-pay-qrcode" id="donate-box-pay-qrcode" src=""/>
            </div>
          </div>
          <div class="donate-box-footer">
            <div class="donate-box-pay-method donate-box-pay-method-checked" data-v="wechat-pay">
              <img class="donate-box-pay-method-image" id="donate-box-pay-method-image-wechat-pay" src=""/>
            </div>
            <div class="donate-box-pay-method" data-v="alipay">
              <img class="donate-box-pay-method-image"  id="donate-box-pay-method-image-alipay" src=""/>
            </div>
          </div>
        </div>
      </div>
    </div>
    <button type="button" class="donate-box-close-button">
      <svg class="donate-box-close-button-icon" fill="#fff" viewBox="0 0 24 24" width="24" height="24"><path d="M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z" fill-rule="evenodd"></path></svg>
    </button>
  </div>
</div>

<script type="text/javascript" src="/js/donate.js"></script>
</script>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/user_blogdown/cn/2020/05/markov-decision-process/">马尔可夫决策过程 (Markov Decision Process)</a></span>
  <span class="nav-next"><a href="/user_blogdown/cn/2020/06/planning-by-dynamic-programming/">利用动态规划求解马尔可夫决策过程 (Planning by Dynamic Programming)</a> &rarr;</span>
</nav>
<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/user_blogdown\/cn\/2020\/05\/markov-decision-process\/';
    
  } else if (e.which == 39) {  
    
    url = '\/user_blogdown\/cn\/2020\/06\/planning-by-dynamic-programming\/';
    
  }
  if (url) window.location = url;
});
</script>




<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-2608165017777396"
     data-ad-slot="1261604535"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>





<section class="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
    this.page.url = "https:\/\/zeqiang.fun" + location.pathname;
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var disqus_js = '//Zeqiang.disqus.com/embed.js';
    var d = document, s = d.createElement('script');
    s.src = disqus_js; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    var t = d.getElementById('disqus_thread');
    var b = false, l = function(scroll) {
      if (b) return;
      (d.head || d.body).appendChild(s); b = true;
      if (scroll) t.scrollIntoView();
    }
    s.onerror = function(e) {
      if (sessionStorage.getItem('failure-note')) return;
      t.innerText = 'Sorry, but you cannot make comments because Disqus failed to load for some reason. It is known to be blocked in China. If you are sure it is not blocked in your region, please refresh the page. 中国大陆地区读者需要翻墙才能发表评论。';
      sessionStorage.setItem('failure-note', true);
    };
    
    if (location.hash.match(/^#comment-[0-9]+$/)) return l(true);
    var c = function() {
      if (b) return;
      var rect = t.getBoundingClientRect();
      if (rect.top < window.innerHeight && rect.bottom >= 0) l();
    };
    window.addEventListener('load', c);
    d.addEventListener('scroll', c);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>





<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/autoloader/prism-autoloader.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/show-language/prism-show-language.min.js"></script>

<script>
    (function() {
        if (!self.Prism) {
            return;
        }

        Prism.languages.dos = Prism.languages.powershell;
        Prism.languages.gremlin = Prism.languages.groovy;

        var Languages = {
            'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',
            'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',
            'powershell': 'PowerShell', 'javascript': 'JavaScript',
            'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',
            'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',
            'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',
            'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration'
        };

        Prism.hooks.add('before-highlight', function(env) {
        	var language = Languages[env.language] || env.language;
        	env.element.setAttribute('data-language', language);
        });
    })();
</script>




<script async src="/js/fix-toc.js"></script>
<script async src="/js/center-img.js"></script>
<script async src="/js/right-quote.js"></script>
<script async src="/js/fix-footnote.js"></script>
<script async src="/js/external-link.js"></script>
<script async src="/js/alt-title.js"></script>
<script src="/js/no-highlight.js"></script>
<script src="/js/math-code.js"></script>


<script>
window.MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js" crossorigin></script>


<script src="//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
    var captionCount = 0;
    $(".pseudocode").each(function() {
        var pseudocode_options = {
            indentSize: '1.2em',
            commentDelimiter: '\/\/',
            lineNumber:  true ,
            lineNumberPunc: ':',
            noEnd:  false 
        };
        pseudocode_options.captionCount = captionCount;
        captionCount += 1;
        var codeEl = $(this).get(0);
        pseudocode.render(codeEl.textContent, codeEl.parentElement, pseudocode_options);
    });
});
</script>






<script async src="/js/load-typekit.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lazysizes/5.3.2/lazysizes.min.js"></script>

<script src="//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js"></script>
<script>
addBackToTop({
  diameter: 48
})
</script>


<script src="/js/figure-slider.js"></script>


  <hr>
  <div class="copyright no-border-bottom">
    <div class="copyright-author-year">
      <span>&copy; 2017-2021 Leo Van</span>
    </div>
    <div class="copyright-links">
      <a href="https://github.com/leovan" rel="noreferrer" target="_blank">Github</a>
      <span> · </span>
      <a href="https://orcid.org/0000-0002-9556-7821" rel="noreferrer" target="_blank">ORCID</a>
      <span> · </span>
      <span>I am Mr. Black.</span>
    </div>
  </div>
  </footer>
  </article>
  </body>
</html>


<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

        <title>卷积神经网络 (Convolutional Neural Network, CNN) - Zeqiang Fang | 方泽强</title>

    <meta name="referrer" content="no-referrer">
    
    <meta property="og:title" content="卷积神经网络 (Convolutional Neural Network, CNN) - Zeqiang Fang | 方泽强">
    <meta name="description" property="og:description" content="发展史 卷积神经网络 (Convolutional Neural Network, CNN) 是一种目前广泛用于图像，自然语言处理等领域的深度神经网络模型。1998 年，Lecun 等人 1 提出了一种基于梯度的反向">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Zeqiang Fang | 方泽强">
    <meta property="og:url" content="https://leovan.me/cn/2018/08/cnn/">

    
    
    
    
    <meta name="author" property="article:author" content="范叶亮">
    
    
    
    <meta name="date" property="article:published_time" content="2018-08-25" scheme="YYYY-MM-DD">
    
    
    <meta name="date" property="article:modified_time" content="2018-08-25" scheme="YYYY-MM-DD">
    

    
    <meta name="keywords" property="article:tag" content ="卷积神经网络,Convolutional Neural Network,CNN">
    
    
    <meta name="theme-color" content="#0d0d0d">
    
    <link rel="icon" type="image/png" sizes="16x16" href="/images/web/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/web/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/images/web/favicon-48x48.png">
    <link rel="icon" type="image/png" sizes="62x62" href="/images/web/favicon-62x62.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/images/web/favicon-192x192.png">
    <link rel="apple-touch-icon" size="192x192" href="/images/web/icon-192x192.png">
    <link rel="manifest" href="/manifest.json">
        
    

    

    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://leovan.me/cn"
        },
        "name": "卷积神经网络 (Convolutional Neural Network, CNN)",
        "headline": "卷积神经网络 (Convolutional Neural Network, CNN)",
        "description" : "发展史 卷积神经网络 (Convolutional Neural Network, CNN) 是一种目前广泛用于图像，自然语言处理等领域的深度神经网络模型。1998 年，Lecun 等人 1 提出了一种基于梯度的反向",
        "genre": [
            "深度学习"
        ],
        "datePublished": "2018-08-25",
        "dateModified": "2018-08-25",
        "wordCount": "8965",
        "keywords": [
            "卷积神经网络", "Convolutional Neural Network", "CNN"
        ],
        "image": [
            "https://leovan.me/images/cn/2018-08-25-cnn/lenet-5.png", "https://leovan.me/images/cn/2018-08-25-cnn/conv-example.png", "https://leovan.me/images/cn/2018-08-25-cnn/conv-zero-padding.png", "https://leovan.me/images/cn/2018-08-25-cnn/conv2d-kernels.png", "https://leovan.me/images/cn/2018-08-25-cnn/conv3d-kernels.png", "https://leovan.me/images/cn/2018-08-25-cnn/sparse-interactions.png", "https://leovan.me/images/cn/2018-08-25-cnn/indirect-interactions.png", "https://leovan.me/images/cn/2018-08-25-cnn/parameter-sharing.png", "https://leovan.me/images/cn/2018-08-25-cnn/alexnet.png", "https://leovan.me/images/cn/2018-08-25-cnn/vgg-16.png", "https://leovan.me/images/cn/2018-08-25-cnn/network-in-network.png", "https://leovan.me/images/cn/2018-08-25-cnn/global-average-pooling.png", "https://leovan.me/images/cn/2018-08-25-cnn/googlenet.png", "https://leovan.me/images/cn/2018-08-25-cnn/inception-v3-v1-3x3.png", "https://leovan.me/images/cn/2018-08-25-cnn/inception-v3-1xn-nx1.png", "https://leovan.me/images/cn/2018-08-25-cnn/inception-v3-reducing-gird-size-old.png", "https://leovan.me/images/cn/2018-08-25-cnn/inception-v3-reducing-gird-size-new.png", "https://leovan.me/images/cn/2018-08-25-cnn/residual-network.png", "https://leovan.me/images/cn/2018-08-25-cnn/residual-block.png", "https://leovan.me/images/cn/2018-08-25-cnn/residual-results.png", "https://leovan.me/images/cn/2018-08-25-cnn/identity-mapping-residual-network-unit.png", "https://leovan.me/images/cn/2018-08-25-cnn/densenet.png", "https://leovan.me/images/cn/2018-08-25-cnn/cnn-accuracy-and-parameters.png"
        ],
        "author": {
            "@type": "Person",
            "name": "范叶亮"
        },
        "publisher": {
            "@type": "Organization",
            "name": "范叶亮",
            "logo": {
                "@type": "ImageObject",
                "url": "https://leovan.me/images/web/publisher-logo.png"
            }
        },
        "url": "https://leovan.me/cn/2018/08/cnn/"
    }
    </script>
    

    <script src='//cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js'></script>
<script src='//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js'></script>

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/6.5.95/css/materialdesignicons.min.css">





<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<link rel="stylesheet" type="text/css" href="/css/fonts.css" />
<link rel="stylesheet" type="text/css" href="/css/style.css">
<link rel="stylesheet" type="text/css" href="/css/light.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" id="dark-mode-style" disabled="disabled">
<link rel="stylesheet" type="text/css" href="/css/icons.css">
<link rel="stylesheet" type="text/css" href="/css/print.css">

  </head>

  
  <body class="cn">
    <header class="masthead">
      

<div class="logo"></div>
<p class="slogan">优雅永不过时</p>
      <nav class="menu">
  <ul>
  
  
  
    
  
  
    <li><a href="/">首页</a></li>
  
    <li><a href="/cn/">博客</a></li>
  
    <li><a href="/categories/">分类</a></li>
  
    <li class="menu-separator"><span>&nbsp;</span></li>
  
    <li><a href="/cn/about/">关于</a></li>
  
    <li><a href="/cn/resume/">简历</a></li>
  
  


<li class="menu-separator"><span>&nbsp;</span></li>

<li><a href="/cn/index.xml" target="_blank" type="application/rss+xml" title="RSS feed">订阅</a></li>

<li><a href="https://github.com/fang-zeqiang/fang-zeqiang.github.io/blob/master/LICENSE" target="_blank" title="Attribution-NonCommercial-ShareAlike 4.0 International">版权</a></li>


  <li class="light-dark-mode no-border-bottom"><a id="light-dark-mode-action"><span id="light-dark-mode-icon" class="mdi mdi-weather-night"></span></a></li>
  </ul>
</nav>

      <script src="/js/toggle-theme.js"></script>
    </header>

    <article class="main">
      <header class="title">
      
<h1>卷积神经网络 (Convolutional Neural Network, CNN)</h1>







<h3>范叶亮 / 
2018-08-25</h3>



<h3 class="post-meta">


<strong>分类: </strong>
<a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>




/




<strong>标签: </strong>
<span>卷积神经网络</span>, <span>Convolutional Neural Network</span>, <span>CNN</span>




/


<strong>字数: </strong>
8965
</h3>



<hr>


      </header>






<h2 id="发展史">发展史</h2>

<p>卷积神经网络 (Convolutional Neural Network, CNN) 是一种目前广泛用于图像，自然语言处理等领域的深度神经网络模型。1998 年，Lecun 等人 <sup class="footnote-ref" id="fnref:lecun1998gradient"><a href="#fn:lecun1998gradient">1</a></sup> 提出了一种基于梯度的反向传播算法用于文档的识别。在这个神经网络中，卷积层 (Convolutional Layer) 扮演着至关重要的角色。</p>

<p>随着运算能力的不断增强，一些大型的 CNN 网络开始在图像领域中展现出巨大的优势，2012 年，Krizhevsky 等人 <sup class="footnote-ref" id="fnref:krizhevsky2012imagenet"><a href="#fn:krizhevsky2012imagenet">2</a></sup> 提出了 AlexNet 网络结构，并在 ImageNet 图像分类竞赛 <sup class="footnote-ref" id="fnref:imagenet"><a href="#fn:imagenet">3</a></sup> 中以超过之前 11% 的优势取得了冠军。随后不同的学者提出了一系列的网络结构并不断刷新 ImageNet 的成绩，其中比较经典的网络包括：VGG (Visual Geometry  Group) <sup class="footnote-ref" id="fnref:simonyan2014very"><a href="#fn:simonyan2014very">4</a></sup>，GoogLeNet <sup class="footnote-ref" id="fnref:szegedy2015going"><a href="#fn:szegedy2015going">5</a></sup> 和 ResNet <sup class="footnote-ref" id="fnref:he2016deep"><a href="#fn:he2016deep">6</a></sup>。</p>

<p>CNN 在图像分类问题上取得了不凡的成绩，同时一些学者也尝试将其应用在图像的其他领域，例如：物体检测 <sup class="footnote-ref" id="fnref:girshick2014rich"><a href="#fn:girshick2014rich">7</a></sup><sup class="footnote-ref" id="fnref:girshick2015fast"><a href="#fn:girshick2015fast">8</a></sup><sup class="footnote-ref" id="fnref:ren2015faster"><a href="#fn:ren2015faster">9</a></sup>，语义分割 <sup class="footnote-ref" id="fnref:long2015fully"><a href="#fn:long2015fully">10</a></sup>，图像摘要 <sup class="footnote-ref" id="fnref:vinyals2015show"><a href="#fn:vinyals2015show">11</a></sup>，行为识别 <sup class="footnote-ref" id="fnref:ji20133d"><a href="#fn:ji20133d">12</a></sup> 等。除此之外，在非图像领域 CNN 也取得了一定的成绩 <sup class="footnote-ref" id="fnref:kim2014convolutional"><a href="#fn:kim2014convolutional">13</a></sup>。</p>

<h2 id="模型原理">模型原理</h2>

<p>下图为 Lecun 等人提出的 LeNet-5 的网络架构：</p>

<p><img src="/images/cn/2018-08-25-cnn/lenet-5.png" alt="LeNet-5" /></p>

<p>下面我们针对 CNN 网络中的不同类型的网络层逐一进行介绍。</p>

<h3 id="输入层">输入层</h3>

<p>LeNet-5 解决的手写数字分类问题的输入为一张 32x32 像素的灰度图像 (Gray Scale)。日常生活中计算机常用的图像的表示方式为 RGB，即将一张图片分为红色通道 (Red Channel)，绿色通道 (Green Channel) 和蓝色通道 (Blue Channel)，其中每个通道的每个像素点的数值范围为 <code>$\left[0, 255\right]$</code>。灰度图像表示该图片仅包含一个通道，也就是不具备彩色信息，每个像素点的数值范围同 RGB 图像的取值范围相同。</p>

<p>因此，一张图片在计算机的眼里就是一个如下图所示的数字矩阵 (示例图片来自于 MNIST 数据集 <sup class="footnote-ref" id="fnref:mnist"><a href="#fn:mnist">14</a></sup>)：</p>

<p><img src="/images/cn/2018-08-25-cnn/digit-pixels.png" alt="Digit-Pixels" /></p>

<p>在将图像输入到 CNN 网络之前，通常我们会对其进行预处理，因为每个像素点的最大取值为 <code>$255$</code>，因此将每个像素点的值除以 <code>$255$</code> 则可以将其归一化到 <code>$\left[0, 1\right]$</code> 的范围。</p>

<h3 id="卷积层">卷积层</h3>

<p>在了解卷积层之前，让我们先来了解一下什么是卷积？设 <code>$f\left(x\right), g\left(x\right)$</code> 是 <code>$\mathbb{R}$</code> 上的两个可积函数，则卷积定义为：</p>

<p><code>$$
\left(f * g\right) \left(x\right) = \int_{- \infty}^{\infty}{f \left(\tau\right) g \left(x - \tau\right) d \tau}
$$</code></p>

<p>离散形式定义为：</p>

<p><code>$$
\left(f * g\right) \left(x\right) = \sum_{\tau = - \infty}^{\infty}{f \left(\tau\right) g \left(x - \tau\right)}
$$</code></p>

<p>我们用一个示例来形象的理解一下卷积的含义，以离散的形式为例，假设我们有两个骰子，<code>$f\left(x\right), g\left(x\right)$</code> 分别表示投两个骰子，<code>$x$</code> 面朝上的概率。</p>

<p><code>$$
f \left(x\right) = g \left(x\right) = \begin{cases}
1/6 &amp; x = 1, 2, 3, 4, 5, 6 \\
0 &amp; \text{otherwise}
\end{cases}
$$</code></p>

<p>卷积 <code>$\left(f * g\right) \left(x\right)$</code> 表示投两个骰子，朝上数字之和为 <code>$x$</code> 的概率。则和为 <code>$4$</code> 的概率为：</p>

<p><code>$$
\begin{equation}
\begin{split}
\left(f * g\right) \left(4\right) &amp;= \sum_{\tau = 1}^{6}{f \left(\tau\right) g \left(4 - \tau\right)} \\
&amp;= f \left(1\right) g \left(4 - 1\right) + f \left(2\right) g \left(4 - 2\right) + f \left(3\right) g \left(4 - 3\right) \\
&amp;= 1/6 \times 1/6 + 1/6 \times 1/6 + 1/6 \times 1/6 \\
&amp;= 1/12
\end{split}
\end{equation}
$$</code></p>

<p>这是一维的情况，我们处理的图像为一个二维的矩阵，因此类似的有：</p>

<p><code>$$
\left(f * g\right) \left(x, y\right) = \sum_{v = - \infty}^{\infty}{\sum_{h = - \infty}^{\infty}{f \left(h, v\right) g \left(x - h, y - v\right)}}
$$</code></p>

<p>这次我们用一个抽象的例子解释二维情况下卷积的计算，设 <code>$f, g$</code> 对应的概率矩阵如下：</p>

<p><code>$$
f =
\left[
    \begin{array}{ccc}
        \color{red}{a_{0, 0}} &amp; \color{orange}{a_{0, 1}} &amp; \color{yellow}{a_{0, 2}} \\
        \color{green}{a_{1, 0}} &amp; \color{cyan}{a_{1, 1}} &amp; \color{blue}{a_{1, 2}} \\
        \color{purple}{a_{2, 0}} &amp; \color{black}{a_{2, 1}} &amp; \color{gray}{a_{2, 2}}
    \end{array}
\right]
,
g =
\left[
    \begin{array}{ccc}
        \color{gray}{b_{-1, -1}} &amp; \color{black}{b_{-1, 0}} &amp; \color{purple}{b_{-1, 1}} \\
        \color{blue}{b_{0, -1}} &amp; \color{cyan}{b_{0, 0}} &amp; \color{green}{b_{0, 1}} \\
        \color{yellow}{b_{1, -1}} &amp; \color{orange}{b_{1, 0}} &amp; \color{red}{b_{1, 1}}
    \end{array}
\right]
$$</code></p>

<p>则 <code>$\left(f * g\right) \left(1, 1\right)$</code> 计算方式如下：</p>

<p><code>$$
\left(f * g\right) \left(1, 1\right) = \sum_{v = 0}^{2}{\sum_{h = 0}^{2}{f \left(h, v\right) g \left(1 - h, 1 - v\right)}}
$$</code></p>

<p>从这个计算公式中我们就不难看出为什么上面的 <code>$f, g$</code> 两个概率矩阵的角标会写成上述形式，即两个矩阵相同位置的角标之和均为 <code>$1$</code>。<code>$\left(f * g\right) \left(1, 1\right)$</code> 即为 <code>$f, g$</code> 两个矩阵中对应颜色的元素乘积之和。</p>

<p>在上例中，<code>$f, g$</code> 两个概率矩阵的大小相同，而在 CNN 中，<code>$f$</code> 为输入的图像，<code>$g$</code> 一般是一个相对较小的矩阵，我们称之为卷积核。这种情况下，卷积的计算方式是类似的，只是会将 <code>$g$</code> 矩阵旋转 <code>$180^{\circ}$</code> 使得相乘的元素的位置也相同，同时需要 <code>$g$</code> 在 <code>$f$</code> 上进行滑动并计算对应位置的卷积值。下图 <sup class="footnote-ref" id="fnref:conv-images"><a href="#fn:conv-images">15</a></sup> 展示了一步计算的具体过程：</p>

<p><img src="/images/cn/2018-08-25-cnn/conv-example.png" alt="Conv-Example" /></p>

<p>下图 <sup class="footnote-ref" id="fnref:conv-images"><a href="#fn:conv-images">15</a></sup> 形象的刻画了利用一个 3x3 大小的卷积核的整个卷积计算过程：</p>

<p><img src="/images/cn/2018-08-25-cnn/conv-sobel.gif" alt="Conv-Sobel" /></p>

<p>一些预设的卷积核对于图片可以起到不同的滤波器效果，例如下面 4 个卷积核分别会对图像产生不同的效果：不改变，边缘检测，锐化和高斯模糊。</p>

<p><code>$$
\left[
    \begin{array}{ccc}
        0 &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; 0 \\
        0 &amp; 0 &amp; 0
    \end{array}
\right]
,
\left[
    \begin{array}{ccc}
        -1 &amp; -1 &amp; -1 \\
        -1 &amp;  8 &amp; -1 \\
        -1 &amp; -1 &amp; -1
    \end{array}
\right]
,
\left[
    \begin{array}{ccc}
        0  &amp; -1 &amp; 0 \\
        -1 &amp;  5 &amp; -1 \\
        0  &amp; -1 &amp; 0
    \end{array}
\right]
,
\dfrac{1}{16} \left[
    \begin{array}{ccc}
        1 &amp; 2 &amp; 1 \\
        2 &amp; 4 &amp; 2 \\
        1 &amp; 2 &amp; 1
    \end{array}
\right]
$$</code></p>

<p>对 lena 图片应用这 4 个卷积核，变换后的效果如下 (从左到右，从上到下)：</p>

<p><img src="/images/cn/2018-08-25-cnn/lena-filters.png" alt="Lena-Filters" /></p>

<p>在上面整个计算卷积的动图中，我们不难发现，利用 3x3 大小 (我们一般将这个参数称之为 <code>kernel_size</code>，即<strong>卷积核的大小</strong>，其可以为一个整数表示长宽大小相同，也可以为两个不同的整数) 的卷积核对 5x5 大小的原始矩阵进行卷积操作后，结果矩阵并没有保持原来的大小，而是变为了 (5-(3-1))x(5-(3-1)) (即 3x3) 大小的矩阵。这就需要引入 CNN 网络中卷积层的两个常用参数 <code>padding</code> 和 <code>strides</code>。</p>

<p><code>padding</code> 是指是否对图像的外侧进行<strong>补零操作</strong>，其取值一般为 <code>VALID</code> 和 <code>SAME</code> 两种。<code>VALID</code> 表示<strong>不进行补零</strong>操作，对于输入形状为 <code>$\left(x, y\right)$</code> 的矩阵，利用形状为 <code>$\left(m, n\right)$</code> 的卷积核进行卷积，得到的结果矩阵的形状则为 <code>$\left(x-m+1, y-n+1\right)$</code>。<code>SAME</code> 表示<strong>进行补零</strong>操作，在进行卷积操作前，会对图像的四个边缘分别向左右补充 <code>$\left(m \mid 2 \right) + 1$</code> 个零，向上下补充 <code>$\left(n \mid 2 \right) + 1$</code> 个零 (<code>$\mid$</code> 表示整除)，从而保证进行卷积操作后，结果的形状与原图像的形状保持相同，如下图 <sup class="footnote-ref" id="fnref:conv-images"><a href="#fn:conv-images">15</a></sup> 所示：</p>

<p><img src="/images/cn/2018-08-25-cnn/conv-zero-padding.png" alt="Conv2d-Zero-Padding" /></p>

<p><code>strides</code> 是指进行卷积操作时，每次卷积核移动的步长。示例中，卷积核在横轴和纵轴方向上的移动步长均为 <code>$1$</code>，除此之外用于也可以指定不同的步长。移动的步长同样会对卷积后的结果的形状产生影响。</p>

<p>除此之外，还有另一个重要的参数 <code>filters</code>，其表示在一个卷积层中使用的<strong>卷积核的个数</strong>。在一个卷积层中，一个卷积核可以学习并提取图像的一种特征，但往往图片中包含多种不同的特征信息，因此我们需要多个不同的卷积核提取不同的特征。下图 <sup class="footnote-ref" id="fnref:conv-images"><a href="#fn:conv-images">15</a></sup> 是一个利用 4 个不同的卷积核对一张图像进行卷积操作的示意图：</p>

<p><img src="/images/cn/2018-08-25-cnn/conv2d-kernels.png" alt="Conv2d-Kernels" /></p>

<p>上面我们都是以一个灰度图像 (仅包含 1 个通道) 为示例进行的讨论，那么对于一个 RGB 图像 (包含 3 个通道)，相应的，卷积核也是一个 3 维的形状，如下图 <sup class="footnote-ref" id="fnref:conv-images"><a href="#fn:conv-images">15</a></sup> 所示：</p>

<p><img src="/images/cn/2018-08-25-cnn/conv3d-kernels.png" alt="Conv3d-Kernels" /></p>

<p>卷积层对于我们的神经网络的模型带来的改进主要包括如下三个方面：<strong>稀疏交互 (sparse interactions)</strong>，<strong>参数共享 (parameter sharing)</strong> 和<strong>等变表示 (equivariant representations)</strong>。</p>

<p>在全连接的神经网络中，隐含层中的每一个节点都和上一层的所有节点相连，同时有被连接到下一层的全部节点。而卷积层不同，节点之间的连接性受到卷积核大小的制约。下图 <sup class="footnote-ref" id="fnref:deep-learning"><a href="#fn:deep-learning">16</a></sup> 分别以自下而上 (左) 和自上而下 (右) 两个角度对比了卷积层和全连接层节点之间连接性的差异。</p>

<p><img src="/images/cn/2018-08-25-cnn/sparse-interactions.png" alt="Sparse-Interactions" /></p>

<p>在上图 (右) 中，我们可以看出节点 <code>$s_3$</code> 受到节点 <code>$x_2$</code>，<code>$x_3$</code> 和 <code>$x_4$</code> 的影响，这些节点被称之为 <code>$s_3$</code> 的<strong>接受域 (receptive field)</strong>。稀疏交互使得在 <code>$m$</code> 个输入和 <code>$n$</code> 个输出的情况下，参数的个数由 <code>$m \times n$</code> 个减少至 <code>$k \times n$</code> 个，其中 <code>$k$</code> 为卷积核的大小。尽管一个节点在一个层级之间仅与其接受域内的节点相关联，但是对于深层中的节点，其与绝大部分输入之间却存在这<strong>间接交互</strong>，如下图 <sup class="footnote-ref" id="fnref:deep-learning"><a href="#fn:deep-learning">16</a></sup> 所示：</p>

<p><img src="/images/cn/2018-08-25-cnn/indirect-interactions.png" alt="Indirect-Interactions" /></p>

<p>节点 <code>$g_3$</code> 尽管<strong>直接</strong>的连接是稀疏的，但处于更深的层中可以<strong>间接</strong>的连接到全部或者大部分的输入节点。这就使得网络可以仅通过这种稀疏交互来高效的描述多个输入变量之间的复杂关系。</p>

<p>除了稀疏交互带来的参数个数减少外，<strong>参数共享</strong>也起到了类似的作用。所谓参数共享就是指在进行不同操作时使用相同的参数，具体而言也就是在我们利用卷积核在图像上滑动计算卷积时，每一步使用的卷积核都是相同的。同全连接网络的情况对比如下图 <sup class="footnote-ref" id="fnref:deep-learning"><a href="#fn:deep-learning">16</a></sup> 所示：</p>

<p><img src="/images/cn/2018-08-25-cnn/parameter-sharing.png" alt="Parameter-Sharing" /></p>

<p>在全连接网络 (上图 - 下) 中，任意两个节点之间的连接 (权重) 仅用于这两个节点之间，而在卷积层中，如上图所示，其对卷积核中间节点 (黑色箭头) 的使用方式 (权重) 是相同的。参数共享虽然对于计算的时间复杂度没有带来改进，仍然是 <code>$O \left(k \times n\right)$</code>，但其却将参数个数降低至 <code>$k$</code> 个。</p>

<p>正是由于参数共享机制，使得卷积层具有平移 <strong>等变 (equivariance)</strong> 的性质。对于函数 <code>$f\left(x\right)$</code> 和 <code>$g\left(x\right)$</code>，如果满足 <code>$f\left(g\left(x\right)\right) = g\left(f\left(x\right)\right)$</code>，我们就称 <code>$f\left(x\right)$</code> 对于变换 <code>$g$</code> 具有等变性。简言之，对于图像如果我们将所有的像素点进行移动，则卷积后的输出表示也会移动同样的量。</p>

<h3 id="非线性层">非线性层</h3>

<p>非线性层并不是 CNN 特有的网络层，在此我们不再详细介绍，一般情况下我们会使用 ReLU 作为我们的激活函数。</p>

<h3 id="池化层">池化层</h3>

<p><strong>池化层</strong> 是一个利用 <strong>池化函数 (pooling function)</strong> 对网络输出进行进一步调整的网络层。池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。常用的池化函数包括最大池化 (max pooling) 函数 (即给出邻域内的最大值) 和平均池化 (average pooling) 函数 (即给出邻域内的平均值) 等。但无论选择何种池化函数，当对输入做出少量平移时，池化对输入的表示都近似 <strong>不变 (invariant)</strong>。<strong>局部平移不变性</strong> 是一个很重要的性质，尤其是当我们关心某个特征是否出现而不关心它出现的位置时。</p>

<p>池化层同卷积层类似，具有三个比较重要的参数：<code>pool_size</code>，<code>strides</code> 和 <code>padding</code>，分别表示池化窗口的大小，步长以及是否对图像的外侧进行补零操作。下图 <sup class="footnote-ref" id="fnref:deep-learning"><a href="#fn:deep-learning">16</a></sup> 是一个 <code>pool_size=3</code>，<code>strides=3</code>，<code>padding='valid'</code> 的最大池化过程示例：</p>

<p><img src="/images/cn/2018-08-25-cnn/max-pooling.gif" alt="Max-Pooling" /></p>

<p>池化层同时也能够提高网络的计算效率，例如上图中在横轴和纵轴的步长均为 <code>$3$</code>，经过池化后，下一层网络节点的个数降低至前一层的 <code>$\frac{1}{3 \times 3} = \frac{1}{9}$</code>。</p>

<h3 id="全连接层">全连接层</h3>

<p>全链接层 (Fully-connected or Dense Layer) 的目的就是将我们最后一个池化层的输出连接到最终的输出节点上。例如，最后一个池化层的输出大小为 <code>$\left[5 \times 5 \times 16\right]$</code>，也就是有 <code>$5 \times 5 \times 16 = 400$</code> 个节点，对于手写数字识别的问题，我们的输出为 0 至 9 共 10 个数字，采用 one-hot 编码的话，输出层共 10 个节点。例如在 LeNet 中有 2 个全连接层，每层的节点数分别为 120 和 84，在实际应用中，通常全连接层的节点数会逐层递减。需要注意的是，在进行编码的时候，第一个全连接层并不是直接与最后一个池化层相连，而是先对池化层进行 flatten 操作，使其变成一个一维向量后再与全连接层相连。</p>

<h3 id="输出层">输出层</h3>

<p>输出层根据具体问题的不同会略有不同，例如对于手写数字识别问题，采用 one-hot 编码的话，输出层则包含 10 个节点。对于回归或二分类问题，输出层则仅包含 1 个节点。当然对于二分类问题，我们也可以像多分类问题一样将其利用 one-hot 进行编码，例如 <code>$\left[1, 0\right]$</code> 表示类型 0，<code>$\left[0, 1\right]$</code> 表示类型 1。</p>

<h2 id="扩展与应用">扩展与应用</h2>

<p>本节我们将介绍一些经典的 CNN 网络架构及其相关的改进。</p>

<h3 id="alexnet-krizhevsky2012imagenet">AlexNet <sup class="footnote-ref" id="fnref:krizhevsky2012imagenet"><a href="#fn:krizhevsky2012imagenet">2</a></sup></h3>

<p><img src="/images/cn/2018-08-25-cnn/alexnet.png" alt="AlexNet" /></p>

<p>AlexNet 在整体结构上同 LeNet-5 类似，其改进大致如下：</p>

<ul>
<li>网络包含了 5 个卷积层和 3 个全连接层，网络规模变大。</li>
<li>使用了 ReLU 非线性激活函数。</li>
<li>应用了 Data Augmentation，Dropout，Momentum，Weight Decay 等策略改进训练。</li>
<li>在算力有限的情况下，对模型进行划分为两部分并行计算。</li>
<li>增加局部响应归一化 (LRN, Local Response Normalization)。</li>
</ul>

<p>LRN 的思想来自与生物学中侧抑制 (Lateral Inhibition) 的概念，简单来说就是相近的神经元之间会发生抑制作用。在 AlexNet 中，给出的 LRN 计算公式如下：</p>

<p><code>$$
b_{x,y}^{i} = a_{x,y}^{i} / \left(k + \alpha \sum_{j = \max \left(0, i - n/2\right)}^{\min \left(N - 1, i + n/2\right)}{\left(a_{x,y}^{j}\right)^2}\right)^{\beta}
$$</code></p>

<p>其中，<code>$a_{x,y}^{i}$</code> 表示第 <code>$i$</code> 个卷积核在位置 <code>$\left(x,y\right)$</code> 的输出，<code>$N$</code> 为卷积核的个数，<code>$k, n, \alpha, \beta$</code> 均为超参数，在原文中分别初值为：<code>$k=2, n=5, \alpha=10^{-4}, \beta=0.75$</code>。在上式中，分母为所有卷积核 (Feature Maps) 的加和，因此 LRN 可以简单理解为一个跨 Feature Maps 的像素级归一化。</p>

<p><strong>开源实现</strong>：</p>

<ul>
<li><i class="icon icon-tensorflow"></i> <a href="https://github.com/tensorflow/models/tree/master/research" rel="noreferrer" target="_blank">tensorflow/models</a>,  <a href="https://github.com/tflearn/tflearn/blob/master/examples/images" rel="noreferrer" target="_blank">tflearn/examples</a></li>
<li><i class="icon icon-pytorch"></i> <a href="https://pytorch.org/docs/stable/torchvision/models.html" rel="noreferrer" target="_blank">pytorch/torchvision/models</a></li>
<li><i class="icon icon-caffe2"></i> <a href="https://github.com/caffe2/models" rel="noreferrer" target="_blank">caffe2/models</a></li>
<li><i class="icon icon-mxnet"></i> <a href="https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/symbols" rel="noreferrer" target="_blank">incubator-mxnet/example</a></li>
</ul>

<h3 id="vgg-net-simonyan2014very">VGG Net <sup class="footnote-ref" id="fnref:simonyan2014very"><a href="#fn:simonyan2014very">4</a></sup></h3>

<p><img src="/images/cn/2018-08-25-cnn/vgg-16.png" width="180" style="float:left; margin-right:3em;"/></p>

<p>左图是 VGG-16 Net 的网络结构，原文中还有一个 VGG-19 Net，其差别在于后面三组卷积层中均多叠加了一个卷积层，使得网络层数由 16 增加至 19。</p>

<p>VGG Net 的主要改变如下：</p>

<ul>
<li>网络层级更深，从 AlexNet 的 8 层增加至 16 层和 19 层，更深的网络层级意味着更强的学习能力，但也需要更多的算力对模型进行优化。</li>
<li>仅使用 3x3 大小的卷积。在 AlexNet 中，浅层部分使用了较大的卷积核，而 VGG 使用了 3x3 的小卷积核进行串联叠加，减少了参数个数。</li>
<li>卷积采样的步长为 1x1，Max Pooling 的步长为 2x2。</li>
<li>去掉了效果提升不明显的但计算耗时的 LRN。</li>
<li>增加了 Feature Maps 的个数。</li>
</ul>

<p><strong>开源实现</strong>：</p>

<ul>
<li><i class="icon icon-tensorflow"></i> <a href="https://github.com/tensorflow/models/tree/master/research" rel="noreferrer" target="_blank">tensorflow/models</a>,  <a href="https://github.com/tflearn/tflearn/tree/master/examples" rel="noreferrer" target="_blank">tflearn/examples</a>, <a href="https://github.com/tensorlayer/awesome-tensorlayer" rel="noreferrer" target="_blank">tensorlayer/awesome-tensorlayer</a></li>
<li><i class="icon icon-keras"></i> <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications" rel="noreferrer" target="_blank">tf/keras/applications</a>, <a href="https://keras.io/applications" rel="noreferrer" target="_blank">keras/applications</a></li>
<li><i class="icon icon-pytorch"></i> <a href="https://pytorch.org/docs/stable/torchvision/models.html" rel="noreferrer" target="_blank">pytorch/torchvision/models</a></li>
<li><i class="icon icon-caffe2"></i> <a href="https://github.com/caffe2/models" rel="noreferrer" target="_blank">caffe2/models</a></li>
<li><i class="icon icon-mxnet"></i> <a href="https://github.com/apache/incubator-mxnet/tree/master/example/image-classification/symbols" rel="noreferrer" target="_blank">incubator-mxnet/example</a></li>
</ul>

<p style="clear:both;"></p>

<h3 id="network-in-network-nin-lin2013network">Network in Network (NIN) <sup class="footnote-ref" id="fnref:lin2013network"><a href="#fn:lin2013network">17</a></sup></h3>

<p><img src="/images/cn/2018-08-25-cnn/network-in-network.png" alt="NIN" /></p>

<p>NIN 网络的主要改变如下：</p>

<ul>
<li>利用多层的全连接网络替换线性的卷积，即 mlpconv (Conv + MLP) 层。其中卷积层为线性的操作，而 MLP 为非线性的操作，因此具有更高的抽象能力。</li>
<li>去掉了全连接层，使用 Global Average Pooling，也就是将每个 Feature Maps 上所有的值求平均，直接作为输出节点，如下图所示：</li>
</ul>

<p><img src="/images/cn/2018-08-25-cnn/global-average-pooling.png" alt="Global-Average-Pooling" /></p>

<ul>
<li>相比 AlexNet 简化了网络结构，仅包含 4 个 NIN 单元和一个 Global Average Pooling，整个参数空间比 AlexNet 小了一个数量级。</li>
</ul>

<p>在 NIN 中，在跨通道的情况下，mlpconv 层又等价于传统的 Conv 层后接一个 1x1 大小的卷积层，因此 mlpconv 层有时也称为 cccp (cascaded cross channel parametric pooling) 层。1x1 大小的卷积核可以说实现了不同通道信息的交互和整合，同时对于输入通道为 <code>$m$</code> 和输出通道为 <code>$n$</code>，1x1 大小的卷积核在不改变分辨率的同时实现了降维 (<code>$m &gt; n$</code> 情况下) 或升维 (<code>$m &lt; n$</code> 情况下) 操作。</p>

<p><strong>开源实现</strong>：</p>

<ul>
<li><i class="icon icon-tensorflow"></i> <a href="https://github.com/tflearn/tflearn/tree/master/examples" rel="noreferrer" target="_blank">tflearn/examples</a></li>
</ul>

<h3 id="googlenet-inception-v1-szegedy2015going-inception-v3-szegedy2016rethinking-inception-v4-szegedy2016inception">GoogLeNet (Inception V1) <sup class="footnote-ref" id="fnref:szegedy2015going"><a href="#fn:szegedy2015going">5</a></sup>, Inception V3 <sup class="footnote-ref" id="fnref:szegedy2016rethinking"><a href="#fn:szegedy2016rethinking">18</a></sup>, Inception V4 <sup class="footnote-ref" id="fnref:szegedy2016inception"><a href="#fn:szegedy2016inception">19</a></sup></h3>

<p><img src="/images/cn/2018-08-25-cnn/googlenet.png" alt="GoogLeNet" /></p>

<p>除了 VGG 这种从网络深度方向进行优化的策略以外，Google 还提出了在同一层利用不同大小的卷积核同时提取不同特征的思路，对于这样的结构我们称之为 Inception。</p>

<p><img src="/images/cn/2018-08-25-cnn/inception-v1-naive-dim-reduction.png" alt="Inception-V1" /></p>

<p>上图 (左) 为原始的 Inception 结构，在这样一层中分别包括了 1x1 卷积，3x3 卷积，5x5 卷积和 3x3 Max Polling，使得网络在每一层都能学到不同尺度的特征。最后通过 Filter Concat 将其拼接为多个 Feature Maps。</p>

<p>这种方式虽然能够带来性能的提升，但同时也增加了计算量，因此为了进一步改善，其选择利用 1x1 大小的卷积进行降维操作，改进后的 Inception 模块如上图 (右) 所示。我们以 GoogLeNet 中的 inception (3a) 模块为例 (输入大小为 28x28x192)，解释 1x1 卷积的降维效果。</p>

<p>对于原始 Inception 模块，1x1 卷积的通道为 64，3x3 卷积的通道为 128，5x5 卷积的通道为 32，卷积层的参数个数为：</p>

<p><code>$$
\begin{equation}
\begin{split}
\# w_{\text{3a_conv_without_1x1}} =&amp; 1 \times 1 \times 192 \times 64 \\
&amp; + 3 \times 3 \times 192 \times 128 \\
&amp; + 5 \times 5 \times 192 \times 32 \\
=&amp; 387072
\end{split}
\end{equation}
$$</code></p>

<p>对于加上 1x1 卷积后的 Inception 模块 (通道数分别为 96 和 16) 后，卷积层的参数个数为：</p>

<p><code>$$
\begin{equation}
\begin{split}
\# w_{\text{3a_conv_with_1x1}} =&amp; 1 \times 1 \times 192 \times 64 \\
&amp; + 1 \times 1 \times 192 \times 96 + 3 \times 3 \times 96 \times 128 \\
&amp; + 1 \times 1 \times 192 \times 16 + 5 \times 5 \times 16 \times 32 \\
=&amp; 157184
\end{split}
\end{equation}
$$</code></p>

<p>可以看出，在添加 1x1 大小的卷积后，参数的个数减少了 2 倍多。通过 1x1 卷积对特征进行降维的层称之为 Bottleneck Layer 或 Bottleneck Block。</p>

<p>在 GoogLeNet 中，作者还提出了 Auxiliary Classifiers (AC)，用于辅助训练。AC 通过增加浅层的梯度来减轻深度梯度弥散的问题，从而加速整个网络的收敛。</p>

<p>随后 Google 在此对 Inception 进行了改进，同时提出了卷积神经网络的 4 项设计原则，概括如下：</p>

<ol>
<li>避免表示瓶颈，尤其是在网络的浅层部分。一般来说，在到达任务的最终表示之前，表示的大小应该从输入到输出缓慢减小。</li>
<li>高维特征在网络的局部更容易处理。在网络中增加更多的非线性有助于获得更多的解耦特征，同时网络训练也会加快。</li>
<li>空间聚合可以在低维嵌入中进行，同时也不会对表征能力带来太多影响。例如，再进行尺寸较大的卷积操作之前可以先对输入进行降维处理。</li>
<li>在网络的宽度和深度之间进行权衡。通过增加网络的深度和宽度均能够带来性能的提升，在同时增加其深度和宽度时，需要综合考虑算力的分配。</li>
</ol>

<p>Inception V3 的主要改进包括：</p>

<ul>
<li>增加了 Batch Normalized 层。</li>
<li>将一个 5x5 的卷积替换为两个串联的 3x3 的卷积 (基于原则 3)，减少了网络参数，如下图所示：</li>
</ul>

<p><img src="/images/cn/2018-08-25-cnn/inception-v3-v1-3x3.png" alt="Inception-V3-3x3" /></p>

<ul>
<li>利用串联的 1xn 和 nx1 的非对称卷积 (Asymmetric Convolutions) 替代 nxn 的卷积 (基于原则 3)，减少了网络参数，如下图 (左) 所示：</li>
<li>增加带有滤波器组 (filter bank) 的 Inception 模块 (基于原则 2)，用于提升高维空间的表示能力，如下图 (右) 所示：</li>
</ul>

<p><img src="/images/cn/2018-08-25-cnn/inception-v3-1xn-nx1.png" alt="Inception-V3-1xn-nx1" /></p>

<ul>
<li>重新探讨了 Auxiliary Classifiers 的作用，发现其在训练初期并没有有效的提高收敛速度，尽在训练快结束时会略微提高网络的精度。</li>
<li>新的下采样方案。在传统的做法中，如果先进行 Pooling，在利用 Inception 模块进行操作，如下图 (左) 所示，会造成表示瓶颈 (原则 1)；而先利用 Inception 模块进行操作，再进行 Pooling，则会增加参数数量。
<img src="/images/cn/2018-08-25-cnn/inception-v3-reducing-gird-size-old.png" alt="Inception-V3-reducing-grid-size-old" /><br />
因此，借助 Inception 结构的思想，作者提出了一种新的下采样方案。下图 (左) 是利用 Inception 的思想进行下采样的内部逻辑，下图 (右) 为同时利用 Inception 思想和 Pooling 进行下采样的整体框架。</li>
</ul>

<p><img src="/images/cn/2018-08-25-cnn/inception-v3-reducing-gird-size-new.png" alt="Inception-V3-reducing-grid-size-new" /></p>

<ul>
<li>Label Smoothing 机制。假设标签的真实分布为 <code>$q\left(k\right)$</code>，则对于一个真实标签 <code>$y$</code> 而言，有 <code>$q\left(y\right) = 1$</code>，对于 <code>$k \neq y$</code>，有 <code>$q\left(k\right) = 0$</code>。这种情况会导致两个问题：一个是当模型对于每个训练样本的真实标签赋予全部的概率时，模型将会发生过拟合；另一个是其鼓励拉大最大概率标签同其他标签之间的概率差距，从而降低网络的适应性。也就是说这种情况的发生是由于网络对于其预测结果过于自信。因此，对于一个真实标签 <code>$y$</code>，我们将其标签的分布 <code>$q\left(k | x\right) = \delta_{k, y}$</code> 替换为：
<code>$$
q' \left(k | x\right) = \left(1 - \epsilon\right) \delta_{k, y} + \epsilon u \left(k\right)
$$</code>
其中，<code>$u \left(k\right)$</code> 是一个固定的分布，文中采用了均匀分布，即 <code>$u \left(k\right) = 1 / K$</code>；<code>$\epsilon$</code> 为权重项，试验中取为 <code>$0.1$</code>。</li>
</ul>

<p>Inception V4 对于 Inception 网络做了进一步细致的调整，其主要是将 Inception V3 中的前几层网络替换为了 stem 模块，具体的 stem 模块结构就不在此详细介绍了。</p>

<p><strong>开源实现</strong>：</p>

<ul>
<li><i class="icon icon-tensorflow"></i> <a href="https://github.com/tensorflow/models/tree/master/research" rel="noreferrer" target="_blank">tensorflow/models</a>,  <a href="https://github.com/tflearn/tflearn/tree/master/examples" rel="noreferrer" target="_blank">tflearn/examples</a>, <a href="https://github.com/tensorlayer/awesome-tensorlayer" rel="noreferrer" target="_blank">tensorlayer/awesome-tensorlayer</a></li>
<li><i class="icon icon-keras"></i> <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications" rel="noreferrer" target="_blank">tf/keras/applications</a>, <a href="https://keras.io/applications" rel="noreferrer" target="_blank">keras/applications</a></li>
<li><i class="icon icon-pytorch"></i> <a href="https://pytorch.org/docs/stable/torchvision/models.html" rel="noreferrer" target="_blank">pytorch/torchvision/models</a></li>
<li><i class="icon icon-caffe2"></i> <a href="https://github.com/caffe2/models" rel="noreferrer" target="_blank">caffe2/models</a></li>
<li><i class="icon icon-mxnet"></i> <a href="https://github.com/apache/incubator-mxnet/tree/master/example/image-classification/symbols" rel="noreferrer" target="_blank">incubator-mxnet/example</a></li>
</ul>

<h3 id="deep-residual-net-he2016deep-identity-mapping-residual-net-he2016ientity-densenet-huang2016densely">Deep Residual Net <sup class="footnote-ref" id="fnref:he2016deep"><a href="#fn:he2016deep">6</a></sup>, Identity Mapping Residual Net <sup class="footnote-ref" id="fnref:he2016ientity"><a href="#fn:he2016ientity">20</a></sup>, DenseNet <sup class="footnote-ref" id="fnref:huang2016densely"><a href="#fn:huang2016densely">21</a></sup></h3>

<p><img src="/images/cn/2018-08-25-cnn/residual-network.png" width="300" style="float:left; margin-right:3em;"/></p>

<p>随着网络深度的不断增加啊，其效果并未如想象一般提升，甚至发生了退化，He 等人 <sup class="footnote-ref" id="fnref:he2016deep"><a href="#fn:he2016deep">6</a></sup> 发现在 CIFAR-10 数据集上，一个 56 层的神经网络的性能要比一个 20 层的神经网络要差。网络层级的不断增加，不仅导致参数的增加，同时也可能导致梯度弥散问题 (vanishing gradients)。</p>

<p>这对这些问题，He 等人提出了一种 Deep Residual Net，在这个网络结构中，残差 (residual) 的思想可以理解为：假设原始潜在的映射关系为 <code>$\mathcal{H} \left(\mathbf{x}\right)$</code>，对于新的网络层我们不再拟合原始的映射关系，而是拟合 <code>$\mathcal{F} \left(\mathbf{x}\right) = \mathcal{H} \left(\mathbf{x}\right) - \mathbf{x}$</code>，也就是说原始潜在的映射关系变为 <code>$\mathcal{F} \left(\mathbf{x}\right) + \mathbf{x}$</code>。新的映射关系可以理解为在网络前向传播中添加了一条捷径 (shortcut connections)，如下图所示：</p>

<p><img src="/images/cn/2018-08-25-cnn/residual-block.png" alt="Residual-Block" /></p>

<p style="clear:both;"></p>

<p>增加 Short Connections 并没有增加参数个数，也没有增加计算量，与此同时模型依旧可以利用 SGD 等算法进行优化。</p>

<p><img src="/images/cn/2018-08-25-cnn/residual-results.png" alt="Residual-Results" /></p>

<p>从 Deep Residual Net 的实验结果 (如上图) 可以看出，在没有加入残差模块的网络中 (上图 - 左) 出现了上文中描述的问题：更多层级的网络的效果反而较差；在加入了残差模块的网络中 (上图 - 右)，其整体性能均比未加入残差模块的网络要好，同时具有更多层级的网络的效果也更好。</p>

<p>随后 He 等人 <sup class="footnote-ref" id="fnref:he2016ientity"><a href="#fn:he2016ientity">20</a></sup> 又提出了 Identity Mapping Residual Net，在原始的 ResNet 中，一个残差单元可以表示为：</p>

<p><code>$$
\begin{equation}
\begin{split}
\mathbb{y}_{\ell} = &amp; h \left(\mathbb{x}_{\ell}\right) + \mathcal{F} \left(\mathbb{x}_{\ell}, \mathcal{W}_l\right) \\
\mathbb{x}_{\ell+1} = &amp; f \left(\mathbb{y}_{\ell}\right)
\end{split}
\end{equation}
$$</code></p>

<p>其中 <code>$\mathbb{x}_{\ell}$</code> 和 <code>$\mathbb{x}_{\ell+1}$</code> 为第 <code>$\ell$</code> 个单元的输入和输出，<code>$\mathcal{F}$</code> 为残差函数，<code>$h \left(\mathbb{x}_{\ell}\right) = \mathbb{x}_{\ell}$</code> 为一个恒等映射，<code>$f$</code> 为 ReLU 函数。在 Identity Mapping Residual Net，作者将 <code>$f$</code> 由原来的 ReLU 函数也替换成一个恒定映射，即 <code>$\mathbb{x}_{\ell+1} \equiv \mathbb{y}_{\ell}$</code>，则上式可以改写为：</p>

<p><code>$$
\mathbb{x}_{\ell+1} = \mathbb{x}_{\ell} + \mathcal{F} \left(\mathbb{x}_{\ell}, \mathcal{W}_{\ell}\right)
$$</code></p>

<p>则对于任意深度的单元 <code>$L$</code>，有如下表示：</p>

<p><code>$$
\mathbb{x}_L = \mathbb{x}_{\ell} + \sum_{i=\ell}^{L-1}{\mathcal{F} \left(\mathbb{x}_i, \mathcal{W}_i\right)}
$$</code></p>

<p>上式形式的表示使得其在反向传播中具有一个很好的性质，假设损失函数为 <code>$\mathcal{E}$</code>，根据链式法则，对于单元 <code>$\ell$</code>，梯度为：</p>

<p><code>$$
\dfrac{\partial \mathcal{E}}{\partial \mathbb{x}_{\ell}} = \dfrac{\partial \mathcal{E}}{\partial \mathbb{x}_L} \dfrac{\partial \mathbb{x}_L}{\partial \mathbb{x}_{\ell}} = \dfrac{\partial \mathcal{E}}{\partial \mathbb{x}_{\ell}} \left(1 + \dfrac{\partial}{\partial \mathbb{x}_{\ell}} \sum_{i=\ell}^{L-1}{\mathcal{F} \left(\mathbb{x}_i, \mathcal{W}_i\right)}\right)
$$</code></p>

<p>对于上式形式的梯度，我们可以将其拆解为两部分：<code>$\frac{\partial \mathcal{E}}{\partial \mathbb{x}_{\ell}}$</code> 为不通过任何权重层的直接梯度传递，<code>$\frac{\partial \mathcal{E}}{\partial \mathbb{x}_{\ell}} \left(\frac{\partial}{\partial \mathbb{x}_{\ell}} \sum_{i=\ell}^{L-1}{\mathcal{F} \left(\mathbb{x}_i, \mathcal{W}_i\right)}\right)$</code> 为通过权重层的梯度传递。前一项保证了梯度能够直接传回任意浅层 <code>$\ell$</code>，同时对于任意一个 mini-batch 的所有样本，<code>$\frac{\partial}{\partial \mathbb{x}_{\ell}} \sum_{i=\ell}^{L-1}\mathcal{F}$</code> 不可能永远为 <code>$-1$</code>，所以保证了即使权重很小的情况下也不会出现梯度弥散。下图展示了原始的 ResNet 和 Identity Mapping Residual Net 之间残差单元的区别和网络的性能差异：</p>

<p><img src="/images/cn/2018-08-25-cnn/identity-mapping-residual-network-unit.png" alt="Identity-Mapping-Residual-Net-Unit" /></p>

<p>Huang 等人 <sup class="footnote-ref" id="fnref:huang2016densely"><a href="#fn:huang2016densely">21</a></sup> 在 ResNet 的基础上又提出了 DenseNet 网络，其网络结构如下所示：</p>

<p><img src="/images/cn/2018-08-25-cnn/densenet.png" alt="DenseNet" /></p>

<p>DenseNet 的主要改进如下：</p>

<ul>
<li>Dense Connectivity：将网络中每一层都与其后续层进行直接连接。</li>
<li>Growth Rate：<code>$H_{\ell}$</code> 将产生 <code>$k$</code> 个 Feature Maps，因此第 <code>$\ell$</code> 层将包含 <code>$k_0 + k \times \left(\ell - 1\right)$</code> 个 Feature Maps，其中 <code>$k_0$</code> 为输入层的通道数。DenseNet 与现有框架的不同之处就是将网络限定的比较窄，例如：<code>$k = 12$</code>，并将该超参数称之为网络的增长率 (Growth Rate)。</li>
<li>Bottleneck Layers：在 3x3 的卷积之前增加 1x1 的卷积进行降维操作。</li>
<li>Compression：在两个 Dense Block 之间增加过渡层 (Transition Layer)，进一步减少 Feature Maps 个数。</li>
</ul>

<p><strong>开源实现</strong>：</p>

<ul>
<li><i class="icon icon-tensorflow"></i> <a href="https://github.com/tensorflow/models/tree/master/research" rel="noreferrer" target="_blank">tensorflow/models</a>,  <a href="https://github.com/tflearn/tflearn/tree/master/examples" rel="noreferrer" target="_blank">tflearn/examples</a>, <a href="https://github.com/tensorlayer/awesome-tensorlayer" rel="noreferrer" target="_blank">tensorlayer/awesome-tensorlayer</a></li>
<li><i class="icon icon-keras"></i> <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications" rel="noreferrer" target="_blank">tf/keras/applications</a>, <a href="https://keras.io/applications" rel="noreferrer" target="_blank">keras/applications</a></li>
<li><i class="icon icon-pytorch"></i> <a href="https://pytorch.org/docs/stable/torchvision/models.html" rel="noreferrer" target="_blank">pytorch/torchvision/models</a></li>
<li><i class="icon icon-caffe2"></i> <a href="https://github.com/caffe2/models" rel="noreferrer" target="_blank">caffe2/models</a></li>
<li><i class="icon icon-mxnet"></i> <a href="https://github.com/apache/incubator-mxnet/tree/master/example/image-classification/symbols" rel="noreferrer" target="_blank">incubator-mxnet/example</a></li>
</ul>

<h3 id="综合比较">综合比较</h3>

<p>Canziani 等人 <sup class="footnote-ref" id="fnref:canziani2016an"><a href="#fn:canziani2016an">22</a></sup> 综合了模型的准确率，参数大小，内存占用，推理时间等多个角度对现有的 CNN 模型进行了对比分析。</p>

<p><img src="/images/cn/2018-08-25-cnn/cnn-accuracy-and-parameters.png" alt="CNN-Accuracy-and-Parameters" /></p>

<p>上图 (左) 展示了在 ImageNet 挑战赛中不同 CNN 网络模型的 Top-1 的准确率。可以看出近期的 ResNet 和 Inception 架构以至少 7% 的显著优势超过了其他架构。上图 (右) 以另一种形式展现了除了准确率以外的更多信息，包括计算成本和网络的参数个数，其中横轴为计算成本，纵轴为 Top-1 的准确率，气泡的大小为网络的参数个数。可以看出 ResNet 和 Inception 架构相比 AlexNet 和 VGG 不仅有更高的准确率，其在计算成本和网络的参数个数 (模型大小) 方面也具有一定优势。</p>

<p>文章部分内容参考了 <strong>刘昕</strong> 的 <a href="http://valser.org/2016/dl/刘昕.pdf" rel="noreferrer" target="_blank"><strong>CNN近期进展与实用技巧</strong></a>。CNN 除了在图像分类问题上取得很大的进展外，在例如：物体检测：R-CNN <sup class="footnote-ref" id="fnref:grishick2014rich"><a href="#fn:grishick2014rich">23</a></sup>, SPP-Net <sup class="footnote-ref" id="fnref:he2015spatial"><a href="#fn:he2015spatial">24</a></sup>, Fast R-CNN <sup class="footnote-ref" id="fnref:grishick2015fast"><a href="#fn:grishick2015fast">25</a></sup>, Faster R-CNN <sup class="footnote-ref" id="fnref:ren2017faster"><a href="#fn:ren2017faster">26</a></sup>，语义分割：FCN <sup class="footnote-ref" id="fnref:shelhamer2017fully"><a href="#fn:shelhamer2017fully">27</a></sup> 等多个领域也取得了不俗的成绩。针对不同的应用场景，网络模型和处理方法均有一定的差异，本文就不再对其他场景一一展开说明，不同场景将在后续进行单独整理。</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:lecun1998gradient">LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. <em>Proceedings of the IEEE, 86</em>(11), 2278-2324.
 <a class="footnote-return" href="#fnref:lecun1998gradient">↩</a></li>
<li id="fn:krizhevsky2012imagenet">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In <em>Advances in neural information processing systems</em> (pp. 1097-1105).
 <a class="footnote-return" href="#fnref:krizhevsky2012imagenet">↩</a></li>
<li id="fn:imagenet"><a href="http://www.image-net.org/" rel="noreferrer" target="_blank">http://www.image-net.org/</a>
 <a class="footnote-return" href="#fnref:imagenet">↩</a></li>
<li id="fn:simonyan2014very">Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. <em>arXiv preprint arXiv:1409.1556.</em>
 <a class="footnote-return" href="#fnref:simonyan2014very">↩</a></li>
<li id="fn:szegedy2015going">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., &hellip; &amp; Rabinovich, A. (2015). Going deeper with convolutions. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 1-9).
 <a class="footnote-return" href="#fnref:szegedy2015going">↩</a></li>
<li id="fn:he2016deep">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 770-778).
 <a class="footnote-return" href="#fnref:he2016deep">↩</a></li>
<li id="fn:girshick2014rich">Girshick, R., Donahue, J., Darrell, T., &amp; Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 580-587).
 <a class="footnote-return" href="#fnref:girshick2014rich">↩</a></li>
<li id="fn:girshick2015fast">Girshick, R. (2015). Fast r-cnn. In <em>Proceedings of the IEEE international conference on computer vision</em> (pp. 1440-1448).
 <a class="footnote-return" href="#fnref:girshick2015fast">↩</a></li>
<li id="fn:ren2015faster">Ren, S., He, K., Girshick, R., &amp; Sun, J. (2015). Faster r-cnn: Towards real-time object detection with region proposal networks. In <em>Advances in neural information processing systems</em> (pp. 91-99).
 <a class="footnote-return" href="#fnref:ren2015faster">↩</a></li>
<li id="fn:long2015fully">Long, J., Shelhamer, E., &amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 3431-3440).
 <a class="footnote-return" href="#fnref:long2015fully">↩</a></li>
<li id="fn:vinyals2015show">Vinyals, O., Toshev, A., Bengio, S., &amp; Erhan, D. (2015). Show and tell: A neural image caption generator. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 3156-3164).
 <a class="footnote-return" href="#fnref:vinyals2015show">↩</a></li>
<li id="fn:ji20133d">Ji, S., Xu, W., Yang, M., &amp; Yu, K. (2013). 3D convolutional neural networks for human action recognition. <em>IEEE transactions on pattern analysis and machine intelligence, 35</em>(1), 221-231.
 <a class="footnote-return" href="#fnref:ji20133d">↩</a></li>
<li id="fn:kim2014convolutional">Kim, Y. (2014). Convolutional neural networks for sentence classification. <em>arXiv preprint arXiv:1408.5882.</em>
 <a class="footnote-return" href="#fnref:kim2014convolutional">↩</a></li>
<li id="fn:mnist"><a href="http://yann.lecun.com/exdb/mnist" rel="noreferrer" target="_blank">http://yann.lecun.com/exdb/mnist</a>
 <a class="footnote-return" href="#fnref:mnist">↩</a></li>
<li id="fn:conv-images"><a href="https://mlnotebook.github.io/post/CNN1/" rel="noreferrer" target="_blank">https://mlnotebook.github.io/post/CNN1/</a>
 <a class="footnote-return" href="#fnref:conv-images">↩</a></li>
<li id="fn:deep-learning">Goodfellow, I., Bengio, Y., Courville, A., &amp; Bengio, Y. (2016). <em>Deep learning</em> (Vol. 1). Cambridge: MIT press.
 <a class="footnote-return" href="#fnref:deep-learning">↩</a></li>
<li id="fn:lin2013network">Lin, M., Chen, Q., &amp; Yan, S. (2013). Network In Network. <em>arXiv preprint arXiv:1312.4400.</em>
 <a class="footnote-return" href="#fnref:lin2013network">↩</a></li>
<li id="fn:szegedy2016rethinking">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &amp; Wojna, Z. (2016). Rethinking the Inception Architecture for Computer Vision. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 2818–2826).
 <a class="footnote-return" href="#fnref:szegedy2016rethinking">↩</a></li>
<li id="fn:szegedy2016inception">Szegedy, C., Ioffe, S., Vanhoucke, V., &amp; Alemi, A. (2016). Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. <em>arXiv preprint arXiv:1602.07261.</em>
 <a class="footnote-return" href="#fnref:szegedy2016inception">↩</a></li>
<li id="fn:he2016ientity">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Identity Mappings in Deep Residual Networks. <em>arXiv preprint arXiv:1603.05027.</em>
 <a class="footnote-return" href="#fnref:he2016ientity">↩</a></li>
<li id="fn:huang2016densely">Huang, G., Liu, Z., van der Maaten, L., &amp; Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. <em>arXiv preprint arXiv:1608.06993</em>
 <a class="footnote-return" href="#fnref:huang2016densely">↩</a></li>
<li id="fn:canziani2016an">Canziani, A., Paszke, A., &amp; Culurciello, E. (2016). An Analysis of Deep Neural Network Models for Practical Applications. <em>arXiv preprint arXiv:1605.07678</em>
 <a class="footnote-return" href="#fnref:canziani2016an">↩</a></li>
<li id="fn:grishick2014rich">Girshick, R., Donahue, J., Darrell, T., &amp; Malik, J. (2014). Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. In <em>Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 580–587).
 <a class="footnote-return" href="#fnref:grishick2014rich">↩</a></li>
<li id="fn:he2015spatial">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(9)</em>, 1904–1916.
 <a class="footnote-return" href="#fnref:he2015spatial">↩</a></li>
<li id="fn:grishick2015fast">Girshick, R. (2015). Fast R-CNN. <em>arXiv preprint arXiv:1504.08083.</em>
 <a class="footnote-return" href="#fnref:grishick2015fast">↩</a></li>
<li id="fn:ren2017faster">Ren, S., He, K., Girshick, R., &amp; Sun, J. (2017). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6),</em> 1137–1149.
 <a class="footnote-return" href="#fnref:ren2017faster">↩</a></li>
<li id="fn:shelhamer2017fully">Shelhamer, E., Long, J., &amp; Darrell, T. (2017). Fully Convolutional Networks for Semantic Segmentation. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4),</em> 640–651.
 <a class="footnote-return" href="#fnref:shelhamer2017fully">↩</a></li>
</ol>
</div>



<link rel="stylesheet" href="/css/donate.css" />


<div class="donate">
  <div class="donate-header"></div>
  <div class="donate-slug" id="donate-slug">cnn</div>
  <button class="donate-button">赞 赏</button>
  <div class="donate-footer">「真诚赞赏，手留余香」</div>
</div>
<div class="donate-modal-wrapper">
  <div class="donate-modal">
    <div class="donate-box">
      <div class="donate-box-content">
        <div class="donate-box-content-inner">
          <div class="donate-box-header">「真诚赞赏，手留余香」</div>
          <div class="donate-box-body">
            <div class="donate-box-money">
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-2" data-v="2" data-unchecked="￥ 2" data-checked="2 元">￥ 2</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-5" data-v="5" data-unchecked="￥ 5" data-checked="5 元">￥ 5</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-10" data-v="10" data-unchecked="￥ 10" data-checked="10 元">￥ 10</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-50" data-v="50" data-unchecked="￥ 50" data-checked="50 元">￥ 50</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-100" data-v="100" data-unchecked="￥ 100" data-checked="100 元">￥ 100</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-custom" data-v="custom" data-unchecked="任意金额" data-checked="任意金额">任意金额</button>
            </div>
            <div class="donate-box-pay">
              <img class="donate-box-pay-qrcode" id="donate-box-pay-qrcode" src=""/>
            </div>
          </div>
          <div class="donate-box-footer">
            <div class="donate-box-pay-method donate-box-pay-method-checked" data-v="wechat-pay">
              <img class="donate-box-pay-method-image" id="donate-box-pay-method-image-wechat-pay" src=""/>
            </div>
            <div class="donate-box-pay-method" data-v="alipay">
              <img class="donate-box-pay-method-image"  id="donate-box-pay-method-image-alipay" src=""/>
            </div>
          </div>
        </div>
      </div>
    </div>
    <button type="button" class="donate-box-close-button">
      <svg class="donate-box-close-button-icon" fill="#fff" viewBox="0 0 24 24" width="24" height="24"><path d="M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z" fill-rule="evenodd"></path></svg>
    </button>
  </div>
</div>

<script type="text/javascript" src="/js/donate.js"></script>
</script>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/cn/2018/07/buy-books-hoard-books-and-read-books/">买书，囤书，看书 (Buy Books, Hoard Books and Read Books)</a></span>
  <span class="nav-next"><a href="/cn/2018/09/war-of-medias/">媒介之战 (War of Medias)</a> &rarr;</span>
</nav>
<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/cn\/2018\/07\/buy-books-hoard-books-and-read-books\/';
    
  } else if (e.which == 39) {  
    
    url = '\/cn\/2018\/09\/war-of-medias\/';
    
  }
  if (url) window.location = url;
});
</script>




<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-2608165017777396"
     data-ad-slot="1261604535"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>





<section class="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
    this.page.url = "https:\/\/zeqiang.fun" + location.pathname;
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var disqus_js = '//Zeqiang.disqus.com/embed.js';
    var d = document, s = d.createElement('script');
    s.src = disqus_js; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    var t = d.getElementById('disqus_thread');
    var b = false, l = function(scroll) {
      if (b) return;
      (d.head || d.body).appendChild(s); b = true;
      if (scroll) t.scrollIntoView();
    }
    s.onerror = function(e) {
      if (sessionStorage.getItem('failure-note')) return;
      t.innerText = 'Sorry, but you cannot make comments because Disqus failed to load for some reason. It is known to be blocked in China. If you are sure it is not blocked in your region, please refresh the page. 中国大陆地区读者需要翻墙才能发表评论。';
      sessionStorage.setItem('failure-note', true);
    };
    
    if (location.hash.match(/^#comment-[0-9]+$/)) return l(true);
    var c = function() {
      if (b) return;
      var rect = t.getBoundingClientRect();
      if (rect.top < window.innerHeight && rect.bottom >= 0) l();
    };
    window.addEventListener('load', c);
    d.addEventListener('scroll', c);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>




<script async src="/js/fix-toc.js"></script>
<script async src="/js/center-img.js"></script>
<script async src="/js/right-quote.js"></script>
<script async src="/js/fix-footnote.js"></script>
<script async src="/js/external-link.js"></script>
<script async src="/js/alt-title.js"></script>
<script src="/js/no-highlight.js"></script>
<script src="/js/math-code.js"></script>


<script>
window.MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js" crossorigin></script>







<script async src="/js/load-typekit.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lazysizes/5.3.2/lazysizes.min.js"></script>

<script src="//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js"></script>
<script>
addBackToTop({
  diameter: 48
})
</script>



  <hr>
  <div class="copyright no-border-bottom">
    <div class="copyright-author-year">
      <span>&copy; 2017-2021 Leo Van</span>
    </div>
    <div class="copyright-links">
      <a href="https://github.com/leovan" rel="noreferrer" target="_blank">Github</a>
      <span> · </span>
      <a href="https://orcid.org/0000-0002-9556-7821" rel="noreferrer" target="_blank">ORCID</a>
      <span> · </span>
      <span>I am Mr. Black.</span>
    </div>
  </div>
  </footer>
  </article>
  </body>
</html>


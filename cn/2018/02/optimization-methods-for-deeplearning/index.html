<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

        <title>深度学习优化算法 (Optimization Methods for Deeplearning) - Zeqiang Fang | 方泽强</title>

    <meta name="referrer" content="no-referrer">
    
    <meta property="og:title" content="深度学习优化算法 (Optimization Methods for Deeplearning) - Zeqiang Fang | 方泽强">
    <meta name="description" property="og:description" content="在构建神经网络模型的时候，除了网络结构设计以外，选取合适的优化算法也对网络起着至关重要的作用，本文将对神经网络中常用的优化算法进行简单的介绍">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Zeqiang Fang | 方泽强">
    <meta property="og:url" content="https://leovan.me/cn/2018/02/optimization-methods-for-deeplearning/">

    
    
    
    
    <meta name="author" property="article:author" content="范叶亮">
    
    
    
    <meta name="date" property="article:published_time" content="2018-02-24" scheme="YYYY-MM-DD">
    
    
    <meta name="date" property="article:modified_time" content="2018-02-24" scheme="YYYY-MM-DD">
    

    
    <meta name="keywords" property="article:tag" content ="梯度下降,SGD,Momentum,NAG,AdaGrad,Adadelta,RMSProp,Adam,Adamax,Nadam,AMSGrad">
    
    
    <meta name="theme-color" content="#0d0d0d">
    
    <link rel="icon" type="image/png" sizes="16x16" href="/images/web/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/web/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/images/web/favicon-48x48.png">
    <link rel="icon" type="image/png" sizes="62x62" href="/images/web/favicon-62x62.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/images/web/favicon-192x192.png">
    <link rel="apple-touch-icon" size="192x192" href="/images/web/icon-192x192.png">
    <link rel="manifest" href="/manifest.json">
        
    

    

    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://leovan.me/cn"
        },
        "name": "深度学习优化算法 (Optimization Methods for Deeplearning)",
        "headline": "深度学习优化算法 (Optimization Methods for Deeplearning)",
        "description" : "在构建神经网络模型的时候，除了网络结构设计以外，选取合适的优化算法也对网络起着至关重要的作用，本文将对神经网络中常用的优化算法进行简单的介绍",
        "genre": [
            "机器学习", "深度学习"
        ],
        "datePublished": "2018-02-24",
        "dateModified": "2018-02-24",
        "wordCount": "4054",
        "keywords": [
            "梯度下降", "SGD", "Momentum", "NAG", "AdaGrad", "Adadelta", "RMSProp", "Adam", "Adamax", "Nadam", "AMSGrad"
        ],
        "image": [
            "https://leovan.me/images/cn/2018-02-24-optimization-methods-for-deeplearning/sgd-and-momentum.png", "https://leovan.me/images/cn/2018-02-24-optimization-methods-for-deeplearning/momentum-and-nag.png", "https://leovan.me/images/cn/2018-02-24-optimization-methods-for-deeplearning/contours-evaluation-optimizers.gif", "https://leovan.me/images/cn/2018-02-24-optimization-methods-for-deeplearning/saddle-point-evaluation-optimizers.gif"
        ],
        "author": {
            "@type": "Person",
            "name": "范叶亮"
        },
        "publisher": {
            "@type": "Organization",
            "name": "范叶亮",
            "logo": {
                "@type": "ImageObject",
                "url": "https://leovan.me/images/web/publisher-logo.png"
            }
        },
        "url": "https://leovan.me/cn/2018/02/optimization-methods-for-deeplearning/"
    }
    </script>
    

    <script src='//cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js'></script>
<script src='//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js'></script>

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/6.5.95/css/materialdesignicons.min.css">





<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css">

<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism-.min.css">





<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<link rel="stylesheet" type="text/css" href="/css/fonts.css" />
<link rel="stylesheet" type="text/css" href="/css/style.css">
<link rel="stylesheet" type="text/css" href="/css/light.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" id="dark-mode-style" disabled="disabled">
<link rel="stylesheet" type="text/css" href="/css/icons.css">
<link rel="stylesheet" type="text/css" href="/css/print.css">

  </head>

  
  <body class="cn">
    <header class="masthead">
      

<div class="logo"></div>
<p class="slogan">优雅永不过时</p>
      <nav class="menu">
  <ul>
  
  
  
    
  
  
    <li><a href="/">首页</a></li>
  
    <li><a href="/cn/">博客</a></li>
  
    <li><a href="/categories/">分类</a></li>
  
    <li class="menu-separator"><span>&nbsp;</span></li>
  
    <li><a href="/cn/about/">关于</a></li>
  
    <li><a href="/cn/resume/">简历</a></li>
  
  


<li class="menu-separator"><span>&nbsp;</span></li>

<li><a href="/cn/index.xml" target="_blank" type="application/rss+xml" title="RSS feed">订阅</a></li>

<li><a href="https://github.com/fang-zeqiang/fang-zeqiang.github.io/blob/master/LICENSE" target="_blank" title="Attribution-NonCommercial-ShareAlike 4.0 International">版权</a></li>


  <li class="light-dark-mode no-border-bottom"><a id="light-dark-mode-action"><span id="light-dark-mode-icon" class="mdi mdi-weather-night"></span></a></li>
  </ul>
</nav>

      <script src="/js/toggle-theme.js"></script>
    </header>

    <article class="main">
      <header class="title">
      
<h1>深度学习优化算法 (Optimization Methods for Deeplearning)</h1>







<h3>范叶亮 / 
2018-02-24</h3>



<h3 class="post-meta">


<strong>分类: </strong>
<a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>, <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>




/




<strong>标签: </strong>
<span>梯度下降</span>, <span>SGD</span>, <span>Momentum</span>, <span>NAG</span>, <span>AdaGrad</span>, <span>Adadelta</span>, <span>RMSProp</span>, <span>Adam</span>, <span>Adamax</span>, <span>Nadam</span>, <span>AMSGrad</span>




/


<strong>字数: </strong>
4054
</h3>



<hr>


      </header>






<p>在构建神经网络模型的时候，除了网络结构设计以外，选取合适的优化算法也对网络起着至关重要的作用，本文将对神经网络中常用的优化算法进行简单的介绍和对比，本文部分参考了 Ruder 的关于梯度下降优化算法一文 <sup class="footnote-ref" id="fnref:ruder2016overview"><a href="#fn:ruder2016overview">1</a></sup>。首先，我们对下文中使用的符号进行同意说明：网络中的参数同一表示为 <code>$\theta$</code>，网络的假设函数为 <code>$h_{\boldsymbol{\theta}}\left(\boldsymbol{x}\right)$</code>，网络的损失函数为 <code>$J\left(\boldsymbol{\theta}\right)$</code>，学习率为 <code>$\alpha$</code>，假设训练数据中共包含 <code>$m$</code> 个样本，网络参数个数为 <code>$n$</code>。</p>

<h1 id="梯度下降">梯度下降</h1>

<p>在梯度下降算法中，常用的主要包含 3 种不同的形式，分别是批量梯度下降 (Batch Gradient Descent, BGD)，随机梯度下降 (Stochastic Gradient Descent, SGD) 和小批量梯度下降 (Mini-Batch Gradient Descent, MBGD)。一般情况下，我们在谈论梯度下降时，更多的是指小批量梯度下降。</p>

<h2 id="bgd">BGD</h2>

<p>BGD 为梯度下降算法中最基础的一个算法，其损失函数定义如下：</p>

<p><code>$$
J \left(\boldsymbol{\theta}\right) = \dfrac{1}{2m} \sum_{i=1}^{m}{\left(h_{\boldsymbol{\theta}}\left(x^{\left(i\right)}\right) - y^{\left(i\right)}\right)}
$$</code></p>

<p>针对任意参数 <code>$\theta_j$</code> 我们可以求得其梯度为：</p>

<p><code>$$
\nabla_{\theta_j} = \dfrac{\partial J\left(\boldsymbol{\theta}\right)}{\partial \theta_j} = - \dfrac{1}{m} \sum_{i=1}^{m}{\left(y^{\left(i\right)} - h_{\boldsymbol{\theta}} \left(x^{\left(i\right)}\right)\right) x_j^{\left(i\right)}}
$$</code></p>

<p>之后，对于任意参数 <code>$\theta_j$</code> 我们按照其<strong>负梯度</strong>方向进行更新：</p>

<p><code>$$
\theta_j = \theta_j + \alpha \left[\dfrac{1}{m} \sum_{i=1}^{m}{\left(y^{\left(i\right)} - h_{\boldsymbol{\theta}} \left(x^{\left(i\right)}\right)\right) x_j^{\left(i\right)}}\right]
$$</code></p>

<p>整个算法流程可以表示如下：</p>



<link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">


<div><pre class="pseudocode">
\begin{algorithm}
\caption{BGD 算法}
\begin{algorithmic}
\FOR{$epoch = 1, 2, ...$}
    \FOR{$j = 1, 2, ..., n$}
        \STATE $J \left(\boldsymbol{\theta}\right) = \dfrac{1}{2m} \sum_{i=1}^{m}{\left(h_{\boldsymbol{\theta}}\left(x^{\left(i\right)}\right) - y^{\left(i\right)}\right)}$
        \STATE $\theta_j = \theta_j - \alpha \dfrac{\partial J\left(\boldsymbol{\theta}\right)}{\partial \theta_j}$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
</pre></div>


<p>从上述算法流程中我们可以看到，BGD 算法每次计算梯度都使用了整个训练集，也就是说对于给定的一个初始点，其每一步的更新都是沿着全局梯度最大的负方向。但这同样是其问题，当 <code>$m$</code> 太大时，整个算法的计算开销就很高了。</p>

<h2 id="sgd">SGD</h2>

<p>SGD 相比于 BGD，其最主要的区别就在于计算梯度时不再利用整个数据集，而是针对单个样本计算梯度并更新权重，因此，其损失函数定义如下：</p>

<p><code>$$
J \left(\boldsymbol{\theta}\right) = \dfrac{1}{2} \left(h_{\boldsymbol{\theta}}\left(x^{\left(i\right)}\right) - y^{\left(i\right)}\right)
$$</code></p>

<p>整个算法流程可以表示如下：</p>



<div><pre class="pseudocode">
\begin{algorithm}
\caption{SGD 算法}
\begin{algorithmic}
\FOR{$epoch = 1, 2, ...$}
    \STATE Randomly shuffle dataset
    \FOR{$i = 1, 2, ..., m$}
        \FOR{$j = 1, 2, ..., n$}
            \STATE $J \left(\boldsymbol{\theta}\right) = \dfrac{1}{2} \left(h_{\boldsymbol{\theta}}\left(x^{\left(i\right)}\right) - y^{\left(i\right)}\right)$
            \STATE $\theta_j = \theta_j - \alpha \dfrac{\partial J\left(\boldsymbol{\theta}\right)}{\partial \theta_j}$
        \ENDFOR
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
</pre></div>


<p>SGD 相比于 BGD 具有训练速度快的优势，但同时由于权重改变的方向并不是全局梯度最大的负方向，甚至相反，因此不能够保证每次损失函数都会减小。</p>

<h2 id="mbgd">MBGD</h2>

<p>针对 BGD 和 SGD 的问题，MBGD 则是一个折中的方案，在每次更新参数时，MBGD 会选取 <code>$b$</code> 个样本计算的梯度，设第 <code>$k$</code> 批中数据的下标的集合为 <code>$B_k$</code>，则其损失函数定义如下：</p>

<p><code>$$
\nabla_{\theta_j} = \dfrac{\partial J\left(\boldsymbol{\theta}\right)}{\partial \theta_j} = - \dfrac{1}{|B_k|} \sum_{i \in B_k}{\left(y^{\left(i\right)} - h_{\boldsymbol{\theta}} \left(x^{\left(i\right)}\right)\right) x_j^{\left(i\right)}}
$$</code></p>

<p>整个算法流程可以表示如下：</p>



<div><pre class="pseudocode">
\begin{algorithm}
\caption{MBGD 算法}
\begin{algorithmic}
\FOR{$epoch = 1, 2, ...$}
    \FOR{$k = 1, 2, ..., m / b$}
        \FOR{$j = 1, 2, ..., n$}
            \STATE $J \left(\boldsymbol{\theta}\right) = \dfrac{1}{|B_k|} \sum_{i \in B_k}{\left(y^{\left(i\right)} - h_{\boldsymbol{\theta}} \left(x^{\left(i\right)}\right)\right) x_j^{\left(i\right)}}$
            \STATE $\theta_j = \theta_j - \alpha \dfrac{\partial J\left(\boldsymbol{\theta}\right)}{\partial \theta_j}$
        \ENDFOR
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
</pre></div>


<h1 id="momentum">Momentum</h1>

<p>当梯度沿着一个方向要明显比其他方向陡峭，我们可以形象的称之为峡谷形梯度，这种情况多位于局部最优点附近。在这种情况下，SGD 通常会摇摆着通过峡谷的斜坡，这就导致了其到达局部最优值的速度过慢。因此，针对这种情况，Momentum <sup class="footnote-ref" id="fnref:qian1999momentum"><a href="#fn:qian1999momentum">2</a></sup> 方法提供了一种解决方案。针对原始的 SGD 算法，参数每 <code>$t$</code> 步的变化量可以表示为</p>

<p><code>$$
\boldsymbol{v}_t = - \alpha \nabla_{\boldsymbol{\theta}} J \left(\boldsymbol{\theta}_t\right)
$$</code></p>

<p>Momentum 算法则在其变化量中添加了一个动量分量，即</p>

<p><code>$$
\begin{equation}
\begin{split}
\boldsymbol{v}_t &amp;= - \alpha \nabla_{\boldsymbol{\theta}} J \left(\boldsymbol{\theta}_t\right) + \gamma \boldsymbol{v}_{t-1} \\
\boldsymbol{\theta}_t &amp;= \boldsymbol{\theta}_{t-1} + \boldsymbol{v}_t
\end{split}
\end{equation}
$$</code></p>

<p>对于添加的动量项，当第 <code>$t$</code> 步和第 <code>$t-1$</code> 步的梯度方向<strong>相同</strong>时，<code>$\boldsymbol{\theta}$</code> 则以更快的速度更新；当第 <code>$t$</code> 步和第 <code>$t-1$</code> 步的梯度方向<strong>相反</strong>时，<code>$\boldsymbol{\theta}$</code> 则以较慢的速度更新。利用 SGD 和 Momentum 两种方法，在峡谷行的二维梯度上更新参数的示意图如下所示</p>

<p><img src="/images/cn/2018-02-24-optimization-methods-for-deeplearning/sgd-and-momentum.png" alt="" /></p>

<h1 id="nag">NAG</h1>

<p>NAG (Nesterov Accelerated Gradient) <sup class="footnote-ref" id="fnref:nesterov1983method"><a href="#fn:nesterov1983method">3</a></sup> 是一种 Momentum 算法的变种，其核心思想会利用“下一步的梯度”确定“这一步的梯度”，当然这里“下一步的梯度”并非真正的下一步的梯度，而是指仅根据动量项更新后位置的梯度。Sutskever <sup class="footnote-ref" id="fnref:sutskever2013training"><a href="#fn:sutskever2013training">4</a></sup> 给出了一种更新参数的方法：</p>

<p><code>$$
\begin{equation}
\begin{split}
\boldsymbol{v}_t &amp;= - \alpha \nabla_{\boldsymbol{\theta}} J \left(\boldsymbol{\theta}_t + \gamma \boldsymbol{v}_{t-1}\right) + \gamma \boldsymbol{v}_{t-1} \\
\boldsymbol{\theta}_t &amp;= \boldsymbol{\theta}_{t-1} + \boldsymbol{v}_t
\end{split}
\end{equation}
$$</code></p>

<p>针对 Momentum 和 NAG 两种不同的方法，其更新权重的差异如下图所示：</p>

<p><img src="/images/cn/2018-02-24-optimization-methods-for-deeplearning/momentum-and-nag.png" alt="" /></p>

<h1 id="adagrad">AdaGrad</h1>

<p>AdaGrad <sup class="footnote-ref" id="fnref:duchi2011adaptive"><a href="#fn:duchi2011adaptive">5</a></sup> 是一种具有自适应学习率的的方法，其对于低频特征的参数选择更大的更新量，对于高频特征的参数选择更小的更新量。因此，AdaGrad算法更加适用于处理稀疏数据。Pennington 等则利用该方法训练 GloVe <sup class="footnote-ref" id="fnref:pennington2014glove"><a href="#fn:pennington2014glove">6</a></sup> 词向量，因为对于出现次数较少的词应当获得更大的参数更新。</p>

<p>因为每个参数的学习速率不再一样，则在 <code>$t$</code> 时刻第 <code>$i$</code> 个参数的变化为</p>

<p><code>$$
\theta_{t, i} = \theta_{t-1, i} - \alpha \nabla_{\theta} J \left(\theta_{t-1, i}\right)
$$</code></p>

<p>根据 AdaGrad 方法的更新方式，我们对学习率做出如下变化</p>

<p><code>$$
\theta_{t, i} = \theta_{t-1, i} - \dfrac{\alpha}{\sqrt{G_{t, i}} + \epsilon} \nabla_{\theta} J \left(\theta_{t-1, i}\right)
$$</code></p>

<p>其中，<code>$G_t$</code> 表示截止到 <code>$t$</code> 时刻梯度的平方和；<code>$\epsilon$</code> 为平滑项，防止除数为零，一般设置为 <code>$10^{-8}$</code>。AdaGrad 最大的优势就在于其能够自动调节每个参数的学习率。</p>

<h1 id="adadelta">Adadelta</h1>

<p>上文中 AdaGrad 算法存在一个缺点，即其用于调节学习率的分母中包含的是一个梯度的平方累加项，随着训练的不断进行，这个值将会越来越大，也就是说学习率将会越来越小，最终导致模型不会再学习到任何知识。Adadelta <sup class="footnote-ref" id="fnref:zeiler2012adadelta"><a href="#fn:zeiler2012adadelta">7</a></sup> 方法针对 AdaGrad 的这个问题，做出了进一步改进，其不再计算历史所以梯度的平方和，而是使用一个固定长度 <code>$w$</code> 的滑动窗口内的梯度。</p>

<p>因为存储 <code>$w$</code> 的梯度平方并不高效，Adadelta 采用了一种递归的方式进行计算，定义 <code>$t$</code> 时刻梯度平方的均值为</p>

<p><code>$$
E \left[g^2\right]_t = \rho E \left[g^2\right]_{t-1} + \left(1 - \rho\right) g^2_{t}
$$</code></p>

<p>其中，<code>$g_t$</code> 表示 <code>$t$</code> 时刻的梯度；<code>$\rho$</code> 为一个衰减项，类似于 Momentum 中的衰减项。在更新参数过程中我们需要其平方根，即</p>

<p><code>$$
\text{RMS} \left[g\right]_t = \sqrt{E \left[g^2\right]_t + \epsilon}
$$</code></p>

<p>则参数的更新量为</p>

<p><code>$$
\Delta \theta_t = - \dfrac{\alpha}{\text{RMS} \left[g\right]_t} g_t
$$</code></p>

<p>除此之外，作者还考虑到上述更新中更新量和参数的假设单位不一致的情况，在上述更新公式中添加了一个关于参数的衰减项</p>

<p><code>$$
\text{RMS} \left[\Delta \theta\right]_t = \sqrt{E \left[\Delta \theta^2\right]_t + \epsilon}
$$</code></p>

<p>其中</p>

<p><code>$$
E \left[\Delta \theta^2\right]_t = \rho E \left[\Delta \theta^2\right]_{t-1} + \left(1 - \rho\right) \Delta \theta_t^2
$$</code></p>

<p>在原始的论文中，作者直接用 <code>$\text{RMS} \left[\Delta \theta^2\right]_t$</code> 替换了学习率，即</p>

<p><code>$$
\Delta \theta_t = - \dfrac{\text{RMS} \left[\Delta \theta\right]_{t-1}}{\text{RMS} \left[g\right]_t} g_t
$$</code></p>

<p>而在 <code>Keras</code> 源码中，则保留了固定的学习率，即</p>

<p><code>$$
\Delta \theta_t = - \alpha \dfrac{\text{RMS} \left[\Delta \theta\right]_{t-1}}{\text{RMS} \left[g\right]_t} g_t
$$</code></p>

<h1 id="rmsprop">RMSprop</h1>

<p>RMSprop <sup class="footnote-ref" id="fnref:hinton2012rmsprop"><a href="#fn:hinton2012rmsprop">8</a></sup> 是由 Hinton 提出的一种针对 AdaGrad 的改进算法。参数的更新量为</p>

<p><code>$$
\Delta \theta_t = - \dfrac{\alpha}{\text{RMS} \left[g\right]_t} g_t
$$</code></p>

<h1 id="adam">Adam</h1>

<p>Adam (Adaptive Moment Estimation) <sup class="footnote-ref" id="fnref:kingma2014adam"><a href="#fn:kingma2014adam">9</a></sup> 是另一种类型的自适应学习率方法，类似 Adadelta，Adam 对于每个参数都计算各自的学习率。Adam 方法中包含一个一阶梯度衰减项 <code>$m_t$</code> 和一个二阶梯度衰减项 <code>$v_t$</code></p>

<p><code>$$
\begin{equation}
\begin{split}
m_t &amp;= \beta_1 m_{t-1} + \left(1 - \beta_1\right) g_t \\
v_t &amp;= \beta_2 v_{t-1} + \left(1 - \beta_2\right) g_t^2
\end{split}
\end{equation}
$$</code></p>

<p>算法中，<code>$m_t$</code> 和 <code>$v_t$</code> 初始化为零向量，作者发现两者会更加偏向 <code>$0$</code>，尤其是在训练的初始阶段和衰减率很小的时候 (即 <code>$\beta_1$</code> 和 <code>$\beta_2$</code> 趋近于1的时候)。因此，对其偏差做如下校正</p>

<p><code>$$
\begin{equation}
\begin{split}
\hat{m}_t &amp;= \dfrac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &amp;= \dfrac{v_t}{1 - \beta_2^t}
\end{split}
\end{equation}
$$</code></p>

<p>最终得到 Adam 算法的参数更新量如下</p>

<p><code>$$
\Delta \theta = - \dfrac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$</code></p>

<h1 id="adamax">Adamax</h1>

<p>在 Adam 中参数的更新方法利用了 <code>$L_2$</code> 正则形式的历史梯度 (<code>$v_{t-1}$</code>) 和当前梯度 (<code>$|g_t|^2$</code>)，因此，更一般的，我们可以使用 <code>$L_p$</code> 正则形式，即</p>

<p><code>$$
\begin{equation}
\begin{split}
v_t &amp;= \beta_2^p v_{t-1} + \left(1 - \beta_2^p\right) |g_t|^p \\
&amp;= \left(1 - \beta_2^p\right) \sum_{i=1}^{t} \beta_2^{p\left(t-i\right)} \cdot |g_t|^p
\end{split}
\end{equation}
$$</code></p>

<p>这样的变换对于值较大的 <code>$p$</code> 而言是很不稳定的，但对于极端的情况，当 <code>$p$</code> 趋近于无穷的时候，则变为了一个简单并且稳定的算法。则在 <code>$t$</code> 时刻对应的我们需要计算 <code>$v_t^{1/p}$</code>，令 <code>$u_t = \lim_{p \to \infty} \left(v_t\right)^{1/p}$</code>，则有</p>

<p><code>$$
\begin{equation}
\begin{split}
u_t &amp;= \lim_{p \to \infty} \left(\left(1 - \beta_2^p\right) \sum_{i=1}^{t} \beta_2^{p\left(t-i\right)} \cdot |g_t|^p\right)^{1/p} \\
&amp;= \lim_{p \to \infty} \left(1 - \beta_2^p\right)^{1/p} \left(\sum_{i=1}^{t} \beta_2^{p\left(t-i\right)} \cdot |g_t|^p\right)^{1/p} \\
&amp;= \lim_{p \to \infty} \left(\sum_{i=1}^{t} \beta_2^{p\left(t-i\right)} \cdot |g_t|^p\right)^{1/p} \\
&amp;= \max \left(\beta_2^{t-1} |g_1|, \beta_2^{t-2} |g_2|, ..., \beta_{t-1} |g_t|\right)
\end{split}
\end{equation}
$$</code></p>

<p>写成递归的形式，则有</p>

<p><code>$$
u_t = \max \left(\beta_2 \cdot u_{t-1}, |g_t|\right)
$$</code></p>

<p>则 Adamax 算法的参数更新量为</p>

<p><code>$$
\Delta \theta = - \dfrac{\alpha}{u_t} \hat{m}_t
$$</code></p>

<h1 id="nadam">Nadam</h1>

<p>Adam 算法可以看做是对 RMSprop 和 Momentum 的结合：历史平方梯度的衰减项 <code>$v_t$</code> (RMSprop) 和 历史梯度的衰减项 <code>$m_t$</code> (Momentum)。Nadam (Nesterov-accelerated Adaptive Moment Estimation) <sup class="footnote-ref" id="fnref:dozat2016incorporating"><a href="#fn:dozat2016incorporating">10</a></sup> 则是将 Adam 同 NAG 进行了进一步结合。我们利用 Adam 中的符号重新回顾一下 NAG 算法</p>

<p><code>$$
\begin{equation}
\begin{split}
g_t &amp;= \nabla_{\theta} J \left(\theta_t - \gamma m_{t-1}\right) \\
m_t &amp;= \gamma m_{t-1} + \alpha g_t \\
\theta_t &amp;= \theta_{t-1} - m_t
\end{split}
\end{equation}
$$</code></p>

<p>NAG 算法的核心思想会利用“下一步的梯度”确定“这一步的梯度”，在 Nadam 算法中，作者在考虑“下一步的梯度”时对 NAG 进行了改动，修改为</p>

<p><code>$$
\begin{equation}
\begin{split}
g_t &amp;= \nabla_{\theta} J \left(\theta_t\right) \\
m_t &amp;= \gamma m_{t-1} + \alpha g_t \\
\theta_t &amp;= \theta_{t-1} - \left(\gamma m_t + \alpha g_t\right)
\end{split}
\end{equation}
$$</code></p>

<p>对于 Adam，根据</p>

<p><code>$$
\hat{m}_t = \dfrac{\beta_1 m_{t-1}}{1 - \beta_1^t} + \dfrac{\left(1 - \beta_1\right) g_t}{1 - \beta_1^t}
$$</code></p>

<p>则有</p>

<p><code>$$
\begin{equation}
\begin{split}
\Delta \theta &amp;= - \dfrac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t \\
&amp;= - \dfrac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \left(\dfrac{\beta_1 m_{t-1}}{1 - \beta_1^t} + \dfrac{\left(1 - \beta_1\right) g_t}{1 - \beta_1^t}\right)
\end{split}
\end{equation}
$$</code></p>

<p>上式中，仅 <code>$\dfrac{\beta_1 m_{t-1}}{1 - \beta_1^t}$</code> 和动量项相关，因此我们类似上文中对 NAG 的改动，通过简单的替换加入 Nesterov 动量项，最终得到 Nadam 方法的参数的更新量</p>

<p><code>$$
\Delta \theta = - \dfrac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \left(\dfrac{\beta_1 m_{t-1}}{1 - \beta_1^{t+1}} + \dfrac{\left(1 - \beta_1\right) g_t}{1 - \beta_1^t}\right)
$$</code></p>

<h1 id="amsgrad">AMSGrad</h1>

<p>对于前面提到的 Adadelta，RMSprop，Adam 和 Nadam 方法，他们均采用了平方梯度的指数平滑平均值迭代产生新的梯度，但根据观察，在一些情况下这些算法并不能收敛到最优解。Reddi 等提出了一种新的 Adam 变体算法 AMSGrad <sup class="footnote-ref" id="fnref:reddi2018convergence"><a href="#fn:reddi2018convergence">11</a></sup>，在文中作者解释了为什么 RMSprop 和 Adam 算法无法收敛到一个最优解的问题。通过分析表明，为了保证得到一个收敛的最优解需要保留过去梯度的“长期记忆”，因此在 AMSGrad 算法中使用了历史平方梯度的最大值而非滑动平均进行更新参数，即</p>

<p><code>$$
\begin{equation}
\begin{split}
m_t &amp;= \beta_1 m_{t-1} + \left(1 - \beta_1\right) g_t \\
v_t &amp;= \beta_2 v_{t-1} + \left(1 - \beta_2\right) g_t^2 \\
\hat{v}_t &amp;= \max \left(\hat{v}_{t-1}, v_t\right) \\
\Delta \theta &amp;= - \dfrac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} m_t
\end{split}
\end{equation}
$$</code></p>

<p>作者在一些小数据集和 CIFAR-10 数据集上得到了相比于 Adam 更好的效果，但与此同时一些其他的 <a href="https://fdlm.github.io/post/amsgrad/" rel="noreferrer" target="_blank">实验</a> 却得到了相比与 Adam 类似或更差的结果，因此对于 AMSGrad 算法的效果还有待进一步确定。</p>

<h1 id="算法可视化">算法可视化</h1>

<p>正所谓一图胜千言，<a href="https://imgur.com/a/Hqolp" rel="noreferrer" target="_blank">Alec Radford</a> 提供了 2 张图形象了描述了不同优化算法之间的区别</p>

<p><img src="/images/cn/2018-02-24-optimization-methods-for-deeplearning/contours-evaluation-optimizers.gif" style="float: left; width: 50%;" />
<img src="/images/cn/2018-02-24-optimization-methods-for-deeplearning/saddle-point-evaluation-optimizers.gif" style="clear: right; width: 50%;" /></p>

<p>左图为 <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization" rel="noreferrer" target="_blank">Beale Function</a> 在二维平面上的等高线，从图中可以看出 AdaGrad，Adadelta 和 RMSprop 算法很快的找到正确的方向并迅速的收敛到最优解；Momentum 和 NAG 则在初期出现了偏离，但偏离之后调整了方向并收敛到最优解；而 SGD 尽管方向正确，但收敛速度过慢。</p>

<p>右图为包含鞍点的一个三维图像，图像函数为 <code>$z = x^2 - y^2$</code>，从图中可以看出 AdaGrad，Adadelta 和 RMSprop 算法能够相对很快的逃离鞍点，而 Momentum，NAG 和 SGD 则相对比较困难逃离鞍点。</p>

<p>很不幸没能找到 Alec Radford 绘图的原始代码，不过 Louis Tiao 在 <a href="http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/" rel="noreferrer" target="_blank">博客</a> 中给出了绘制类似动图的方法。因此，本文参考该博客和 <code>Keras</code> 源码中对不同优化算法的实现重新绘制了 2 张类似图像，详细过程参见 <a href="https://github.com/leovan/leovan.me/tree/master/scripts/cn/2018-02-24-optimization-methods-for-deeplearning" rel="noreferrer" target="_blank">源代码</a>，动图如下所示：</p>

<p><img src="/images/cn/2018-02-24-optimization-methods-for-deeplearning/beales-2d-anim.gif" style="float: left; clear: both; width: 50%;" />
<img src="/images/cn/2018-02-24-optimization-methods-for-deeplearning/saddle-3d-anim.gif" style="clear: both; width: 50%;" /></p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:ruder2016overview">Ruder, Sebastian. &ldquo;An overview of gradient descent optimization algorithms.&rdquo; <em>arXiv preprint arXiv:1609.04747</em> (2016).
 <a class="footnote-return" href="#fnref:ruder2016overview">↩</a></li>
<li id="fn:qian1999momentum">Qian, Ning. &ldquo;On the momentum term in gradient descent learning algorithms.&rdquo; <em>Neural networks</em> 12.1 (1999): 145-151.
 <a class="footnote-return" href="#fnref:qian1999momentum">↩</a></li>
<li id="fn:nesterov1983method">Nesterov, Yurii. &ldquo;A method for unconstrained convex minimization problem with the rate of convergence O (1/k^2).&rdquo; <em>Doklady AN USSR.</em> Vol. 269. 1983.
 <a class="footnote-return" href="#fnref:nesterov1983method">↩</a></li>
<li id="fn:sutskever2013training">Sutskever, Ilya. &ldquo;Training recurrent neural networks.&rdquo; University of Toronto, Toronto, Ont., Canada (2013).
 <a class="footnote-return" href="#fnref:sutskever2013training">↩</a></li>
<li id="fn:duchi2011adaptive">Duchi, John, Elad Hazan, and Yoram Singer. &ldquo;Adaptive subgradient methods for online learning and stochastic optimization.&rdquo; <em>Journal of Machine Learning Research</em> 12.Jul (2011): 2121-2159.
 <a class="footnote-return" href="#fnref:duchi2011adaptive">↩</a></li>
<li id="fn:pennington2014glove">Pennington, Jeffrey, Richard Socher, and Christopher Manning. &ldquo;Glove: Global vectors for word representation.&rdquo; <em>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP).</em> 2014.
 <a class="footnote-return" href="#fnref:pennington2014glove">↩</a></li>
<li id="fn:zeiler2012adadelta">Zeiler, Matthew D. &ldquo;ADADELTA: an adaptive learning rate method.&rdquo; <em>arXiv preprint arXiv:1212.5701</em> (2012).
 <a class="footnote-return" href="#fnref:zeiler2012adadelta">↩</a></li>
<li id="fn:hinton2012rmsprop">Hinton, G., Nitish Srivastava, and Kevin Swersky. &ldquo;Rmsprop: Divide the gradient by a running average of its recent magnitude.&rdquo; <em>Neural networks for machine learning, Coursera lecture 6e</em> (2012).
 <a class="footnote-return" href="#fnref:hinton2012rmsprop">↩</a></li>
<li id="fn:kingma2014adam">Kingma, Diederik P., and Jimmy Ba. &ldquo;Adam: A method for stochastic optimization.&rdquo; <em>arXiv preprint arXiv:1412.6980</em> (2014).
 <a class="footnote-return" href="#fnref:kingma2014adam">↩</a></li>
<li id="fn:dozat2016incorporating">Dozat, Timothy. &ldquo;Incorporating nesterov momentum into adam.&rdquo; (2016).
 <a class="footnote-return" href="#fnref:dozat2016incorporating">↩</a></li>
<li id="fn:reddi2018convergence">Reddi, Sashank J., Satyen Kale, and Sanjiv Kumar. &ldquo;On the convergence of adam and beyond.&rdquo; International Conference on Learning Representations. 2018.
 <a class="footnote-return" href="#fnref:reddi2018convergence">↩</a></li>
</ol>
</div>



<link rel="stylesheet" href="/css/donate.css" />


<div class="donate">
  <div class="donate-header"></div>
  <div class="donate-slug" id="donate-slug">optimization-methods-for-deeplearning</div>
  <button class="donate-button">赞 赏</button>
  <div class="donate-footer">「真诚赞赏，手留余香」</div>
</div>
<div class="donate-modal-wrapper">
  <div class="donate-modal">
    <div class="donate-box">
      <div class="donate-box-content">
        <div class="donate-box-content-inner">
          <div class="donate-box-header">「真诚赞赏，手留余香」</div>
          <div class="donate-box-body">
            <div class="donate-box-money">
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-2" data-v="2" data-unchecked="￥ 2" data-checked="2 元">￥ 2</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-5" data-v="5" data-unchecked="￥ 5" data-checked="5 元">￥ 5</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-10" data-v="10" data-unchecked="￥ 10" data-checked="10 元">￥ 10</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-50" data-v="50" data-unchecked="￥ 50" data-checked="50 元">￥ 50</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-100" data-v="100" data-unchecked="￥ 100" data-checked="100 元">￥ 100</button>
              <button class="donate-box-money-button donate-box-money-button-unchecked" id="donate-box-money-button-custom" data-v="custom" data-unchecked="任意金额" data-checked="任意金额">任意金额</button>
            </div>
            <div class="donate-box-pay">
              <img class="donate-box-pay-qrcode" id="donate-box-pay-qrcode" src=""/>
            </div>
          </div>
          <div class="donate-box-footer">
            <div class="donate-box-pay-method donate-box-pay-method-checked" data-v="wechat-pay">
              <img class="donate-box-pay-method-image" id="donate-box-pay-method-image-wechat-pay" src=""/>
            </div>
            <div class="donate-box-pay-method" data-v="alipay">
              <img class="donate-box-pay-method-image"  id="donate-box-pay-method-image-alipay" src=""/>
            </div>
          </div>
        </div>
      </div>
    </div>
    <button type="button" class="donate-box-close-button">
      <svg class="donate-box-close-button-icon" fill="#fff" viewBox="0 0 24 24" width="24" height="24"><path d="M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z" fill-rule="evenodd"></path></svg>
    </button>
  </div>
</div>

<script type="text/javascript" src="/js/donate.js"></script>
</script>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/cn/2018/02/gan-introduction/">生成对抗网络简介 (GAN Introduction)</a></span>
  <span class="nav-next"><a href="/cn/2018/03/manifold-learning/">流形学习 (Manifold Learning)</a> &rarr;</span>
</nav>
<script type="text/javascript">
document.addEventListener('keyup', function(e) {
  if (e.target.nodeName.toUpperCase() != 'BODY') return;
  var url = false;
  if (e.which == 37) {  
    
    url = '\/cn\/2018\/02\/gan-introduction\/';
    
  } else if (e.which == 39) {  
    
    url = '\/cn\/2018\/03\/manifold-learning\/';
    
  }
  if (url) window.location = url;
});
</script>




<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-2608165017777396"
     data-ad-slot="1261604535"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>





<section class="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
    this.page.url = "https:\/\/zeqiang.fun" + location.pathname;
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var disqus_js = '//Zeqiang.disqus.com/embed.js';
    var d = document, s = d.createElement('script');
    s.src = disqus_js; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    var t = d.getElementById('disqus_thread');
    var b = false, l = function(scroll) {
      if (b) return;
      (d.head || d.body).appendChild(s); b = true;
      if (scroll) t.scrollIntoView();
    }
    s.onerror = function(e) {
      if (sessionStorage.getItem('failure-note')) return;
      t.innerText = 'Sorry, but you cannot make comments because Disqus failed to load for some reason. It is known to be blocked in China. If you are sure it is not blocked in your region, please refresh the page. 中国大陆地区读者需要翻墙才能发表评论。';
      sessionStorage.setItem('failure-note', true);
    };
    
    if (location.hash.match(/^#comment-[0-9]+$/)) return l(true);
    var c = function() {
      if (b) return;
      var rect = t.getBoundingClientRect();
      if (rect.top < window.innerHeight && rect.bottom >= 0) l();
    };
    window.addEventListener('load', c);
    d.addEventListener('scroll', c);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>





<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/autoloader/prism-autoloader.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/plugins/show-language/prism-show-language.min.js"></script>

<script>
    (function() {
        if (!self.Prism) {
            return;
        }

        Prism.languages.dos = Prism.languages.powershell;
        Prism.languages.gremlin = Prism.languages.groovy;

        var Languages = {
            'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',
            'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',
            'powershell': 'PowerShell', 'javascript': 'JavaScript',
            'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',
            'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',
            'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',
            'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration'
        };

        Prism.hooks.add('before-highlight', function(env) {
        	var language = Languages[env.language] || env.language;
        	env.element.setAttribute('data-language', language);
        });
    })();
</script>




<script async src="/js/fix-toc.js"></script>
<script async src="/js/center-img.js"></script>
<script async src="/js/right-quote.js"></script>
<script async src="/js/fix-footnote.js"></script>
<script async src="/js/external-link.js"></script>
<script async src="/js/alt-title.js"></script>
<script src="/js/no-highlight.js"></script>
<script src="/js/math-code.js"></script>


<script>
window.MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js" crossorigin></script>


<script src="//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
    var captionCount = 0;
    $(".pseudocode").each(function() {
        var pseudocode_options = {
            indentSize: '1.2em',
            commentDelimiter: '\/\/',
            lineNumber:  true ,
            lineNumberPunc: ':',
            noEnd:  false 
        };
        pseudocode_options.captionCount = captionCount;
        captionCount += 1;
        var codeEl = $(this).get(0);
        pseudocode.render(codeEl.textContent, codeEl.parentElement, pseudocode_options);
    });
});
</script>






<script async src="/js/load-typekit.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lazysizes/5.3.2/lazysizes.min.js"></script>

<script src="//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js"></script>
<script>
addBackToTop({
  diameter: 48
})
</script>



  <hr>
  <div class="copyright no-border-bottom">
    <div class="copyright-author-year">
      <span>&copy; 2017-2021 Leo Van</span>
    </div>
    <div class="copyright-links">
      <a href="https://github.com/leovan" rel="noreferrer" target="_blank">Github</a>
      <span> · </span>
      <a href="https://orcid.org/0000-0002-9556-7821" rel="noreferrer" target="_blank">ORCID</a>
      <span> · </span>
      <span>I am Mr. Black.</span>
    </div>
  </div>
  </footer>
  </article>
  </body>
</html>

